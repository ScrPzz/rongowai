{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import uniform, randint\n",
    "import warnings\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "from scipy.fft import fft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LightGBMBinaryPipeline:\n",
    "    def __init__(self, fit_data, labels_data, test_size=0.2, val_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Pipeline completa per classificatore binario LightGBM\n",
    "        \n",
    "        Args:\n",
    "            fit_data: DataFrame o array con le features\n",
    "            labels_data: Serie o array con le etichette\n",
    "            test_size: dimensione del test set\n",
    "            val_size: dimensione del validation set (dal training set)\n",
    "            random_state: seed per riproducibilitÃ \n",
    "        \"\"\"\n",
    "        self.fit_data = fit_data\n",
    "        self.labels_data = labels_data\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Modelli e risultati\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.train_scores = {}\n",
    "        self.val_scores = {}\n",
    "        self.test_scores = {}\n",
    "        \n",
    "        # Dati preprocessati\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        # Feature categoriche\n",
    "        self.categorical_features = []\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = None\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocessa i dati e li divide in train/val/test\"\"\"\n",
    "        print(\"ðŸ”„ Preprocessing dei dati...\")\n",
    "        \n",
    "        # Converti in DataFrame se necessario\n",
    "        if not isinstance(self.fit_data, pd.DataFrame):\n",
    "            self.fit_data = pd.DataFrame(self.fit_data)\n",
    "        \n",
    "        # Copia per evitare modifiche ai dati originali\n",
    "        X = self.fit_data.copy()\n",
    "        \n",
    "        # Identifica e preprocessa feature categoriche\n",
    "        self.categorical_features = []\n",
    "        categorical_indices = []\n",
    "        \n",
    "        for idx, col in enumerate(X.columns):\n",
    "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "                self.categorical_features.append(col)\n",
    "                categorical_indices.append(idx)\n",
    "                # Label encoding per LightGBM\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                self.label_encoders[col] = le\n",
    "        \n",
    "        # Salva gli indici per LightGBM\n",
    "        self.categorical_indices = categorical_indices\n",
    "        \n",
    "        print(f\"ðŸ“Š Feature categoriche identificate: {self.categorical_features}\")\n",
    "        print(f\"ðŸ“Š Indici feature categoriche: {categorical_indices}\")\n",
    "        \n",
    "        # Split train-test\n",
    "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
    "            X, self.labels_data, \n",
    "            test_size=self.test_size, \n",
    "            stratify=self.labels_data,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Split train-validation\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=self.val_size,\n",
    "            stratify=y_temp,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Dimensioni datasets:\")\n",
    "        print(f\"  - Train: {self.X_train.shape}\")\n",
    "        print(f\"  - Validation: {self.X_val.shape}\")\n",
    "        print(f\"  - Test: {self.X_test.shape}\")\n",
    "        \n",
    "        # Statistiche sulle classi\n",
    "        print(f\"ðŸ“Š Distribuzione classi (Train): {pd.Series(self.y_train).value_counts().to_dict()}\")\n",
    "        \n",
    "    def define_search_space(self):\n",
    "        \"\"\"Definisce lo spazio di ricerca per gli iperparametri di LightGBM\"\"\"\n",
    "        param_distributions = {\n",
    "            'n_estimators': randint(50, 1000),\n",
    "            'learning_rate': uniform(0.01, 0.2),\n",
    "            'max_depth': randint(3, 15),\n",
    "            'num_leaves': randint(10, 300),\n",
    "            'min_child_samples': randint(5, 100),\n",
    "            'min_child_weight': uniform(1e-3, 10),\n",
    "            'subsample': uniform(0.6, 0.4),  # 0.6 to 1.0\n",
    "            'colsample_bytree': uniform(0.6, 0.4),  # 0.6 to 1.0\n",
    "            'reg_alpha': uniform(0, 10),\n",
    "            'reg_lambda': uniform(0, 10),\n",
    "            'min_split_gain': uniform(0, 1),\n",
    "            'subsample_freq': randint(0, 10),\n",
    "            'class_weight': [None, 'balanced'],\n",
    "        }\n",
    "        return param_distributions\n",
    "    \n",
    "    def hyperparameter_search(self, n_iter=50, cv_folds=5, scoring='roc_auc'):\n",
    "        \"\"\"\n",
    "        Esegue la ricerca degli iperparametri\n",
    "        \n",
    "        Args:\n",
    "            n_iter: numero di iterazioni per RandomizedSearchCV\n",
    "            cv_folds: numero di fold per cross-validation\n",
    "            scoring: metrica di scoring\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ” Avvio ricerca iperparametri ({n_iter} iterazioni, {cv_folds}-fold CV)...\")\n",
    "        \n",
    "        # Modello base\n",
    "        base_model = LGBMClassifier(\n",
    "            random_state=self.random_state,\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            boosting_type='gbdt',\n",
    "            verbose=-1,\n",
    "            n_jobs=-1,\n",
    "            categorical_feature=self.categorical_indices if self.categorical_indices else 'auto'\n",
    "        )\n",
    "        \n",
    "        # Spazio di ricerca\n",
    "        param_distributions = self.define_search_space()\n",
    "        \n",
    "        # Cross-validation stratificata\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # RandomizedSearchCV\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            random_state=self.random_state,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit del modello\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Salva i migliori parametri\n",
    "        self.best_params = random_search.best_params_\n",
    "        self.best_model = random_search.best_estimator_\n",
    "        \n",
    "        print(f\"âœ… Ricerca completata!\")\n",
    "        print(f\"ðŸ† Migliore score CV: {random_search.best_score_:.4f}\")\n",
    "        print(f\"ðŸŽ¯ Migliori parametri:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "            \n",
    "    def train_final_model(self):\n",
    "        \"\"\"Addestra il modello finale con i migliori iperparametri\"\"\"\n",
    "        print(\"ðŸš€ Addestramento modello finale...\")\n",
    "        \n",
    "        if self.best_params is None:\n",
    "            print(\"âš ï¸  Nessun iperparametro trovato. Uso parametri di default.\")\n",
    "            self.best_model = LGBMClassifier(\n",
    "                random_state=self.random_state,\n",
    "                objective='binary',\n",
    "                metric='auc',\n",
    "                verbose=-1,\n",
    "                categorical_feature=self.categorical_indices if self.categorical_indices else 'auto'\n",
    "            )\n",
    "        \n",
    "        # Addestramento con early stopping su validation set\n",
    "        self.best_model.fit(\n",
    "            self.X_train, \n",
    "            self.y_train,\n",
    "            eval_set=[(self.X_val, self.y_val)],\n",
    "            eval_names=['validation'],\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100\n",
    "        )\n",
    "        \n",
    "        # Salva la storia del training\n",
    "        if hasattr(self.best_model, 'evals_result_'):\n",
    "            self.training_history = self.best_model.evals_result_\n",
    "        \n",
    "        print(\"âœ… Addestramento completato!\")\n",
    "        print(f\"ðŸ“Š Best iteration: {self.best_model.best_iteration}\")\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Valuta il modello su tutti i dataset\"\"\"\n",
    "        print(\"ðŸ“Š Valutazione del modello...\")\n",
    "        \n",
    "        datasets = {\n",
    "            'train': (self.X_train, self.y_train),\n",
    "            'validation': (self.X_val, self.y_val),\n",
    "            'test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        all_scores = {}\n",
    "        \n",
    "        for name, (X, y) in datasets.items():\n",
    "            # Predizioni\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            y_pred_proba = self.best_model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            # Metriche\n",
    "            scores = {\n",
    "                'accuracy': accuracy_score(y, y_pred),\n",
    "                'precision': precision_score(y, y_pred),\n",
    "                'recall': recall_score(y, y_pred),\n",
    "                'f1': f1_score(y, y_pred),\n",
    "                'roc_auc': roc_auc_score(y, y_pred_proba)\n",
    "            }\n",
    "            \n",
    "            all_scores[name] = scores\n",
    "            \n",
    "            print(f\"\\nðŸ“ˆ Metriche {name.upper()}:\")\n",
    "            for metric, value in scores.items():\n",
    "                print(f\"  - {metric.upper()}: {value:.4f}\")\n",
    "        \n",
    "        # Salva i risultati\n",
    "        self.train_scores = all_scores['train']\n",
    "        self.val_scores = all_scores['validation']\n",
    "        self.test_scores = all_scores['test']\n",
    "        \n",
    "        return all_scores\n",
    "    \n",
    "    def plot_feature_importance(self, top_n=20, importance_type='gain'):\n",
    "        \"\"\"Visualizza l'importanza delle feature\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"âš ï¸  Modello non ancora addestrato!\")\n",
    "            return\n",
    "            \n",
    "        # Ottieni feature importance\n",
    "        feature_names = self.X_train.columns if hasattr(self.X_train, 'columns') else [f'feature_{i}' for i in range(self.X_train.shape[1])]\n",
    "        \n",
    "        # LightGBM supporta diversi tipi di importance\n",
    "        importance = self.best_model.feature_importances_\n",
    "        \n",
    "        # Crea DataFrame per plotting\n",
    "        fi_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False).head(top_n)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(data=fi_df, x='importance', y='feature')\n",
    "        plt.title(f'Top {top_n} Feature Importance (LightGBM - {importance_type})')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot aggiuntivo con importanza nativa di LightGBM\n",
    "        try:\n",
    "            lgb.plot_importance(self.best_model, max_num_features=top_n, importance_type=importance_type, figsize=(12, 8))\n",
    "            plt.title(f'LightGBM Native Feature Importance ({importance_type})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except:\n",
    "            print(\"âš ï¸  Impossibile creare il plot nativo di LightGBM\")\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Visualizza la storia del training\"\"\"\n",
    "        if self.training_history is None:\n",
    "            print(\"âš ï¸  Storia del training non disponibile!\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot delle metriche di training\n",
    "        for dataset_name, metrics in self.training_history.items():\n",
    "            for metric_name, values in metrics.items():\n",
    "                plt.plot(values, label=f'{dataset_name}_{metric_name}')\n",
    "        \n",
    "        plt.title('Training History - LightGBM')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Metric Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Aggiungi linea verticale per best iteration\n",
    "        if hasattr(self.best_model, 'best_iteration'):\n",
    "            plt.axvline(x=self.best_model.best_iteration, color='red', linestyle='--', \n",
    "                       label=f'Best Iteration ({self.best_model.best_iteration})')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self):\n",
    "        \"\"\"Visualizza le confusion matrix per tutti i dataset\"\"\"\n",
    "        datasets = {\n",
    "            'Train': (self.X_train, self.y_train),\n",
    "            'Validation': (self.X_val, self.y_val),\n",
    "            'Test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        for idx, (name, (X, y)) in enumerate(datasets.items()):\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            cm = confusion_matrix(y, y_pred)\n",
    "            \n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx], cmap='Blues')\n",
    "            axes[idx].set_title(f'Confusion Matrix - {name}')\n",
    "            axes[idx].set_xlabel('Predicted')\n",
    "            axes[idx].set_ylabel('Actual')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_tree_structure(self, tree_index=0):\n",
    "        \"\"\"Visualizza la struttura di un albero specifico\"\"\"\n",
    "        try:\n",
    "            lgb.plot_tree(self.best_model, tree_index=tree_index, figsize=(20, 15), show_info=['split_gain'])\n",
    "            plt.title(f'LightGBM Tree Structure (Tree {tree_index})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Impossibile visualizzare l'albero: {e}\")\n",
    "            print(\"Assicurati di avere graphviz installato: pip install graphviz\")\n",
    "    \n",
    "    def get_classification_reports(self):\n",
    "        \"\"\"Stampa i classification report dettagliati\"\"\"\n",
    "        datasets = {\n",
    "            'Train': (self.X_train, self.y_train),\n",
    "            'Validation': (self.X_val, self.y_val),\n",
    "            'Test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        for name, (X, y) in datasets.items():\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            print(f\"\\nðŸ“‹ CLASSIFICATION REPORT - {name.upper()}\")\n",
    "            print(\"=\" * 50)\n",
    "            print(classification_report(y, y_pred))\n",
    "    \n",
    "    def get_model_complexity_info(self):\n",
    "        \"\"\"Informazioni sulla complessitÃ  del modello\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"âš ï¸  Modello non ancora addestrato!\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\nðŸŒ² INFORMAZIONI MODELLO LIGHTGBM:\")\n",
    "        print(f\"  - Numero alberi: {self.best_model.n_estimators}\")\n",
    "        print(f\"  - Learning rate: {self.best_model.learning_rate}\")\n",
    "        print(f\"  - Max depth: {self.best_model.max_depth}\")\n",
    "        print(f\"  - Num leaves: {self.best_model.num_leaves}\")\n",
    "        print(f\"  - Min child samples: {self.best_model.min_child_samples}\")\n",
    "        print(f\"  - Subsample: {self.best_model.subsample}\")\n",
    "        print(f\"  - Feature fraction: {self.best_model.colsample_bytree}\")\n",
    "        if hasattr(self.best_model, 'best_iteration'):\n",
    "            print(f\"  - Best iteration: {self.best_model.best_iteration}\")\n",
    "        print(f\"  - Objective: {self.best_model.objective}\")\n",
    "        print(f\"  - Boosting type: {self.best_model.boosting_type}\")\n",
    "    \n",
    "    def analyze_overfitting(self):\n",
    "        \"\"\"Analizza il potenziale overfitting\"\"\"\n",
    "        print(f\"\\nðŸ” ANALISI OVERFITTING:\")\n",
    "        \n",
    "        train_auc = self.train_scores['roc_auc']\n",
    "        val_auc = self.val_scores['roc_auc']\n",
    "        test_auc = self.test_scores['roc_auc']\n",
    "        \n",
    "        train_val_gap = train_auc - val_auc\n",
    "        train_test_gap = train_auc - test_auc\n",
    "        \n",
    "        print(f\"  - AUC Train: {train_auc:.4f}\")\n",
    "        print(f\"  - AUC Validation: {val_auc:.4f}\")\n",
    "        print(f\"  - AUC Test: {test_auc:.4f}\")\n",
    "        print(f\"  - Gap Train-Val: {train_val_gap:.4f}\")\n",
    "        print(f\"  - Gap Train-Test: {train_test_gap:.4f}\")\n",
    "        \n",
    "        if train_val_gap > 0.05:\n",
    "            print(\"  âš ï¸  Possibile overfitting (gap > 0.05)\")\n",
    "            print(\"  ðŸ’¡ Considera: early stopping piÃ¹ aggressivo, regolarizzazione, meno iterazioni\")\n",
    "        elif train_val_gap < 0.01:\n",
    "            print(\"  âœ… Buon bilanciamento bias-variance\")\n",
    "        else:\n",
    "            print(\"  âš–ï¸  Overfitting moderato, accettabile\")\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Salva il modello addestrato\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"âš ï¸  Nessun modello da salvare!\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            self.best_model.booster_.save_model(filepath)\n",
    "            print(f\"âœ… Modello salvato in: {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Errore nel salvataggio: {e}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Carica un modello salvato\"\"\"\n",
    "        try:\n",
    "            self.best_model = lgb.Booster(model_file=filepath)\n",
    "            print(f\"âœ… Modello caricato da: {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Errore nel caricamento: {e}\")\n",
    "    \n",
    "    def run_complete_pipeline(self, n_iter=50, cv_folds=5):\n",
    "        \"\"\"Esegue la pipeline completa\"\"\"\n",
    "        print(\"ðŸš€ AVVIO PIPELINE COMPLETA LIGHTGBM\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Preprocessing\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # 2. Ricerca iperparametri\n",
    "        self.hyperparameter_search(n_iter=n_iter, cv_folds=cv_folds)\n",
    "        \n",
    "        # 3. Addestramento finale\n",
    "        self.train_final_model()\n",
    "        \n",
    "        # 4. Valutazione\n",
    "        scores = self.evaluate_model()\n",
    "        \n",
    "        # 5. Informazioni modello\n",
    "        self.get_model_complexity_info()\n",
    "        \n",
    "        # 6. Analisi overfitting\n",
    "        self.analyze_overfitting()\n",
    "        \n",
    "        # 7. Report dettagliati\n",
    "        self.get_classification_reports()\n",
    "        \n",
    "        # 8. Visualizzazioni\n",
    "        self.plot_feature_importance()\n",
    "        self.plot_training_history()\n",
    "        self.plot_confusion_matrices()\n",
    "        \n",
    "        # 9. Visualizzazione albero (opzionale)\n",
    "        print(\"\\nðŸŒ³ Visualizzazione struttura albero (puÃ² richiedere graphviz)...\")\n",
    "        self.plot_tree_structure(tree_index=0)\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ PIPELINE COMPLETATA CON SUCCESSO!\")\n",
    "        print(f\"ðŸ† Performance finale (Test Set): AUC = {self.test_scores['roc_auc']:.4f}\")\n",
    "        \n",
    "        return self.best_model, self.best_params, scores\n",
    "\n",
    "# UTILITY FUNCTIONS PER LIGHTGBM\n",
    "def compare_lgb_importance_types(model, feature_names, top_n=15):\n",
    "    \"\"\"Confronta diversi tipi di feature importance in LightGBM\"\"\"\n",
    "    importance_types = ['split', 'gain']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(importance_types), figsize=(15, 8))\n",
    "    if len(importance_types) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, imp_type in enumerate(importance_types):\n",
    "        try:\n",
    "            lgb.plot_importance(model, max_num_features=top_n, \n",
    "                              importance_type=imp_type, ax=axes[idx])\n",
    "            axes[idx].set_title(f'Feature Importance ({imp_type})')\n",
    "        except:\n",
    "            axes[idx].text(0.5, 0.5, f'Errore plot {imp_type}', \n",
    "                          ha='center', va='center', transform=axes[idx].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def lgb_hyperparameter_ranges_analysis(pipeline):\n",
    "    \"\"\"Analizza i range degli iperparametri trovati\"\"\"\n",
    "    if pipeline.best_params is None:\n",
    "        print(\"âš ï¸  Nessun parametro da analizzare!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nðŸ“Š ANALISI IPERPARAMETRI OTTIMALI:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    param_analysis = {\n",
    "        'n_estimators': 'Numero di alberi nel boosting',\n",
    "        'learning_rate': 'Tasso di apprendimento (eta)',\n",
    "        'max_depth': 'ProfonditÃ  massima alberi',\n",
    "        'num_leaves': 'Numero max foglie per albero',\n",
    "        'min_child_samples': 'Campioni minimi per foglia',\n",
    "        'subsample': 'Frazione campioni per training',\n",
    "        'colsample_bytree': 'Frazione features per albero',\n",
    "        'reg_alpha': 'Regolarizzazione L1',\n",
    "        'reg_lambda': 'Regolarizzazione L2'\n",
    "    }\n",
    "    \n",
    "    for param, description in param_analysis.items():\n",
    "        if param in pipeline.best_params:\n",
    "            value = pipeline.best_params[param]\n",
    "            print(f\"  - {param}: {value}\")\n",
    "            print(f\"    â””â”€ {description}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCDFPreprocessor:\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        \n",
    "        raw_counts = f.variables['raw_counts']\n",
    "        raw_counts = np.array(raw_counts)\n",
    "\n",
    "        # Calcolo distanza tra il punto speculare e l'aereo\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        # Seleziona gli indici dove sp_rx_gain_copol > 5, sp_rx_gain_xpol > 5 e ddm_snr > 0 e distanza tra punto speculare e antenna > 2000 e < 10000\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "        \n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        #filtered_raw_counts_arr = np.array(filtered_raw_counts)\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Inserisci i dati filtrati nelle posizioni di to_keep_indices\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "        del output_array\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "\n",
    "        label_data = surface_types_unravelled\n",
    "        label_data = [label_data[l] for l in range(len(label_data)) if l in keep_indices]\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def process_all_files(self, chunk_size = int, sample_fraction = float, remove_chunks= bool):\n",
    "        \n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        #counter = 0\n",
    "        for file_name in tqdm(self.netcdf_file_list, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                continue\n",
    "            try:\n",
    "                f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                data, labels = self.preprocess(f)\n",
    "                full_data.append(data)\n",
    "                full_labels.append(labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "            #counter += 1\n",
    "            #if counter == 100:  # Limita a 50 file per il caricamento\n",
    "            #    break\n",
    "        \n",
    "        # Trova gli indici degli elementi di full_data con seconda dimensione uguale a 200\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "\n",
    "        # Applica la selezione a full_data e full_labels\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "\n",
    "        # Chunking \n",
    "        \n",
    "        os.makedirs('processed_data/multiclass', exist_ok=True)\n",
    "\n",
    "        chunk_size = chunk_size # dimensione del chunk in numero di campioni\n",
    "        sample_fraction = sample_fraction  # frazione di dati da campionare per ogni chunk\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean))\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            print(f\"Distribuzione etichette nel chunk: {Counter(chunk_labels)}\")\n",
    "            \n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "            print(''-' * 50')\n",
    "            \n",
    "\n",
    "            # Salva ogni chunk come file parquet separato\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'processed_data/multiclass/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'processed_data/multiclass/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "\n",
    "        # Imposta la frazione di dati da campionare per ogni chunk (es: 0.2 per il 20%)\n",
    "        \n",
    "            #_, X_sampled, _, y_sampled = train_test_split(\n",
    "            #    chunk_data, chunk_labels, \n",
    "            #    test_size=sample_fraction, \n",
    "            #    stratify=chunk_labels, \n",
    "            #    random_state=42\n",
    "            #) \n",
    "\n",
    "            # Trova le classi piÃ¹ rare (meno rappresentate)\n",
    "            label_counts = Counter(chunk_labels)\n",
    "            min_count = min(label_counts.values())\n",
    "            rare_classes = [cls for cls, count in label_counts.items() if count == min_count]\n",
    "\n",
    "            # Seleziona tutte le occorrenze delle classi rare\n",
    "            rare_indices = np.isin(chunk_labels, rare_classes)\n",
    "            X_rare = chunk_data[rare_indices]\n",
    "            y_rare = chunk_labels[rare_indices]\n",
    "\n",
    "            # Per le altre classi, esegui un campionamento casuale per raggiungere la frazione desiderata\n",
    "            other_indices = ~rare_indices\n",
    "            X_other = chunk_data[other_indices]\n",
    "            y_other = chunk_labels[other_indices]\n",
    "\n",
    "            _, X_other_sampled, _, y_other_sampled = train_test_split(\n",
    "                X_other, y_other,\n",
    "                test_size=sample_fraction,\n",
    "                stratify=y_other,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            # Combina i dati delle classi rare con quelli campionati delle altre classi\n",
    "            X_sampled = np.vstack([X_rare, X_other_sampled])\n",
    "            y_sampled = np.hstack([y_rare, y_other_sampled])\n",
    "\n",
    "            print(f\"Distribuzione etichette prima del campionamento nel chunk {idx + 1}: {Counter(chunk_labels)}\")\n",
    "            print(f\"Distribuzione etichette dopo campionamento nel chunk {idx + 1}: {Counter(y_sampled)}\")\n",
    "            \n",
    "            del full_data, full_labels\n",
    "            \n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "\n",
    "        print(f\"Distribuzione totale etichette dopo stratificazione: {Counter(full_labels_sampled_stratified)}\")\n",
    "\n",
    "        \n",
    "        print(f\"Shape of sampled data after chunking and sampling: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels after chunking and sampling: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "        \n",
    "        # Crea la cartella processed_data se non esiste\n",
    "        os.makedirs('processed_data/multiclass', exist_ok=True)\n",
    "\n",
    "        # Salva fit_data in formato parquet ottimizzato\n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'processed_data/multiclass/fit_data_multiclass.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Salva labels in formato parquet ottimizzato\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'processed_data/multiclass/labels_multiclass.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        del fit_data_df, labels_df, table_fit, table_labels\n",
    "        # Remove all chunk parquet files if flag is set\n",
    "        if remove_chunks:\n",
    "            try:\n",
    "                chunk_dir = 'processed_data/multiclass'\n",
    "                for fname in os.listdir(chunk_dir):\n",
    "                    if fname.startswith('fit_data_chunk_') or fname.startswith('labels_chunk_'):\n",
    "                        os.remove(os.path.join(chunk_dir, fname))\n",
    "                print(\"All chunk files removed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing chunk files: {e}\")\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_ddm_features_MORE(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Estrae features dettagliate da raw_counts DDM (shape: n_samples x 200)\n",
    "        \"\"\"\n",
    "\n",
    "        def gini(array):\n",
    "            \"\"\"Calcola il coefficiente di Gini (disuguaglianza)\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))\n",
    "        \n",
    "\n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float32) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. Statistiche base\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = gini(x)\n",
    "\n",
    "            # 2. Posizionali\n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentazione\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "            \n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivate e cambiamenti\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelazioni (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT (spettro frequenze)\n",
    "            spectrum = np.abs(fft(x))\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  # simmetrico\n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "            features.append(f)\n",
    "\n",
    "        return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR= 'D:/data/RONGOWAI_L1_SDR_V1.0/'\n",
    "\n",
    "read_from_backup = True\n",
    "if read_from_backup:\n",
    "    #import polars as pl\n",
    "\n",
    "    # Leggi i file parquet con polars\n",
    "    fit_data_pl = pd.read_parquet('processed_data/multiclass/fit_data_multiclass.parquet')\n",
    "    labels_pl = pd.read_parquet('processed_data/multiclass/labels_multiclass.parquet')\n",
    "\n",
    "    # Trasforma in numpy array\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4436264",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extractor = DDMFeatureExtractor()\n",
    "ddm_features = features_extractor.create_ddm_features_MORE(fit_data)\n",
    "fit_data_with_features = np.hstack([fit_data, ddm_features.values])\n",
    "fit_data_with_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza e esegui la pipeline\n",
    "pipeline = LightGBMBinaryPipeline(fit_data, labels)\n",
    "\n",
    "# Esegui pipeline completa\n",
    "best_model, best_params, final_scores = pipeline.run_complete_pipeline(\n",
    "    n_iter=30,  # Riduci per test piÃ¹ veloce\n",
    "    cv_folds=5\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ RISULTATI FINALI:\")\n",
    "print(f\"Migliori parametri: {best_params}\")\n",
    "print(f\"Score finale: {final_scores}\")\n",
    "\n",
    "# Analisi aggiuntive\n",
    "lgb_hyperparameter_ranges_analysis(pipeline)\n",
    "\n",
    "# Confronto tipi di importance\n",
    "if hasattr(pipeline.X_train, 'columns'):\n",
    "    compare_lgb_importance_types(best_model, pipeline.X_train.columns)\n",
    "\n",
    "# Salvataggio modello (opzionale)\n",
    "# pipeline.save_model('best_lightgbm_model.txt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
