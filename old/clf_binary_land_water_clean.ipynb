{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f928518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pycaret\n",
    "from datetime import timezone, datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import os\n",
    "from pycaret.classification import *\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "class SurfaceTypeUtils:\n",
    "    surface_type_dict = {\n",
    "        -1: \"Ocean\",\n",
    "        0: \"NaN\",\n",
    "        1: \"Artifical\",\n",
    "        2: \"Barely vegetated\",\n",
    "        3: \"Inland water\",\n",
    "        4: \"Crop\",\n",
    "        5: \"Grass\",\n",
    "        6: \"Shrub\",\n",
    "        7: \"Forest\"\n",
    "    }\n",
    "    ddm_antennas = {\n",
    "        0: 'None',\n",
    "        1: 'Zenith',\n",
    "        2: 'LHCP',\n",
    "        3: 'RHCP',\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25ff6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeoUtils:\n",
    "    def __init__(self, world_shapefile_path):\n",
    "        self.world = gpd.read_file(world_shapefile_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def is_land(self, lat, lon):\n",
    "        point = Point(lon, lat)\n",
    "        return any(self.world.contains(point))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_ocean_and_land(lst):\n",
    "        has_ocean = -1 in lst\n",
    "        has_land = any(1 <= num <= 7 for num in lst)\n",
    "        return has_ocean and has_land\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3489f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetCDFPreprocessor:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        raw_counts = np.array(f.variables['raw_counts'])\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "        label_data = [1 if surface_type in np.arange(1, 8) else 0 for surface_type in surface_types_unravelled]\n",
    "        label_data = [label_data[l] for l in range(len(label_data)) if l in keep_indices]\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def process_all_files(self):\n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        for file_name in tqdm(self.netcdf_file_list, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                continue\n",
    "            f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "            data, labels = self.preprocess(f)\n",
    "            full_data.append(data)\n",
    "            full_labels.append(labels)\n",
    "\n",
    "        # Stampa i nomi dei file che hanno dimensione minore di due\n",
    "        for i, arr in enumerate(full_data):\n",
    "            if np.array(arr).ndim < 2:\n",
    "                print(f\"File con dimensione < 2: {self.netcdf_file_list[i]}\")\n",
    "        \n",
    "        # Seleziona solo gli array di full_data e full_labels che hanno due dimensioni\n",
    "        valid_dim_indices = [i for i, arr in enumerate(full_data) if np.array(arr).ndim == 2 and np.array(full_labels[i]).ndim == 1]\n",
    "        full_data = [full_data[i] for i in valid_dim_indices]\n",
    "        full_labels = [full_labels[i] for i in valid_dim_indices] \n",
    "\n",
    "        # Trova gli indici degli elementi di full_data con seconda dimensione uguale a 200\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.shape[1] == 200]\n",
    "\n",
    "        # Applica la selezione a full_data e full_labels\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "\n",
    "        # Chunking \n",
    "        \n",
    "        import pyarrow as pa\n",
    "        import pyarrow.parquet as pq\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        os.makedirs('processed_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        chunk_size = 50000  # puoi modificare la dimensione del chunk se necessario\n",
    "        sample_fraction = 0.2\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean))\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            \n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "\n",
    "            # Salva ogni chunk come file parquet separato\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'processed_data/binary_classification/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'processed_data/binary_classification/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "\n",
    "        # Imposta la frazione di dati da campionare per ogni chunk (es: 0.2 per il 20%)\n",
    "        \n",
    "            _, X_sampled, _, y_sampled = train_test_split(\n",
    "                chunk_data, chunk_labels, \n",
    "                test_size=sample_fraction, \n",
    "                stratify=chunk_labels, \n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "        print(f\"Shape of sampled data: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "        \n",
    "        # Crea la cartella processed_data se non esiste\n",
    "        os.makedirs('processed_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        # Salva fit_data in formato parquet ottimizzato\n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'processed_data/binary_classification/fit_data_stratified_binary.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            #data_page_size=2097152\n",
    "        )\n",
    "\n",
    "        # Salva labels in formato parquet ottimizzato\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'processed_data/binary_classification/labels_stratified_binary.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            #data_page_size=2097152\n",
    "        )\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.final_model = None\n",
    "\n",
    "    def train(self, model_search=True):\n",
    "        os.environ[\"PYCARET_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "        if model_search:\n",
    "            scaler = MinMaxScaler()\n",
    "            fit_data_scaled = scaler.fit_transform(self.data)\n",
    "            clf = setup(data=fit_data_scaled,\n",
    "                        target=self.labels,\n",
    "                        pca=True,\n",
    "                        pca_method='incremental',\n",
    "                        use_gpu=True\n",
    "                        )\n",
    "            best_models = compare_models(n_select=5)\n",
    "            best_model = best_models[0]\n",
    "            print(f\"Il modello migliore è: {best_model}\")\n",
    "            tuned_model = tune_model(best_model,\n",
    "                                    optimize='Accuracy',\n",
    "                                    n_iter=10,\n",
    "                                    search_library='optuna',\n",
    "                                    search_algorithm='tpe',\n",
    "                                    choose_better=True)\n",
    "            print(\"Valutazione del modello ottimizzato:\")\n",
    "            evaluate_model(tuned_model)\n",
    "            best_params = tuned_model.get_params()\n",
    "            print(\"Migliori iperparametri trovati:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "            self.final_model = finalize_model(tuned_model)\n",
    "            save_model(self.final_model, 'best_classification_model')\n",
    "            # loaded_model = load_model('best_classification_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "131abe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'C:/Users/alessandro.togni/Desktop/great/sample_data/'\n",
    "\n",
    "\n",
    "read_from_backup = True\n",
    "if read_from_backup:\n",
    "    #import polars as pl\n",
    "\n",
    "    # Leggi i file parquet con polars\n",
    "    fit_data_pl = pd.read_parquet('./processed_data/binary_classification/fit_data_stratified_binary.parquet')\n",
    "    labels_pl = pd.read_parquet('./processed_data/binary_classification/labels_stratified_binary.parquet')\n",
    "\n",
    "    # Trasforma in numpy array\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()\n",
    "else:\n",
    "    preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR)\n",
    "    fit_data, labels = preprocessor.process_all_files(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d57029eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1796779, 1796779)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fit_data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13720236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Esegui il campionamento stratificato, prendi 1-test_size%  del campione\n",
    "fit_data_sample, _, labels_sample, _ = train_test_split(\n",
    "    fit_data, labels, test_size=0.50, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbcbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fit_data_sample), len(labels_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18243fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_trainer = ModelTrainer(data=fit_data, labels=labels)\n",
    "model_trainer.train(model_search=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
