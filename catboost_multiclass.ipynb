{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c33ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "from scipy.fft import fft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624d7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_ddm_features_MORE(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"\n",
    "        Estrae features dettagliate da raw_counts DDM (shape: n_samples x 200)\n",
    "        \"\"\"\n",
    "\n",
    "        def gini(array):\n",
    "            \"\"\"Calcola il coefficiente di Gini (disuguaglianza)\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))\n",
    "        \n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float32) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. Statistiche base\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = gini(x)\n",
    "\n",
    "            # 2. Posizionali\n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentazione\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "            \n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivate e cambiamenti\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelazioni (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT (spettro frequenze)\n",
    "            spectrum = np.abs(fft(x))\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  # simmetrico\n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "            features.append(f)\n",
    "\n",
    "        return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949592d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import uniform, randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CatBoostBinaryPipeline:\n",
    "    def __init__(self, fit_data, labels_data, test_size=0.2, val_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Pipeline completa per classificatore binario CatBoost\n",
    "        \n",
    "        Args:\n",
    "            fit_data: DataFrame o array con le features\n",
    "            labels_data: Serie o array con le etichette\n",
    "            test_size: dimensione del test set\n",
    "            val_size: dimensione del validation set (dal training set)\n",
    "            random_state: seed per riproducibilit√†\n",
    "        \"\"\"\n",
    "        self.fit_data = fit_data\n",
    "        self.labels_data = labels_data\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Modelli e risultati\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.train_scores = {}\n",
    "        self.val_scores = {}\n",
    "        self.test_scores = {}\n",
    "        \n",
    "        # Dati preprocessati\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        # Feature categoriche\n",
    "        self.categorical_features = []\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocessa i dati e li divide in train/val/test\"\"\"\n",
    "        print(\"üîÑ Preprocessing dei dati...\")\n",
    "        \n",
    "        # Converti in DataFrame se necessario\n",
    "        if not isinstance(self.fit_data, pd.DataFrame):\n",
    "            self.fit_data = pd.DataFrame(self.fit_data)\n",
    "        \n",
    "        # Identifica feature categoriche\n",
    "        self.categorical_features = []\n",
    "        for col in self.fit_data.columns:\n",
    "            if self.fit_data[col].dtype == 'object' or self.fit_data[col].dtype.name == 'category':\n",
    "                self.categorical_features.append(col)\n",
    "        \n",
    "        print(f\"üìä Feature categoriche identificate: {self.categorical_features}\")\n",
    "        \n",
    "        # Split train-test\n",
    "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
    "            self.fit_data, self.labels_data, \n",
    "            test_size=self.test_size, \n",
    "            stratify=self.labels_data,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Split train-validation\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=self.val_size,\n",
    "            stratify=y_temp,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"üìà Dimensioni datasets:\")\n",
    "        print(f\"  - Train: {self.X_train.shape}\")\n",
    "        print(f\"  - Validation: {self.X_val.shape}\")\n",
    "        print(f\"  - Test: {self.X_test.shape}\")\n",
    "        \n",
    "        # Statistiche sulle classi\n",
    "        print(f\"üìä Distribuzione classi (Train): {pd.Series(self.y_train).value_counts().to_dict()}\")\n",
    "        \n",
    "    def define_search_space(self):\n",
    "        \"\"\"Definisce lo spazio di ricerca per gli iperparametri\"\"\"\n",
    "        param_distributions = {\n",
    "            'iterations': randint(100, 1000),\n",
    "            'learning_rate': uniform(0.01, 0.2),\n",
    "            'depth': randint(3, 10),\n",
    "            'l2_leaf_reg': uniform(1, 10),\n",
    "            'border_count': randint(32, 255),\n",
    "            'bagging_temperature': uniform(0, 1),\n",
    "            'random_strength': uniform(0, 10),\n",
    "            'subsample': uniform(0.6, 0.4),  # 0.6 to 1.0\n",
    "        }\n",
    "        return param_distributions\n",
    "    \n",
    "    def hyperparameter_search(self, n_iter=50, cv_folds=5, scoring='roc_auc'):\n",
    "        \"\"\"\n",
    "        Esegue la ricerca degli iperparametri\n",
    "        \n",
    "        Args:\n",
    "            n_iter: numero di iterazioni per RandomizedSearchCV\n",
    "            cv_folds: numero di fold per cross-validation\n",
    "            scoring: metrica di scoring\n",
    "        \"\"\"\n",
    "        print(f\"üîç Avvio ricerca iperparametri ({n_iter} iterazioni, {cv_folds}-fold CV)...\")\n",
    "        \n",
    "        # Modello base\n",
    "        base_model = CatBoostClassifier(\n",
    "            random_state=self.random_state,\n",
    "            verbose=False,\n",
    "            cat_features=self.categorical_features,\n",
    "            eval_metric='AUC',\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        \n",
    "        # Spazio di ricerca\n",
    "        param_distributions = self.define_search_space()\n",
    "        \n",
    "        # Cross-validation stratificata\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # RandomizedSearchCV\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            random_state=self.random_state,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit del modello\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Salva i migliori parametri\n",
    "        self.best_params = random_search.best_params_\n",
    "        self.best_model = random_search.best_estimator_\n",
    "        \n",
    "        print(f\"‚úÖ Ricerca completata!\")\n",
    "        print(f\"üèÜ Migliore score CV: {random_search.best_score_:.4f}\")\n",
    "        print(f\"üéØ Migliori parametri:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "            \n",
    "    def train_final_model(self):\n",
    "        \"\"\"Addestra il modello finale con i migliori iperparametri\"\"\"\n",
    "        print(\"üöÄ Addestramento modello finale...\")\n",
    "        \n",
    "        if self.best_params is None:\n",
    "            print(\"‚ö†Ô∏è  Nessun iperparametro trovato. Uso parametri di default.\")\n",
    "            self.best_model = CatBoostClassifier(\n",
    "                random_state=self.random_state,\n",
    "                verbose=False,\n",
    "                cat_features=self.categorical_features,\n",
    "                eval_metric='AUC'\n",
    "            )\n",
    "        \n",
    "        # Pool per CatBoost (migliori performance)\n",
    "        train_pool = Pool(\n",
    "            data=self.X_train,\n",
    "            label=self.y_train,\n",
    "            cat_features=self.categorical_features\n",
    "        )\n",
    "        \n",
    "        val_pool = Pool(\n",
    "            data=self.X_val,\n",
    "            label=self.y_val,\n",
    "            cat_features=self.categorical_features\n",
    "        )\n",
    "        \n",
    "        # Addestramento con early stopping\n",
    "        self.best_model.fit(\n",
    "            train_pool,\n",
    "            eval_set=val_pool,\n",
    "            verbose=100,\n",
    "            plot=False\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Addestramento completato!\")\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Valuta il modello su tutti i dataset\"\"\"\n",
    "        print(\"üìä Valutazione del modello...\")\n",
    "        \n",
    "        datasets = {\n",
    "            'train': (self.X_train, self.y_train),\n",
    "            'validation': (self.X_val, self.y_val),\n",
    "            'test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        all_scores = {}\n",
    "        \n",
    "        for name, (X, y) in datasets.items():\n",
    "            # Predizioni\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            y_pred_proba = self.best_model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            # Metriche\n",
    "            scores = {\n",
    "                'accuracy': accuracy_score(y, y_pred),\n",
    "                'precision': precision_score(y, y_pred),\n",
    "                'recall': recall_score(y, y_pred),\n",
    "                'f1': f1_score(y, y_pred),\n",
    "                'roc_auc': roc_auc_score(y, y_pred_proba)\n",
    "            }\n",
    "            \n",
    "            all_scores[name] = scores\n",
    "            \n",
    "            print(f\"\\nüìà Metriche {name.upper()}:\")\n",
    "            for metric, value in scores.items():\n",
    "                print(f\"  - {metric.upper()}: {value:.4f}\")\n",
    "        \n",
    "        # Salva i risultati\n",
    "        self.train_scores = all_scores['train']\n",
    "        self.val_scores = all_scores['validation']\n",
    "        self.test_scores = all_scores['test']\n",
    "        \n",
    "        return all_scores\n",
    "    \n",
    "    def plot_feature_importance(self, top_n=20):\n",
    "        \"\"\"Visualizza l'importanza delle feature\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"‚ö†Ô∏è  Modello non ancora addestrato!\")\n",
    "            return\n",
    "            \n",
    "        # Ottieni feature importance\n",
    "        feature_names = self.X_train.columns if hasattr(self.X_train, 'columns') else [f'feature_{i}' for i in range(self.X_train.shape[1])]\n",
    "        importance = self.best_model.get_feature_importance()\n",
    "        \n",
    "        # Crea DataFrame per plotting\n",
    "        fi_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False).head(top_n)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=fi_df, x='importance', y='feature')\n",
    "        plt.title(f'Top {top_n} Feature Importance')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_confusion_matrices(self):\n",
    "        \"\"\"Visualizza le confusion matrix per tutti i dataset\"\"\"\n",
    "        datasets = {\n",
    "            'Train': (self.X_train, self.y_train),\n",
    "            'Validation': (self.X_val, self.y_val),\n",
    "            'Test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        for idx, (name, (X, y)) in enumerate(datasets.items()):\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            cm = confusion_matrix(y, y_pred)\n",
    "            \n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx], cmap='Blues')\n",
    "            axes[idx].set_title(f'Confusion Matrix - {name}')\n",
    "            axes[idx].set_xlabel('Predicted')\n",
    "            axes[idx].set_ylabel('Actual')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_classification_reports(self):\n",
    "        \"\"\"Stampa i classification report dettagliati\"\"\"\n",
    "        datasets = {\n",
    "            'Train': (self.X_train, self.y_train),\n",
    "            'Validation': (self.X_val, self.y_val),\n",
    "            'Test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        for name, (X, y) in datasets.items():\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            print(f\"\\nüìã CLASSIFICATION REPORT - {name.upper()}\")\n",
    "            print(\"=\" * 50)\n",
    "            print(classification_report(y, y_pred))\n",
    "    \n",
    "    def run_complete_pipeline(self, n_iter=50, cv_folds=5):\n",
    "        \"\"\"Esegue la pipeline completa\"\"\"\n",
    "        print(\"üöÄ AVVIO PIPELINE COMPLETA CATBOOST\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Preprocessing\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # 2. Ricerca iperparametri\n",
    "        self.hyperparameter_search(n_iter=n_iter, cv_folds=cv_folds)\n",
    "        \n",
    "        # 3. Addestramento finale\n",
    "        self.train_final_model()\n",
    "        \n",
    "        # 4. Valutazione\n",
    "        scores = self.evaluate_model()\n",
    "        \n",
    "        # 5. Report dettagliati\n",
    "        self.get_classification_reports()\n",
    "        \n",
    "        # 6. Visualizzazioni\n",
    "        self.plot_feature_importance()\n",
    "        self.plot_confusion_matrices()\n",
    "        \n",
    "        print(\"\\nüéâ PIPELINE COMPLETATA CON SUCCESSO!\")\n",
    "        print(f\"üèÜ Performance finale (Test Set): AUC = {self.test_scores['roc_auc']:.4f}\")\n",
    "        \n",
    "        return self.best_model, self.best_params, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR= 'D:/data/RONGOWAI_L1_SDR_V1.0/'\n",
    "\n",
    "read_from_backup = True\n",
    "if read_from_backup:\n",
    "    #import polars as pl\n",
    "\n",
    "    # Leggi i file parquet con polars\n",
    "    fit_data_pl = pd.read_parquet('processed_data/multiclass/fit_data_multiclass.parquet')\n",
    "    labels_pl = pd.read_parquet('processed_data/multiclass/labels_multiclass.parquet')\n",
    "\n",
    "    # Trasforma in numpy array\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b35f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting DDM features:  11%|‚ñà         | 190424/1795932 [02:03<17:20, 1542.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m features_extractor \u001b[38;5;241m=\u001b[39m DDMFeatureExtractor()\n\u001b[1;32m----> 2\u001b[0m ddm_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_ddm_features_MORE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mDDMFeatureExtractor.create_ddm_features_MORE\u001b[1;34m(self, fit_data)\u001b[0m\n\u001b[0;32m     31\u001b[0m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkurtosis\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kurtosis(x)\n\u001b[0;32m     32\u001b[0m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m entropy(x)\n\u001b[1;32m---> 33\u001b[0m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 2. Posizionali\u001b[39;00m\n\u001b[0;32m     36\u001b[0m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(x)\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mDDMFeatureExtractor.create_ddm_features_MORE.<locals>.gini\u001b[1;34m(array)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgini\u001b[39m(array):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calcola il coefficiente di Gini (disuguaglianza)\"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (np\u001b[38;5;241m.\u001b[39msum((\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m index \u001b[38;5;241m-\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m array)) \u001b[38;5;241m/\u001b[39m (array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(array))\n",
      "File \u001b[1;32mc:\\Users\\Alessandro\\anaconda3\\envs\\great_clf\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1017\u001b[0m, in \u001b[0;36msort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1016\u001b[0m     a \u001b[38;5;241m=\u001b[39m asanyarray(a)\u001b[38;5;241m.\u001b[39mcopy(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1017\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_extractor = DDMFeatureExtractor()\n",
    "ddm_features = features_extractor.create_ddm_features_MORE(fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af08d7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fit_data_with_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([\u001b[43mfit_data\u001b[49m, ddm_features\u001b[38;5;241m.\u001b[39mvalues])\n\u001b[0;32m      2\u001b[0m fit_data_with_features\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fit_data' is not defined"
     ]
    }
   ],
   "source": [
    "fit_data_with_features = np.hstack([fit_data, ddm_features.values])\n",
    "fit_data_with_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = CatBoostBinaryPipeline(fit_data, labels)\n",
    "best_model, best_params, scores = pipeline.run_complete_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
