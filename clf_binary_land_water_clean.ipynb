{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f928518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "from scipy.fft import fft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3221e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurfaceTypeUtils:\n",
    "    surface_type_dict = {\n",
    "        -1: \"Ocean\",\n",
    "        0: \"NaN\",\n",
    "        1: \"Artifical\",\n",
    "        2: \"Barely vegetated\",\n",
    "        3: \"Inland water\",\n",
    "        4: \"Crop\",\n",
    "        5: \"Grass\",\n",
    "        6: \"Shrub\",\n",
    "        7: \"Forest\"\n",
    "    }\n",
    "    ddm_antennas = {\n",
    "        0: 'None',\n",
    "        1: 'Zenith',\n",
    "        2: 'LHCP',\n",
    "        3: 'RHCP',\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ff6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeoUtils:\n",
    "    def __init__(self, world_shapefile_path):\n",
    "        self.world = gpd.read_file(world_shapefile_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def is_land(self, lat, lon):\n",
    "        point = Point(lon, lat)\n",
    "        return any(self.world.contains(point))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_ocean_and_land(lst):\n",
    "        has_ocean = -1 in lst\n",
    "        has_land = any(1 <= num <= 7 for num in lst)\n",
    "        return has_ocean and has_land\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3489f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetCDFPreprocessor:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        raw_counts = np.array(f.variables['raw_counts'])\n",
    "        # Calcolo distanza tra il punto speculare e l'aereo\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        # Seleziono gli indici dove sp_rx_gain_copol > 5, sp_rx_gain_xpol > 5 e ddm_snr > 0 e distanza tra punto speculare e antenna > 2000 e < 10000\n",
    "\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "        \n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "\n",
    "        del output_array\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "        label_data = [1 if surface_type in np.arange(1, 8) else 0 for surface_type in surface_types_unravelled]\n",
    "        label_data = [label_data[lab] for lab in range(len(label_data)) if lab in keep_indices]\n",
    "        return fit_data, label_data\n",
    "\n",
    "\n",
    "    def process_all_files(self, chunk_size = int, sample_fraction = float, remove_chunks= bool):\n",
    "        \n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        #counter = 0\n",
    "        for file_name in tqdm(self.netcdf_file_list, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                continue\n",
    "            try:\n",
    "                    f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                    data, labels = self.preprocess(f)\n",
    "                    full_data.append(data)\n",
    "                    full_labels.append(labels)\n",
    "            except Exception as e:\n",
    "                    print(f\"Error processing file {file_name}: {e}\")\n",
    "                    continue\n",
    "            #counter += 1\n",
    "            #if counter == 70:\n",
    "                #break  # Limita il numero di file processati per test\n",
    "                \n",
    "        # Trova gli indici degli elementi di full_data con seconda dimensione uguale a 200\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "\n",
    "        # Applica la selezione a full_data e full_labels\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "        \n",
    "        # Chunking \n",
    "        \n",
    "        os.makedirs('processed_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        chunk_size = chunk_size # dimensione del chunk in numero di campioni\n",
    "        sample_fraction = sample_fraction # frazione di dati da campionare per ogni chunk\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean))\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            \n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "\n",
    "            # Salva ogni chunk come file parquet separato\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'processed_data/binary_classification/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'processed_data/binary_classification/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            \n",
    "        # Imposta la frazione di dati da campionare per ogni chunk (es: 0.2 per il 20%)\n",
    "        \n",
    "            _, X_sampled, _, y_sampled = train_test_split(\n",
    "                chunk_data, chunk_labels, \n",
    "                test_size=sample_fraction, \n",
    "                stratify=chunk_labels, \n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            \n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        del full_data, full_labels\n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "\n",
    "        del full_data_sampled, full_labels_sampled\n",
    "        print(f\"Shape of sampled data after chunking and sampling: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels after chunking and sampling: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "        \n",
    "        # Crea la cartella processed_data se non esiste\n",
    "        os.makedirs('processed_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        # Salva fit_data in formato parquet ottimizzato\n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'processed_data/binary_classification/fit_data_stratified_binary.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Salva labels in formato parquet ottimizzato\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'processed_data/binary_classification/labels_stratified_binary.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "        del fit_data_df, labels_df, table_fit, table_labels\n",
    "        \n",
    "        print(\"Data and labels saved in processed_data/binary_classification directory.\")\n",
    "        # Remove all chunk parquet files if flag is set\n",
    "        if remove_chunks:\n",
    "            try:\n",
    "                chunk_dir = 'processed_data/binary_classification'\n",
    "                for fname in os.listdir(chunk_dir):\n",
    "                    if fname.startswith('fit_data_chunk_') or fname.startswith('labels_chunk_'):\n",
    "                        os.remove(os.path.join(chunk_dir, fname))\n",
    "                print(\"All chunk files removed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing chunk files: {e}\")\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89cd8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_ddm_features_MORE(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"\n",
    "        Estrae features dettagliate da raw_counts DDM (shape: n_samples x 200)\n",
    "        \"\"\"\n",
    "\n",
    "        def gini(array):\n",
    "            \"\"\"Calcola il coefficiente di Gini (disuguaglianza)\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))\n",
    "        \n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float32) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. Statistiche base\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = gini(x)\n",
    "\n",
    "            # 2. Posizionali\n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentazione\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "            \n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivate e cambiamenti\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelazioni (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT (spettro frequenze)\n",
    "            spectrum = np.abs(fft(x))\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  # simmetrico\n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "            features.append(f)\n",
    "\n",
    "        return pd.DataFrame(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0511771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.final_model = None\n",
    "\n",
    "    def plot_feature_importance(self, feature_names=None, top_n=20):\n",
    "        \"\"\"\n",
    "        Plotta la feature importance del modello finale se disponibile.\n",
    "        Args:\n",
    "            feature_names (list or None): nomi delle feature, se disponibili.\n",
    "            top_n (int): mostra solo le top_n feature.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        importances = None\n",
    "\n",
    "        if self.final_model is None:\n",
    "            print(\"Nessun modello finale disponibile.\")\n",
    "            return\n",
    "\n",
    "        # Tree-based models\n",
    "        if hasattr(self.final_model, \"feature_importances_\"):\n",
    "            importances = self.final_model.feature_importances_\n",
    "        # Linear models\n",
    "        elif hasattr(self.final_model, \"coef_\"):\n",
    "            importances = self.final_model.coef_.ravel()\n",
    "        else:\n",
    "            print(\"Il modello non supporta la feature importance.\")\n",
    "            return\n",
    "\n",
    "        if feature_names is None:\n",
    "            feature_names = [f\"f{i}\" for i in range(len(importances))]\n",
    "\n",
    "        # Ordina per importanza\n",
    "        indices = importances.argsort()[::-1][:top_n]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(top_n), importances[indices][::-1], align=\"center\")\n",
    "        plt.yticks(range(top_n), [feature_names[i] for i in indices][::-1])\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.title(\"Top Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, model_search=True):\n",
    "        os.environ[\"PYCARET_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "        if model_search:\n",
    "            scaler = MinMaxScaler()\n",
    "            fit_data_scaled = scaler.fit_transform(self.data)\n",
    "            clf = setup(data=fit_data_scaled,\n",
    "                        target=self.labels,\n",
    "                        pca=True,\n",
    "                        pca_method='incremental',\n",
    "                        use_gpu=True,\n",
    "                        feature_selection=True\n",
    "                        )\n",
    "            best_models = compare_models(n_select=5)\n",
    "            best_model = best_models[0]\n",
    "            print(f\"Il modello migliore è: {best_model}\")\n",
    "            tuned_model = tune_model(best_model,\n",
    "                                    optimize='Accuracy',\n",
    "                                    n_iter=10,\n",
    "                                    search_library='optuna',\n",
    "                                    search_algorithm='tpe',\n",
    "                                    choose_better=True)\n",
    "            print(\"Valutazione del modello ottimizzato:\")\n",
    "            evaluate_model(tuned_model)\n",
    "            best_params = tuned_model.get_params()\n",
    "            print(\"Migliori iperparametri trovati:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "            self.final_model = finalize_model(tuned_model)\n",
    "            save_model(self.final_model, 'best_binary_classification_model')\n",
    "            # loaded_model = load_model('best_classification_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54afc5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d804d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 2475/4943 [16:45<20:30,  2.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file 20231104-094532_NZWB-NZAA_L1.nc: [Errno -101] NetCDF: HDF error: 'D:/data/RONGOWAI_L1_SDR_V1.0/20231104-094532_NZWB-NZAA_L1.nc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 2488/4943 [16:52<14:59,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file 20231107-134644_NZAA-NZKK_L1.nc: [Errno -101] NetCDF: HDF error: 'D:/data/RONGOWAI_L1_SDR_V1.0/20231107-134644_NZAA-NZKK_L1.nc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  87%|████████▋ | 4305/4943 [31:44<08:03,  1.32it/s]"
     ]
    }
   ],
   "source": [
    "ROOT_DIR= 'D:/data/RONGOWAI_L1_SDR_V1.0/'\n",
    "\n",
    "read_from_backup = False\n",
    "if read_from_backup:\n",
    "    #import polars as pl\n",
    "\n",
    "    # Leggi i file parquet con polars\n",
    "    fit_data_pl = pd.read_parquet('C:/Users/Alessandro/Desktop/great/rongowai/processed_data/binary_classification/fit_data_stratified_binary.parquet')\n",
    "    labels_pl = pd.read_parquet('C:/Users/Alessandro/Desktop/great/rongowai/processed_data/binary_classification/labels_stratified_binary.parquet')\n",
    "\n",
    "    # Trasforma in numpy array\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()\n",
    "else:\n",
    "    preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR)\n",
    "    fit_data, labels = preprocessor.process_all_files(chunk_size=250, sample_fraction=0.05, remove_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57029eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR)\n",
    "features_extractor = DDMFeatureExtractor()\n",
    "ddm_features = features_extractor.create_ddm_features_MORE(fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit_data_with_features = np.hstack([fit_data, ddm_features.values])\n",
    "fit_data_with_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlla infiniti o valori troppo grandi per float64\n",
    "mask_finite = np.isfinite(fit_data_with_features).all(axis=1) & (np.abs(fit_data_with_features) < np.finfo(np.float64).max).all(axis=1)\n",
    "\n",
    "fit_data_with_features_clean = fit_data_with_features[mask_finite]\n",
    "labels_clean = labels[mask_finite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18243fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(data=fit_data_with_features_clean, labels=labels_clean)\n",
    "model_trainer.train(model_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carica il modello finale addestrato da pycaret\n",
    "final_model = load_model('best_binary_classification_model')\n",
    "\n",
    "# Ottieni la feature importance se disponibile\n",
    "\n",
    "if hasattr(final_model, \"feature_importances_\"):\n",
    "    importances = final_model.feature_importances_\n",
    "elif hasattr(final_model, \"coef_\"):\n",
    "    importances = final_model.coef_.ravel()\n",
    "else:\n",
    "    print(\"Il modello non supporta la feature importance.\")\n",
    "    importances = None\n",
    "\n",
    "if importances is not None:\n",
    "    # Usa i nomi delle colonne di fit_data_with_features se disponibili\n",
    "    feature_names = [str(i) for i in range(fit_data_with_features.shape[1])]\n",
    "    top_n = 20\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(top_n), importances[indices][::-1], align=\"center\")\n",
    "    plt.yticks(range(top_n), [feature_names[i] for i in indices][::-1])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d959d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
