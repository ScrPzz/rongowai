{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee8083a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Union, List, Dict\n",
    "from dataclasses import dataclass, field\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis, entropy, randint, uniform, loguniform\n",
    "from scipy.fft import fft\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.calibration import calibration_curve\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0262064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MlFlow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\") # Check your MLflow server URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acd639e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @staticmethod\n",
    "    def gini(array):\n",
    "            \"\"\"Gini coefficient calculation\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))  \n",
    "      \n",
    "    def extract_ddm_features(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract features from DDM data.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float64) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. General statistics\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = self.gini(x)\n",
    "\n",
    "            # 2. Positional \n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentations in thirds\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "\n",
    "            # 3.1 Segmentations in windows of 5\n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivative statistics and differences\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelations (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT \n",
    "            spectrum = np.abs(fft(x)) # type: ignore\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  \n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "\n",
    "            features.append(f)\n",
    "        return features # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa19a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostPipeline:\n",
    "    def __init__(self, features_df, labels_df, test_size=0.2, random_state=42, \n",
    "                 experiment_name=\"xgboost_binary_classification\", model_save_dir=\"./models\", save_voter=True):\n",
    "        \n",
    "        self.features_df = features_df\n",
    "        self.labels_df = labels_df.values.ravel() if isinstance(labels_df, pd.DataFrame) else labels_df\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.cv_results = None\n",
    "        self.scaler = None\n",
    "        self.save_voter = save_voter\n",
    "\n",
    "        # MLflow setup\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.run_id = None\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(model_save_dir):\n",
    "            print(f\"Creating model save directory: {model_save_dir}\")\n",
    "            os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "        # Setup MLflow experiment\n",
    "        self._setup_mlflow_experiment()\n",
    "\n",
    "    def _setup_mlflow_experiment(self):\n",
    "        try:\n",
    "            experiment = mlflow.get_experiment_by_name(self.experiment_name)\n",
    "            if experiment is None:\n",
    "                mlflow.create_experiment(self.experiment_name)\n",
    "\n",
    "            mlflow.set_experiment(self.experiment_name)\n",
    "            print(f\"MLflow experiment '{self.experiment_name}' correctly set up.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in mlflow configuration: {e}\")\n",
    "\n",
    "    def _log_to_mlflow(self, key, value):\n",
    "        try:\n",
    "            if isinstance(value, (int, float)):\n",
    "                mlflow.log_metric(key, value)\n",
    "            else:\n",
    "                mlflow.log_param(key, value)\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging MLflow {key}: {e}\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Split data into train and test sets\"\"\"\n",
    "        print(\"=== Data preparation ===\")\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.features_df, self.labels_df, \n",
    "            test_size=self.test_size, \n",
    "            random_state=self.random_state,\n",
    "            stratify=self.labels_df\n",
    "        )\n",
    "\n",
    "        # Features scaling\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "\n",
    "        try:\n",
    "            mlflow.log_param(\"dataset_total_samples\", len(self.features_df))\n",
    "            mlflow.log_param(\"train_samples\", self.X_train.shape[0])\n",
    "            mlflow.log_param(\"test_samples\", self.X_test.shape[0])\n",
    "            mlflow.log_param(\"n_features\", self.X_train.shape[1])\n",
    "            mlflow.log_param(\"test_size\", self.test_size)\n",
    "            mlflow.log_param(\"random_state\", self.random_state)\n",
    "\n",
    "            class_distribution = pd.Series(self.y_train).value_counts(normalize=True)\n",
    "            for class_label, proportion in class_distribution.items():\n",
    "                mlflow.log_metric(f\"class_{class_label}_proportion\", proportion)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error mlflow: {e}\")\n",
    "\n",
    "        print(\"Dataset dimensions:\")\n",
    "        print(f\"Train set: {self.X_train.shape[0]} campioni\")\n",
    "        print(f\"Test set: {self.X_test.shape[0]} campioni\")\n",
    "        print(f\"Features: {self.X_train.shape[1]}\")\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(pd.Series(self.y_train).value_counts(normalize=True))\n",
    "\n",
    "    def hyperparameter_tuning(self, cv_folds=5, verbose=True, n_iter=250):\n",
    "        print(\"\\n=== HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "        self.param_distributions = {\n",
    "            'n_estimators': [500, 1000, 1500],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.001, 0.03, 0.005],\n",
    "            'subsample': uniform(0.4, 0.6),     \n",
    "            'colsample_bytree': uniform(0.4, 0.6), \n",
    "            #'colsample_bylevel': uniform(0.4, 0.6),\n",
    "            #'colsample_bynode': uniform(0.4, 0.6), \n",
    "            'gamma': loguniform(1e-8, 1),\n",
    "            'reg_alpha': loguniform(1e-8, 10),   \n",
    "            'reg_lambda': loguniform(1e-8, 10),\n",
    "            'min_child_weight': randint(1, 10),\n",
    "            #'scale_pos_weight': uniform(0.5, 5.0),\n",
    "            #'booster': ['gbtree', 'dart'],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            mlflow.log_param(\"cv_folds\", cv_folds)\n",
    "            mlflow.log_param(\"search_iterations\", n_iter)\n",
    "            mlflow.log_param(\"search_scoring\", \"roc_auc\")\n",
    "\n",
    "            # Log hyperparameter search space\n",
    "            for param, distribution in self.param_distributions.items():\n",
    "                if hasattr(distribution, 'args'):\n",
    "                    mlflow.log_param(f\"search_space_{param}\", str(distribution.args))\n",
    "                else:\n",
    "                    mlflow.log_param(f\"search_space_{param}\", str(distribution))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{e}\")\n",
    "\n",
    "        n_cores = multiprocessing.cpu_count()\n",
    "        self.n_jobs = np.floor(0.75 * n_cores).astype(int)\n",
    "        print(f\"Using {self.n_jobs} cores for hyperparameter tuning (75% of {n_cores} total cores).\")\n",
    "\n",
    "        # Base model\n",
    "        base_model = xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            tree_method='hist',\n",
    "            use_label_encoder=False,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=self.n_jobs\n",
    "        )\n",
    "\n",
    "        # Stratified K-Fold\n",
    "        stratified_kfold = StratifiedKFold(\n",
    "            n_splits=cv_folds, \n",
    "            shuffle=True, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # RandomizedSearchCV\n",
    "        self.grid_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=self.param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=stratified_kfold,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=self.n_jobs,\n",
    "            verbose=2,\n",
    "            random_state=self.random_state,\n",
    "            return_train_score=True\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(\"Searching for hyperparameters!\")\n",
    "        self.grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "\n",
    "        self.best_params = self.grid_search.best_params_\n",
    "        self.cv_results = pd.DataFrame(self.grid_search.cv_results_)\n",
    "\n",
    "        try:\n",
    "            mlflow.log_metric(\"best_cv_score\", self.grid_search.best_score_)\n",
    "\n",
    "            for param, value in self.best_params.items():\n",
    "                mlflow.log_param(f\"best_{param}\", value)\n",
    "\n",
    "            cv_results_path = os.path.join(self.model_save_dir, \"cv_results.csv\")\n",
    "            self.cv_results.to_csv(cv_results_path, index=False)\n",
    "            mlflow.log_artifact(cv_results_path, \"hyperparameter_search\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in mlflow logging: {e}\")\n",
    "\n",
    "        print(\"\\nBest hyperparameters found:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest  ROC-AUC (CV): {self.grid_search.best_score_:.4f}\")\n",
    "\n",
    "        self._plot_hyperparameter_results()\n",
    "\n",
    "    def train_final_model(self):\n",
    "        \"\"\"Finalize the model training with the best hyperparameters found\"\"\"\n",
    "        print(\"\\n=== Finalizing model ===\")\n",
    "\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            **self.best_params,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=self.n_jobs\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Split a validation set from the training data for evaluation\n",
    "        print(\"Splitting training data for final model training...\")\n",
    "        self.X_train_final, self.X_eval, self.y_train_final, self.y_eval = train_test_split(\n",
    "            self.X_train_scaled, self.y_train,\n",
    "            test_size=0.1,  # 10% of training data for evaluation\n",
    "            random_state=self.random_state,\n",
    "            stratify=self.y_train\n",
    "        )\n",
    "\n",
    "        eval_set = [(self.X_train_final, self.y_train_final), (self.X_eval, self.y_eval)]\n",
    "\n",
    "        self.model.fit(\n",
    "            self.X_train_final, self.y_train_final,\n",
    "            eval_set=eval_set,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        self.y_pred = self.model.predict(self.X_test_scaled)\n",
    "        self.y_pred_proba = self.model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, self.y_pred),\n",
    "            'precision': precision_score(self.y_test, self.y_pred),\n",
    "            'recall': recall_score(self.y_test, self.y_pred),\n",
    "            'f1_score': f1_score(self.y_test, self.y_pred),\n",
    "            'roc_auc': auc(*roc_curve(self.y_test, self.y_pred_proba)[:2]),\n",
    "            'average_precision': average_precision_score(self.y_test, self.y_pred_proba)\n",
    "        }\n",
    "\n",
    "        # Log metrics MLflow\n",
    "        try:\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"test_{metric_name}\", metric_value)\n",
    "\n",
    "            \n",
    "            mlflow.log_metric(\"n_estimators_used\", self.model.n_estimators)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging metrics: {e}\")\n",
    "\n",
    "        print(\"\\nMetrics on test set:\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            print(f\"{metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "\n",
    "    \n",
    "    def save_model_and_scaler(self, model_name=None):\n",
    "        print(\"\\n=== Saving finalized model and scaler ===\")\n",
    "\n",
    "        if model_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_name = f\"xgboost_model_{timestamp}\"\n",
    "\n",
    "        model_path = os.path.join(self.model_save_dir, f\"{model_name}.joblib\")\n",
    "        scaler_path = os.path.join(self.model_save_dir, f\"{model_name}_scaler.joblib\")\n",
    "        metadata_path = os.path.join(self.model_save_dir, f\"{model_name}_metadata.json\")\n",
    "\n",
    "        try:\n",
    "            # Save model and scaler\n",
    "            joblib.dump(self.model, model_path)\n",
    "            print(f\"Model saved in: {model_path}\")\n",
    "            joblib.dump(self.scaler, scaler_path)\n",
    "            print(f\"Scaler saved in: {scaler_path}\")\n",
    "                \n",
    "            # Save model and scaler to voters directory\n",
    "            if self.save_voter:\n",
    "                print(\"Saving voters...\")\n",
    "                joblib.dump(self.model, 'models/voters/geok_xgboost_voter.joblib')\n",
    "                joblib.dump(self.scaler, 'models/voters/geok_xgboost_scaler.joblib')\n",
    "\n",
    "            metadata = {\n",
    "                'model_name': model_name,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'best_params': self.best_params,\n",
    "                'cv_score': float(self.grid_search.best_score_),\n",
    "                'test_metrics': {\n",
    "                    'accuracy': float(accuracy_score(self.y_test, self.y_pred)),\n",
    "                    'precision': float(precision_score(self.y_test, self.y_pred)),\n",
    "                    'recall': float(recall_score(self.y_test, self.y_pred)),\n",
    "                    'f1_score': float(f1_score(self.y_test, self.y_pred)),\n",
    "                    'roc_auc': float(auc(*roc_curve(self.y_test, self.y_pred_proba)[:2]))\n",
    "                },\n",
    "                'feature_names': list(self.features_df.columns),\n",
    "                'n_features': len(self.features_df.columns),\n",
    "                'train_samples': len(self.y_train),\n",
    "                'test_samples': len(self.y_test)\n",
    "            }\n",
    "\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            print(f\" Metadata saved: {metadata_path}\")\n",
    "\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                signature = infer_signature(self.X_train_scaled, self.y_pred_proba)\n",
    "                mlflow.xgboost.log_model(\n",
    "                    self.model, \n",
    "                    \"geok_xgboost_model\",\n",
    "                    signature=signature\n",
    "                )\n",
    "                \n",
    "                mlflow.log_artifact(scaler_path, \"preprocessing\")\n",
    "                mlflow.log_artifact(metadata_path, \"model_info\")\n",
    "\n",
    "                mlflow.log_param(\"model_save_path\", model_path)\n",
    "                mlflow.log_param(\"scaler_save_path\", scaler_path)\n",
    "                mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "                print(\"Model and scaler logged to MLflow successfully.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error logging MLflow: {e}\")\n",
    "\n",
    "            return model_path, scaler_path, metadata_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model_and_scaler(model_path, scaler_path):\n",
    "        try:\n",
    "            model = joblib.load(model_path)\n",
    "            scaler = joblib.load(scaler_path)\n",
    "            print(f\"Model loaded from: {model_path}\")\n",
    "            print(f\"Scaler loaded from: {scaler_path}\")\n",
    "            return model, scaler\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading scaler or model: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def predict_new_data(self, new_data, model_path=None, scaler_path=None):\n",
    "\n",
    "        if model_path and scaler_path:\n",
    "            model, scaler = self.load_model_and_scaler(model_path, scaler_path)\n",
    "        else:\n",
    "            model, scaler = self.model, self.scaler\n",
    "\n",
    "        if model is None or scaler is None:\n",
    "            print(\"Model or scaler not loaded. Cannot predict.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "            predictions = model.predict(new_data_scaled)\n",
    "            probabilities = model.predict_proba(new_data_scaled)[:, 1]\n",
    "\n",
    "            return predictions, probabilities\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    \n",
    "    def plot_all_visualizations(self):\n",
    "        print(\"\\n=== Data Viz ===\")\n",
    "\n",
    "        try:\n",
    "            # Crea directory temporanea per i grafici\n",
    "            plots_dir = os.path.join(self.model_save_dir, \"plots\")\n",
    "            os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "            # 1. Feature Importance\n",
    "            self._plot_feature_importance(save_dir=plots_dir)\n",
    "\n",
    "            # 2. ROC\n",
    "            self._plot_roc_curve(save_dir=plots_dir)\n",
    "\n",
    "            # 3. Precision-Recall\n",
    "            self._plot_pr_curve(save_dir=plots_dir)\n",
    "\n",
    "            # 4. Confusion Matrix\n",
    "            self._plot_confusion_matrix(save_dir=plots_dir)\n",
    "\n",
    "            # 5. Predicted proba Distribution\n",
    "            self._plot_probability_distribution(save_dir=plots_dir)\n",
    "\n",
    "            # 6. Learning Curves\n",
    "            self._plot_learning_curves(save_dir=plots_dir)\n",
    "\n",
    "            # 7. Calibration Plot\n",
    "            self._plot_calibration_curve(save_dir=plots_dir)\n",
    "\n",
    "            # 8. Classification Report Heatmap\n",
    "            self._plot_classification_report(save_dir=plots_dir)\n",
    "\n",
    "            try:\n",
    "                for plot_file in os.listdir(plots_dir):\n",
    "                    if plot_file.endswith('.png'):\n",
    "                        mlflow.log_artifact(os.path.join(plots_dir, plot_file), \"plots\")\n",
    "                print(\"Plots saved on MLflow\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving plots on MlFlow: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating plots {e}\")\n",
    "\n",
    "    def _plot_hyperparameter_results(self, save_dir=None):\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        #top_results = self.cv_results.nlargest(20, 'mean_test_score')\n",
    "\n",
    "        # 1. Grid Search Convergence\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(range(len(self.cv_results)), self.cv_results['mean_test_score'])\n",
    "        ax.axhline(y=self.grid_search.best_score_, color='r', linestyle='--', label='Best Score')\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('ROC-AUC Score')\n",
    "        ax.set_title('Convergenza della Grid Search')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Relative importance of parameters\n",
    "        ax = axes[0, 1]\n",
    "        param_importance = {}\n",
    "        for param in self.best_params.keys():\n",
    "            col = f'param_{param}'\n",
    "            if col in self.cv_results.columns: \n",
    "                if self.cv_results[col].dtype == 'object':\n",
    "                    unique_vals = self.cv_results[col].nunique()\n",
    "                    param_importance[param] = unique_vals / len(self.cv_results)\n",
    "                else:\n",
    "                    correlation = self.cv_results[[col, 'mean_test_score']].corr().iloc[0, 1]\n",
    "                    if not np.isnan(correlation):\n",
    "                        param_importance[param] = abs(correlation)\n",
    "\n",
    "        if param_importance:\n",
    "            pd.Series(param_importance).sort_values().plot(kind='barh', ax=ax)\n",
    "            ax.set_title('Relative Importance ')\n",
    "            ax.set_xlabel('Importance score')\n",
    "\n",
    "        # 3. Best Score Distribution\n",
    "        ax = axes[1, 0]\n",
    "        ax.hist(self.cv_results['mean_test_score'], bins=30, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(self.grid_search.best_score_, color='red', linestyle='--', \n",
    "                  label=f'Best: {self.grid_search.best_score_:.4f}')\n",
    "        ax.set_xlabel('ROC-AUC Score')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Scores CV distribution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Training Time vs Performance\n",
    "        ax = axes[1, 1]\n",
    "        if 'mean_fit_time' in self.cv_results.columns:\n",
    "            scatter = ax.scatter(self.cv_results['mean_fit_time'], \n",
    "                               self.cv_results['mean_test_score'],\n",
    "                               c=self.cv_results['mean_test_score'],\n",
    "                               cmap='viridis', alpha=0.6)\n",
    "            ax.set_xlabel('Training time (seconds)')\n",
    "            ax.set_ylabel('ROC-AUC Score')\n",
    "            ax.set_title('Trade-off Time vs Performance')\n",
    "            plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'hyperparameter_results.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_feature_importance(self, save_dir=None):\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "        # 1. XGBoost native importance\n",
    "        ax = axes[0]\n",
    "        plot_importance(self.model, max_num_features=20, ax=ax, importance_type='weight')\n",
    "        ax.set_title('Feature Importance - Weight (Usage frequency)')\n",
    "\n",
    "        # 2. Gain importance\n",
    "        ax = axes[1]\n",
    "        plot_importance(self.model, max_num_features=20, ax=ax, importance_type='gain')\n",
    "        ax.set_title('Feature Importance - Gain (Average gain of splits)')\n",
    "\n",
    "        # 3. Cover importance\n",
    "        ax = axes[2]\n",
    "        plot_importance(self.model, max_num_features=20, ax=ax, importance_type='cover')\n",
    "        ax.set_title('Feature Importance - Cover (Relative quantity of observations)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'feature_importance_types.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': self.features_df.columns,\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "        sns.barplot(data=feature_importance, y='feature', x='importance', ax=ax)\n",
    "        ax.set_title('Top 15 Features')\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'top_features_importance.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_roc_curve(self, save_dir=None):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, self.y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "\n",
    "        # Find optimal threshold\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        ax.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, \n",
    "                  label=f'Optimal point (threshold={optimal_threshold:.3f})')\n",
    "\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'roc_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_pr_curve(self, save_dir=None):\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(self.y_test, self.y_pred_proba)\n",
    "        average_precision = average_precision_score(self.y_test, self.y_pred_proba)\n",
    "\n",
    "        # Plot PR Curve\n",
    "        ax.plot(recall, precision, color='blue', lw=2,\n",
    "                label=f'PR curve (AP = {average_precision:.3f})')\n",
    "\n",
    "        # Baseline \n",
    "        baseline = np.sum(self.y_test) / len(self.y_test)\n",
    "        ax.axhline(y=baseline, color='red', linestyle='--', \n",
    "                  label=f'Baseline (Random) = {baseline:.3f}')\n",
    "\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_title('Precision-Recall Curve')\n",
    "        ax.legend(loc=\"lower left\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'precision_recall_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_confusion_matrix(self, save_dir=None):\n",
    "        \"\"\"Visualizza la matrice di confusione\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "\n",
    "        # 1. Confusion matrix with absolute values\n",
    "        ax = axes[0]\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_title('Confusion Matrix - Valori Assoluti')\n",
    "\n",
    "        # 2. Normalized confusion matrix\n",
    "        ax = axes[1]\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=ax)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_title('Confusion Matrix - Normalizzata per Riga')\n",
    "        plt.tight_layout()\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_probability_distribution(self, save_dir=None):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # 1. Class probability distribution\n",
    "        ax = axes[0]\n",
    "        ax.hist(self.y_pred_proba[self.y_test == 0], bins=50, alpha=0.5, \n",
    "                label='Classe 0', color='blue', density=True)\n",
    "        ax.hist(self.y_pred_proba[self.y_test == 1], bins=50, alpha=0.5, \n",
    "                label='Classe 1', color='red', density=True)\n",
    "        ax.axvline(x=0.5, color='black', linestyle='--', label='Threshold=0.5')\n",
    "        ax.set_xlabel('Predicted Probability')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title('Probability Distribution per Class')\n",
    "        ax.legend()\n",
    "\n",
    "        # 2. Probability box plot\n",
    "        ax = axes[1]\n",
    "        data_to_plot = [self.y_pred_proba[self.y_test == 0], \n",
    "                       self.y_pred_proba[self.y_test == 1]]\n",
    "        ax.boxplot(data_to_plot, labels=['Classe 0', 'Classe 1'])\n",
    "        ax.set_ylabel('Predicted Probability')\n",
    "        ax.set_title('Probability Box Plot per Class')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'probability_distribution.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_learning_curves(self, save_dir=None):\n",
    "        _, ax = plt.subplots(figsize=(10, 8))\n",
    "        results = self.model.evals_result()\n",
    "\n",
    "        if results:\n",
    "            epochs = len(results['validation_0']['logloss'])\n",
    "            x_axis = range(0, epochs)\n",
    "\n",
    "            # Plot training & validation loss\n",
    "            ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "            ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "            ax.set_xlabel('Epoca')\n",
    "            ax.set_ylabel('Log Loss')\n",
    "            ax.set_title('Learning Curves - Training Log Loss')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'learning_curves.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_calibration_curve(self, save_dir=None):\n",
    "        from sklearn.calibration import calibration_curve\n",
    "        _, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            self.y_test, self.y_pred_proba, n_bins=10\n",
    "        )\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(mean_predicted_value, fraction_of_positives, 's-', label='XGBoost')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "        ax.set_xlabel('Mean Predicted Probability')\n",
    "        ax.set_ylabel('Fraction of Positives')\n",
    "        ax.set_title('Calibration Plot (Reliability Diagram)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'calibration_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_classification_report(self, save_dir=None):\n",
    "        _, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "        report = classification_report(self.y_test, self.y_pred, output_dict=True)\n",
    "        df_report = pd.DataFrame(report).transpose()\n",
    "        df_report = df_report.iloc[:-1, :-1]\n",
    "\n",
    "        # Heatmap\n",
    "        sns.heatmap(df_report, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax)\n",
    "        ax.set_title('Classification Report Heatmap')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'classification_report.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def run_complete_pipeline(self, model_name=None):\n",
    "        print(\"=== Starting complete pipeline ===\\n\")\n",
    "\n",
    "        with mlflow.start_run() as run:\n",
    "            self.run_id = run.info.run_id\n",
    "            print(f\"🚀 MLflow Run ID: {self.run_id}\")\n",
    "\n",
    "            try:\n",
    "                self.prepare_data()\n",
    "                self.hyperparameter_tuning()\n",
    "                self.train_final_model()\n",
    "                model_path, scaler_path, _ = self.save_model_and_scaler(model_name)\n",
    "                self.plot_all_visualizations()\n",
    "                \n",
    "                mlflow.set_tag(\"pipeline_status\", \"completed\")\n",
    "                mlflow.set_tag(\"model_type\", \"xgboost_binary_classifier\")\n",
    "\n",
    "                print(\"\\n=== PIPELINE COMPLETED ===\")\n",
    "                print(f\"MLflow Run: {self.run_id}\")\n",
    "                print(f\"Modello saved in: {model_path}\")\n",
    "                print(f\"Scaler saved in: {scaler_path}\")\n",
    "\n",
    "                return self.model, self.best_params, self.run_id\n",
    "\n",
    "            except Exception as e:\n",
    "                mlflow.set_tag(\"pipeline_status\", \"failed\")\n",
    "                mlflow.log_param(\"error_message\", str(e))\n",
    "                print(f\"Error pipeline: {e}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d56e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_path = r\"E:\\data\\geo_k_compressed\\full_data_dict.json\"\n",
    "with open(json_path, \"r\") as f:\n",
    "    full_data_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69ead5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_numpy(dizionario):\n",
    "    \"\"\"\n",
    "    Converte un dizionario con struttura specificata in array numpy\n",
    "    \n",
    "    Args:\n",
    "        dizionario: {\"nome_file\": {\"compressed_data\": [...], \"labels\": [...]}}\n",
    "    \n",
    "    Returns:\n",
    "        data_matrix: array numpy (n_features, n_samples)\n",
    "        labels_array: array numpy con le labels\n",
    "        file_names: lista con i nomi dei file per riferimento\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for nome_file, contenuto in dizionario.items():\n",
    "        compressed_data = contenuto[\"compressed_data\"]\n",
    "        labels = contenuto[\"labels\"]\n",
    "        \n",
    "        # Verifica che il numero di labels corrisponda al numero di array\n",
    "        if len(labels) != len(compressed_data):\n",
    "            print(f\"Attenzione: {nome_file} ha {len(compressed_data)} array ma {len(labels)} labels\")\n",
    "        \n",
    "        # Aggiungi i dati\n",
    "        for i, array_data in enumerate(compressed_data):\n",
    "            all_data.append(array_data)\n",
    "            all_labels.append(labels[i] if i < len(labels) else None)\n",
    "            \n",
    "    \n",
    "    # Converti in array numpy\n",
    "    data_matrix = np.array(all_data).T  # Trasponi per avere (features, samples)\n",
    "    labels_array = np.array(all_labels)\n",
    "    \n",
    "    return data_matrix.T, labels_array,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "403eba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data, full_labels = dict_to_numpy(full_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d44c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data, full_labels = dict_to_numpy(full_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6477b28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estrazione features: 100%|██████████| 2555904/2555904 [05:13<00:00, 8165.15it/s]\n"
     ]
    }
   ],
   "source": [
    "features_extractor = DDMFeatureExtractor()\n",
    "\n",
    "def extract_ddm_features_row(row):\n",
    "    return features_extractor.extract_ddm_features(np.array([row]))\n",
    "\n",
    "combined_features = Parallel(n_jobs=12, backend=\"loky\")(delayed(extract_ddm_features_row)(row) for row in tqdm(full_data, desc=\"Estrazione features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abe3c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES=list(combined_features[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04d38263",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_features = [row[0] if isinstance(row, list) and len(row) > 0 else row for row in combined_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "123dbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = np.array([[row[key] for key in FEATURES] for row in flat_features])\n",
    "del flat_features\n",
    "combined_features.shape\n",
    "\n",
    "# Check for NaN and infinite values\n",
    "mask_finite = np.isfinite(combined_features).all(axis=1) & (np.abs(combined_features) < np.finfo(np.float64).max).all(axis=1)\n",
    "\n",
    "fit_data_with_features_clean = combined_features[mask_finite]\n",
    "labels_clean = full_labels[mask_finite]\n",
    "del combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b194ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "median",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "range",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "skew",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "kurtosis",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "entropy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gini",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "peak_index",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "peak_value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "center_of_mass",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "inertia",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sum_third_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_third_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_third_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sum_third_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_third_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_third_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sum_third_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_third_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_third_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_w1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std_w1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_w1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_w2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std_w2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_w2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_w3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std_w3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_w3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_w4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std_w4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_w4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_w5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std_w5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_w5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean_diff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std_diff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_diff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min_diff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "n_positive_diff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "n_negative_diff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "n_zero_diff",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "autocorr_lag1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "autocorr_lag2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "autocorr_lag3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fft_peak_freq",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fft_max",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fft_median",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fft_mean",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7a02bfa1-c973-427e-8f07-335a50d2c302",
       "rows": [
        [
         "0",
         "2.067256853084429",
         "1.5569760067407215",
         "1e-10",
         "5.397672653298242",
         "1.9146729708717896",
         "5.397672653198242",
         "0.48653608339612087",
         "-0.5120702936975405",
         "2.6517090229272178",
         "0.42155548741304927",
         "6.0",
         "5.397672653298242",
         "7.645489413546266",
         "925.9493980801146",
         "20.119695664152147",
         "2.874242237736021",
         "5.397672653298242",
         "15.454664588674548",
         "2.207809226953507",
         "4.381167411904199",
         "5.770776808861871",
         "0.9617961348103119",
         "2.742814540963037",
         "1.730421245198114",
         "0.29926891812737816",
         "2.033299684624536",
         "4.186060309510095",
         "0.972091851700387",
         "5.397672653298242",
         "1.8681297899292542",
         "1.5996348392645543",
         "4.381167411904199",
         "1.79468235383497",
         "1.139927076191378",
         "2.817779064278467",
         "0.7569905669497086",
         "0.8913748465736354",
         "2.1795852185295654",
         "-0.10701577286971242",
         "1.9293652132915644",
         "2.8822141885757446",
         "-3.168738007545471",
         "10.0",
         "9.0",
         "0.0",
         "0.23539666451697022",
         "0.5275502103755515",
         "0.021426822428903584",
         "0.0",
         "41.34513706168857",
         "4.968898361450429",
         "9.69189617700167"
        ],
        [
         "1",
         "0.839392091433866",
         "0.5330162596271215",
         "1e-10",
         "1.7385952473686768",
         "0.9123589099453522",
         "1.7385952472686768",
         "-0.25352340722259425",
         "-0.9456138938849588",
         "2.7089113231049846",
         "0.3593731961268021",
         "7.0",
         "1.7385952473686768",
         "7.782593791236272",
         "433.3493217795415",
         "7.238474608167651",
         "1.0340678011668074",
         "1.5899603367851807",
         "7.05463171075249",
         "1.0078045301074987",
         "1.7385952473686768",
         "2.494735509757181",
         "0.4157892516261968",
         "1.128584861855371",
         "0.937329575519426",
         "0.19636113919570636",
         "1.235378623108728",
         "1.306937888364656",
         "0.45273397931054576",
         "1.7385952473686768",
         "0.7541649789618088",
         "0.48238255768228255",
         "1.3293018342064453",
         "0.8241141737507416",
         "0.5292700877474816",
         "1.4654642344521118",
         "0.3744138405726982",
         "0.46075316832973046",
         "1.128584861855371",
         "-0.06501992752677516",
         "0.7540862695184045",
         "1.4654642343521118",
         "-0.9970801472663879",
         "8.0",
         "11.0",
         "0.0",
         "-0.0332077688908616",
         "0.22130733809601832",
         "0.1827362244186528",
         "0.0",
         "16.787841828677323",
         "2.50283180027967",
         "3.7599622960978025"
        ],
        [
         "2",
         "1.9266716511057453",
         "1.3699516783950352",
         "1e-10",
         "4.614892959694727",
         "1.7116361857460571",
         "4.614892959594727",
         "0.2554226023189942",
         "-0.8005917347930063",
         "2.673218518184578",
         "0.4008617702372974",
         "6.0",
         "4.614892959694727",
         "7.600606353487563",
         "894.1463235013285",
         "18.11883187364006",
         "2.588404553377152",
         "4.614892959694727",
         "14.784155131086353",
         "2.1120221615837647",
         "4.026425361733301",
         "5.630446017388483",
         "0.9384076695647471",
         "2.925693988900049",
         "1.7471782566116882",
         "0.5084901506686816",
         "2.6132254601524902",
         "3.6887550951050354",
         "0.7411603550112053",
         "4.614892959694727",
         "1.8049529493855072",
         "1.4392072306174324",
         "4.026425361733301",
         "1.7162839473293854",
         "1.0740254581491966",
         "2.925693988900049",
         "0.6761880070971085",
         "0.7688727128986419",
         "1.8699376584099365",
         "-0.1375381821080258",
         "1.6897901046867845",
         "2.925693988800049",
         "-2.925693988800049",
         "8.0",
         "11.0",
         "0.0",
         "0.2325529187067029",
         "0.4735851632306923",
         "0.08927842576325573",
         "0.0",
         "38.5334330221149",
         "5.8158504604217045",
         "8.855535915476148"
        ],
        [
         "3",
         "1.4177349121378493",
         "0.8944056455259473",
         "1e-10",
         "2.867295980553491",
         "1.4181379080818726",
         "2.867295980453491",
         "-0.22962473969725394",
         "-0.881033839074743",
         "2.7128007790449797",
         "0.354780454560801",
         "6.0",
         "2.867295980553491",
         "7.7847265933123415",
         "714.4593176745225",
         "12.391990662321094",
         "1.7702843803315849",
         "2.867295980553491",
         "11.798818946584705",
         "1.685545563797815",
         "2.8141582013176514",
         "4.16388863385119",
         "0.6939814389751984",
         "1.7586450577782227",
         "1.4847283364342285",
         "0.3635921954710711",
         "2.0771775246666504",
         "2.3168088794754578",
         "0.6539917532150396",
         "2.867295980553491",
         "1.307594746451242",
         "0.8319690409448947",
         "2.3103435040520264",
         "1.3664712012814118",
         "0.8689246965300383",
         "2.3832011223839356",
         "0.613071397046907",
         "0.7194691572417764",
         "1.7586450577782227",
         "-0.10932513287192897",
         "1.223579502853827",
         "2.3832011222839355",
         "-1.711603045463562",
         "7.0",
         "12.0",
         "0.0",
         "0.033329366531989896",
         "0.2859659503216136",
         "0.20569842774958702",
         "0.0",
         "28.35469824275699",
         "3.753739847090668",
         "6.290838998942444"
        ],
        [
         "4",
         "2.0814951689051227",
         "1.5553144548665916",
         "1e-10",
         "5.358918190102441",
         "1.8633341194199158",
         "5.358918190002441",
         "0.45695506008147824",
         "-0.5586823336533553",
         "2.654890503059106",
         "0.4186592136929166",
         "6.0",
         "5.358918190102441",
         "7.633936779282065",
         "937.3084140784867",
         "20.174118638738634",
         "2.8820169483912332",
         "5.358918190102441",
         "15.543980599149707",
         "2.2205686570213867",
         "4.410599708657129",
         "5.911804140214105",
         "0.9853006900356842",
         "2.912749767403467",
         "1.753555029730661",
         "0.33767862916587943",
         "2.2023940087364746",
         "4.1987627149628235",
         "0.9361356198425425",
         "5.358918190102441",
         "1.8953385056065155",
         "1.60290475809077",
         "4.410599708657129",
         "1.8100560010479523",
         "1.1384942771699869",
         "2.912749767403467",
         "0.7497635931776596",
         "0.8757605370160642",
         "2.1395444870995117",
         "-0.11591547413876183",
         "1.9231481635213556",
         "2.9595823287963867",
         "-3.11350679397583",
         "10.0",
         "9.0",
         "0.0",
         "0.23799432162514902",
         "0.5295400631877246",
         "0.02889522707158383",
         "0.0",
         "41.62990337810245",
         "5.207047642770043",
         "9.69124292405069"
        ]
       ],
       "shape": {
        "columns": 52,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "      <th>range</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>gini</th>\n",
       "      <th>...</th>\n",
       "      <th>n_positive_diff</th>\n",
       "      <th>n_negative_diff</th>\n",
       "      <th>n_zero_diff</th>\n",
       "      <th>autocorr_lag1</th>\n",
       "      <th>autocorr_lag2</th>\n",
       "      <th>autocorr_lag3</th>\n",
       "      <th>fft_peak_freq</th>\n",
       "      <th>fft_max</th>\n",
       "      <th>fft_median</th>\n",
       "      <th>fft_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.067257</td>\n",
       "      <td>1.556976</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>5.397673</td>\n",
       "      <td>1.914673</td>\n",
       "      <td>5.397673</td>\n",
       "      <td>0.486536</td>\n",
       "      <td>-0.512070</td>\n",
       "      <td>2.651709</td>\n",
       "      <td>0.421555</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235397</td>\n",
       "      <td>0.527550</td>\n",
       "      <td>0.021427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.345137</td>\n",
       "      <td>4.968898</td>\n",
       "      <td>9.691896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.839392</td>\n",
       "      <td>0.533016</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.738595</td>\n",
       "      <td>0.912359</td>\n",
       "      <td>1.738595</td>\n",
       "      <td>-0.253523</td>\n",
       "      <td>-0.945614</td>\n",
       "      <td>2.708911</td>\n",
       "      <td>0.359373</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.033208</td>\n",
       "      <td>0.221307</td>\n",
       "      <td>0.182736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.787842</td>\n",
       "      <td>2.502832</td>\n",
       "      <td>3.759962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.926672</td>\n",
       "      <td>1.369952</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>4.614893</td>\n",
       "      <td>1.711636</td>\n",
       "      <td>4.614893</td>\n",
       "      <td>0.255423</td>\n",
       "      <td>-0.800592</td>\n",
       "      <td>2.673219</td>\n",
       "      <td>0.400862</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232553</td>\n",
       "      <td>0.473585</td>\n",
       "      <td>0.089278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.533433</td>\n",
       "      <td>5.815850</td>\n",
       "      <td>8.855536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.417735</td>\n",
       "      <td>0.894406</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.867296</td>\n",
       "      <td>1.418138</td>\n",
       "      <td>2.867296</td>\n",
       "      <td>-0.229625</td>\n",
       "      <td>-0.881034</td>\n",
       "      <td>2.712801</td>\n",
       "      <td>0.354780</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033329</td>\n",
       "      <td>0.285966</td>\n",
       "      <td>0.205698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.354698</td>\n",
       "      <td>3.753740</td>\n",
       "      <td>6.290839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.081495</td>\n",
       "      <td>1.555314</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>5.358918</td>\n",
       "      <td>1.863334</td>\n",
       "      <td>5.358918</td>\n",
       "      <td>0.456955</td>\n",
       "      <td>-0.558682</td>\n",
       "      <td>2.654891</td>\n",
       "      <td>0.418659</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237994</td>\n",
       "      <td>0.529540</td>\n",
       "      <td>0.028895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.629903</td>\n",
       "      <td>5.207048</td>\n",
       "      <td>9.691243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean       std           min       max    median     range      skew  \\\n",
       "0  2.067257  1.556976  1.000000e-10  5.397673  1.914673  5.397673  0.486536   \n",
       "1  0.839392  0.533016  1.000000e-10  1.738595  0.912359  1.738595 -0.253523   \n",
       "2  1.926672  1.369952  1.000000e-10  4.614893  1.711636  4.614893  0.255423   \n",
       "3  1.417735  0.894406  1.000000e-10  2.867296  1.418138  2.867296 -0.229625   \n",
       "4  2.081495  1.555314  1.000000e-10  5.358918  1.863334  5.358918  0.456955   \n",
       "\n",
       "   kurtosis   entropy      gini  ...  n_positive_diff  n_negative_diff  \\\n",
       "0 -0.512070  2.651709  0.421555  ...             10.0              9.0   \n",
       "1 -0.945614  2.708911  0.359373  ...              8.0             11.0   \n",
       "2 -0.800592  2.673219  0.400862  ...              8.0             11.0   \n",
       "3 -0.881034  2.712801  0.354780  ...              7.0             12.0   \n",
       "4 -0.558682  2.654891  0.418659  ...             10.0              9.0   \n",
       "\n",
       "   n_zero_diff  autocorr_lag1  autocorr_lag2  autocorr_lag3  fft_peak_freq  \\\n",
       "0          0.0       0.235397       0.527550       0.021427            0.0   \n",
       "1          0.0      -0.033208       0.221307       0.182736            0.0   \n",
       "2          0.0       0.232553       0.473585       0.089278            0.0   \n",
       "3          0.0       0.033329       0.285966       0.205698            0.0   \n",
       "4          0.0       0.237994       0.529540       0.028895            0.0   \n",
       "\n",
       "     fft_max  fft_median  fft_mean  \n",
       "0  41.345137    4.968898  9.691896  \n",
       "1  16.787842    2.502832  3.759962  \n",
       "2  38.533433    5.815850  8.855536  \n",
       "3  28.354698    3.753740  6.290839  \n",
       "4  41.629903    5.207048  9.691243  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_data_with_features_df = pd.DataFrame(fit_data_with_features_clean, columns=FEATURES)\n",
    "labels_clean_df = pd.DataFrame(labels_clean, columns=['0'])\n",
    "del fit_data_with_features_clean\n",
    "fit_data_with_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model save directory: ./models/geoxgboost_run_200825_1428_F\n",
      "MLflow experiment 'geok_xgboost_experiment_200825_1428_F' correctly set up.\n",
      "=== Starting complete pipeline ===\n",
      "\n",
      "🚀 MLflow Run ID: f32b0b18d73f490cadaa8ba8bfc18ebe\n",
      "=== Data preparation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atogni\\anaconda3\\envs\\great_clf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1018: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(self.var_), copy=False, constant_mask=constant_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "Train set: 2044723 campioni\n",
      "Test set: 511181 campioni\n",
      "Features: 52\n",
      "\n",
      "Class distribution:\n",
      "1    0.710904\n",
      "0    0.289096\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== HYPERPARAMETER TUNING ===\n",
      "Using 12 cores for hyperparameter tuning (75% of 16 total cores).\n",
      "Searching for hyperparameters!\n",
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n"
     ]
    }
   ],
   "source": [
    "date = datetime.now().strftime(\"%d%m%y_%H%M\")\n",
    "\n",
    "# Starting pipeline\n",
    "pipeline = XGBoostPipeline(\n",
    "    features_df=fit_data_with_features_df,\n",
    "    labels_df=labels_clean_df,\n",
    "    experiment_name=f\"geok_xgboost_experiment_{date}_F\",\n",
    "    model_save_dir=f\"./models/geoxgboost_run_{date}_F\", #TODO\n",
    "    save_voter=True,\n",
    ")\n",
    "\n",
    "# Run pipeline\n",
    "model, best_params, run_id = pipeline.run_complete_pipeline(\n",
    "    model_name=f\"geok_xgboost_dev_model_{date}_F\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
