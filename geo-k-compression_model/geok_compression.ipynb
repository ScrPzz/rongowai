{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d86095cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa64d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurfaceTypeUtils:\n",
    "    surface_type_dict = {\n",
    "        -1: \"Ocean\",\n",
    "        0: \"NaN\",\n",
    "        1: \"Artifical\",\n",
    "        2: \"Barely vegetated\",\n",
    "        3: \"Inland water\",\n",
    "        4: \"Crop\",\n",
    "        5: \"Grass\",\n",
    "        6: \"Shrub\",\n",
    "        7: \"Forest\"\n",
    "    }\n",
    "    ddm_antennas = {\n",
    "        0: 'None',\n",
    "        1: 'Zenith',\n",
    "        2: 'LHCP',\n",
    "        3: 'RHCP',\n",
    "    }\n",
    "    \n",
    "class GeoUtils:\n",
    "    def __init__(self, world_shapefile_path):\n",
    "        self.world = gpd.read_file(world_shapefile_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def is_land(self, lat, lon):\n",
    "        point = Point(lon, lat)\n",
    "        return any(self.world.contains(point))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_ocean_and_land(lst):\n",
    "        has_ocean = -1 in lst\n",
    "        has_land = any(1 <= num <= 7 for num in lst)\n",
    "        return has_ocean and has_land\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])\n",
    "    \n",
    "class NetCDFPreprocessor:\n",
    "    def __init__(self, root_dir, preprocessing_method=str):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "        self.preprocessing_method = preprocessing_method\n",
    "        if self.preprocessing_method not in ['filtered', 'with_lat_lons', 'unfiltered']:\n",
    "            raise ValueError(\"Invalid preprocessing method. Choose from 'filtered', 'with_lat_lons', or 'unfiltered'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def check_integrity(f):\n",
    "        \"\"\"Check integrity of the netCDF file\"\"\"\n",
    "        if not isinstance(f, netCDF4.Dataset):\n",
    "            raise ValueError(\"Input must be a netCDF4.Dataset object\")\n",
    "        if 'L1a_power_ddm' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'L1a_power_ddm' variable\")\n",
    "        if 'sp_alt' not in f.variables or 'sp_inc_angle' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_alt' or 'sp_inc_angle' variables\")\n",
    "        if 'sp_rx_gain_copol' not in f.variables or 'sp_rx_gain_xpol' not in f.variables or 'ddm_snr' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_rx_gain_copol', 'sp_rx_gain_xpol' or 'ddm_snr' variables\")\n",
    "        if 'sp_lat' not in f.variables or 'sp_lon' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_lat' or 'sp_lon' variables\")\n",
    "        if 'sp_surface_type' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_surface_type' variable\")\n",
    "        if 'ac_alt' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'ac_alt' variable\")\n",
    "        if f.variables['L1a_power_ddm'].ndim != 4:\n",
    "            raise ValueError(\"The 'L1a_power_ddm' variable must have 4 dimensions\")\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        \"\"\" Preprocess the netCDF file and return fit data and labels \"\"\"\n",
    "        # Check integrity of the netCDF file\n",
    "        self.check_integrity(f)\n",
    "\n",
    "        raw_counts = f.variables['raw_counts'][:]\n",
    "        ac_alt = f.variables['ac_alt'][:]\n",
    "        sp_alt = f.variables['sp_alt'][:]\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        sp_inc_angle = f.variables['sp_inc_angle'][:]\n",
    "\n",
    "        distance_2d = (ac_alt[:, np.newaxis] - sp_alt) / np.cos(np.deg2rad(sp_inc_angle)) # Distance between the aircraft and the specular point\n",
    "\n",
    "        # Filtering mask\n",
    "        keep_mask = (\n",
    "            (copol >= 5) & # # SP copolarized gain\n",
    "            (xpol >= 5) & # SP cross-polarized gain\n",
    "            (snr > 0) & # Positive signal-to-Noise Ratio\n",
    "            (distance_2d >= 2000) & #SP distance min\n",
    "            (distance_2d <= 10000) & #SP distance max\n",
    "            ~np.isnan(copol) & \n",
    "            ~np.isnan(xpol) & \n",
    "            ~np.isnan(snr) & \n",
    "            ~np.isnan(distance_2d)\n",
    "        )\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "        i_indices, j_indices = np.where(keep_mask)\n",
    "        output_array[i_indices, j_indices] = raw_counts[i_indices, j_indices]\n",
    "\n",
    "        n_time, n_samples = raw_counts.shape[:2]\n",
    "        raw_counts_reshaped = output_array.reshape(n_time * n_samples, *raw_counts.shape[2:])\n",
    "        del output_array\n",
    "\n",
    "        # Filter out NaN and zero-sum rows\n",
    "        valid_mask = ~np.any(np.isnan(raw_counts_reshaped), axis=(1, 2)) & (np.sum(raw_counts_reshaped, axis=(1, 2)) > 0)\n",
    "        fit_data = raw_counts_reshaped[valid_mask].reshape(valid_mask.sum(), -1)\n",
    "\n",
    "        surface_types = np.nan_to_num(f.variables[\"sp_surface_type\"][:], nan=0).ravel()\n",
    "        label_data = np.isin(surface_types, np.arange(1, 8)).astype(np.int32)\n",
    "        label_data = label_data[valid_mask]\n",
    "\n",
    "        # Ensure that fit_data and label_data have the same length\n",
    "        assert fit_data.shape[0] == len(label_data), \\\n",
    "            f\"Shape mismatch: fit_data {fit_data.shape[0]}, label_data {len(label_data)}\"\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def preprocess_w_lat_lons(self, f):\n",
    "        \"\"\" Version of the preprocessing function returning latitude and longitudes of the specular points \"\"\"\n",
    "\n",
    "        self.check_integrity(f)\n",
    "        raw_counts = np.array(f.variables['raw_counts'])\n",
    "\n",
    "        # Distance between the aircraft and the specular point\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "        specular_point_lat = f.variables['sp_lat'][:]\n",
    "        specular_point_lon = f.variables['sp_lon'][:]\n",
    "\n",
    "        # Filtering with mask\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data) & ~np.isnan(specular_point_lat.data) & ~np.isnan(specular_point_lon.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "\n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        specular_point_lats = specular_point_lat[to_keep_indices[:, 0]]\n",
    "        specular_point_lons = specular_point_lon[to_keep_indices[:, 0]]\n",
    "\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "        # Reshape the output array to match the original dimensions\n",
    "            raw_counts_filtered = output_array.copy()\n",
    "\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "        del output_array\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "\n",
    "        specular_point_lats = specular_point_lat.ravel()[keep_indices]\n",
    "        specular_point_lons = specular_point_lon.ravel()[keep_indices]\n",
    "\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "        label_data = [1 if surface_type in np.arange(1, 8) else 0 for surface_type in surface_types_unravelled]\n",
    "        label_data = [label_data[lab] for lab in range(len(label_data)) if lab in keep_indices]\n",
    "\n",
    "        assert np.array(fit_data).shape[0] == len(label_data) == np.array(specular_point_lats).shape[0] == np.array(specular_point_lons).shape[0], \\\n",
    "            f\"Shape mismatch: fit_data {np.array(fit_data).shape[0]}, label_data {len(label_data)}, lats {np.array(specular_point_lats).shape[0]}, lons {np.array(specular_point_lons).shape[0]}\"\n",
    "\n",
    "\n",
    "        return fit_data, label_data, specular_point_lats, specular_point_lons\n",
    "\n",
    "    def preprocess_snr_unfiltered(self, f):\n",
    "        \"\"\" Preprocess the netCDF file and return fit data and labels without filtering on signal-to-noise ratio \"\"\"\n",
    "        # Check integrity of the netCDF file\n",
    "        self.check_integrity(f)\n",
    "\n",
    "        L1a_power_ddm = f.variables['L1a_power_ddm'][:]\n",
    "        ac_alt = f.variables['ac_alt'][:]\n",
    "        sp_alt = f.variables['sp_alt'][:]\n",
    "        sp_inc_angle = f.variables['sp_inc_angle'][:]\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        #snr = f.variables['ddm_snr'][:]\n",
    "        \n",
    "        #Distance between the aircraft and the specular point\n",
    "        distance_2d = (ac_alt[:, np.newaxis] - sp_alt) / np.cos(np.deg2rad(sp_inc_angle))\n",
    "        # Filtering mask without SNR\n",
    "        keep_mask = (\n",
    "            (copol >= 5) & \n",
    "            (xpol >= 5) & \n",
    "        #   (snr > 0)  &\n",
    "            (distance_2d >= 2000) & \n",
    "            (distance_2d <= 10000) &\n",
    "            ~np.isnan(copol) & \n",
    "            ~np.isnan(xpol) & \n",
    "            #~np.isnan(snr) & \n",
    "            ~np.isnan(distance_2d)\n",
    "        )\n",
    "\n",
    "        output_array = np.full(L1a_power_ddm.shape, np.nan, dtype=np.float32)\n",
    "        i_indices, j_indices = np.where(keep_mask)\n",
    "        output_array[i_indices, j_indices] = L1a_power_ddm[i_indices, j_indices]\n",
    "\n",
    "        n_time, n_samples = L1a_power_ddm.shape[:2]\n",
    "        L1a_power_ddm_reshaped = output_array.reshape(n_time * n_samples, *L1a_power_ddm.shape[2:])\n",
    "        del output_array\n",
    "        valid_mask = ~np.any(np.isnan(L1a_power_ddm_reshaped), axis=(1, 2)) & (np.sum(L1a_power_ddm_reshaped, axis=(1, 2)) > 0)\n",
    "        fit_data = L1a_power_ddm_reshaped[valid_mask].reshape(valid_mask.sum(), -1)\n",
    "\n",
    "        surface_types = np.nan_to_num(f.variables[\"sp_surface_type\"][:], nan=0).ravel()\n",
    "        label_data = np.isin(surface_types, np.arange(1, 8)).astype(np.int32)\n",
    "        label_data = label_data[valid_mask]\n",
    "        # Ensure that fit_data and label_data have the same length\n",
    "        assert fit_data.shape[0] == len(label_data), \\\n",
    "            f\"Shape mismatch: fit_data {fit_data.shape[0]}, label_data {len(label_data)}\"\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "\n",
    "    def process_all_files_random_picked(self, chunk_size = int, sample_fraction = float, n_files_to_pick= int, remove_chunks= bool):\n",
    "        \"\"\" Process all netCDF files in the directory, randomly picking a specified number of files,\n",
    "        and save the processed data and labels in chunks.\"\"\"\n",
    "\n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        counter = 0\n",
    "\n",
    "        # Take a random number of netCDF files\n",
    "        if int(len(self.netcdf_file_list)) > n_files_to_pick: # type: ignore\n",
    "            np.random.seed(42)\n",
    "            random_netcdf_selected_files = np.random.choice(self.netcdf_file_list, n_files_to_pick, replace=False) # type: ignore\n",
    "            print('Selezionati 500 file netCDF casuali dalla lista')\n",
    "        else:\n",
    "            random_netcdf_selected_files = self.netcdf_file_list\n",
    "\n",
    "        for file_name in tqdm(random_netcdf_selected_files, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                continue\n",
    "            try:\n",
    "                f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                if self.preprocessing_method == 'unfiltered':\n",
    "                    data, labels = self.preprocess_snr_unfiltered(f)\n",
    "                elif self.preprocessing_method == 'with_lat_lons':\n",
    "                    data, labels, latitudes, longitudes = self.preprocess_w_lat_lons(f)\n",
    "                else:\n",
    "                    # Default to filtered preprocessing\n",
    "                    data, labels = self.preprocess(f)\n",
    "                assert (len(data) == len(labels)), f\"Data and labels length mismatch in file {file_name}: {len(data)} != {len(labels)}\"\n",
    "                full_data.append(data)\n",
    "                full_labels.append(labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "            counter += 1\n",
    "            if counter == n_files_to_pick:\n",
    "                break\n",
    "        print(f\"Processed {counter} files out of {len(random_netcdf_selected_files)} selected files.\")\n",
    "        # Filtering on data shape\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "        print(f\"Number of valid data arrays after filtering: {len(full_data_clean)}\")\n",
    "        # Chunking \n",
    "        os.makedirs('geok_test_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))  # type: ignore\n",
    "        print(f\"Total number of chunks: {num_chunks}\")\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size # type: ignore\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean)) # type: ignore\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            if chunk_data.size == 0 or chunk_labels.size == 0:\n",
    "                print(f\"Skipping empty chunk {idx + 1}/{num_chunks}\")\n",
    "                continue\n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "\n",
    "            # Save each chunk to parquet files\n",
    "            if chunk_data.shape[0] == 0 or chunk_labels.shape[0] == 0:\n",
    "                print(f\"Skipping empty chunk {idx + 1}/{num_chunks}\")\n",
    "                continue\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'geok_test_data/binary_classification/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'geok_test_data/binary_classification/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            # Stratified sampling from each chunk\n",
    "            _, X_sampled, _, y_sampled = train_test_split(\n",
    "                chunk_data, chunk_labels,\n",
    "                test_size=sample_fraction,  # type: ignore\n",
    "                stratify=chunk_labels,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        del full_data, full_labels\n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "\n",
    "        del full_data_sampled, full_labels_sampled\n",
    "        print(f\"Shape of sampled data after chunking and sampling: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels after chunking and sampling: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "\n",
    "        # Save the final sampled data and labels in parquet format\n",
    "        if not os.path.exists('geok_test_data/binary_classification'):\n",
    "            print(\"Creating directory geok_test_data/binary_classification\")\n",
    "            os.makedirs('geok_test_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        # Save fit_data \n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'geok_test_data/binary_classification/fit_data_binary_test.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "        )\n",
    "        # Save labels\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'geok_test_data/binary_classification/labels_binary_test.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "        )\n",
    "        # Clean up\n",
    "        del fit_data_df, labels_df, table_fit, table_labels\n",
    "\n",
    "        print(\"Data and labels saved in geok_test_data/binary_classification directory.\")\n",
    "        # Remove all chunk parquet files if flag is set (to save space)\n",
    "        if remove_chunks:\n",
    "            try:\n",
    "                chunk_dir = 'geok_test_data/binary_classification'\n",
    "                for fname in os.listdir(chunk_dir):\n",
    "                    if fname.startswith('fit_data_chunk_') or fname.startswith('labels_chunk_'):\n",
    "                        os.remove(os.path.join(chunk_dir, fname))\n",
    "                print(\"All chunk files removed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing chunk files: {e}\")\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDMProcessor:\n",
    "    \"\"\"\n",
    "    Class for processing and compressing DDM (Delay Doppler Map) files from NetCDF format.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def check_integrity(f):\n",
    "        \"\"\"Check integrity of the netCDF file\"\"\"\n",
    "        if not isinstance(f, netCDF4.Dataset):\n",
    "            raise ValueError(\"Input must be a netCDF4.Dataset object\")\n",
    "        if 'raw_counts' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'raw_counts' variable\")\n",
    "        if 'sp_alt' not in f.variables or 'sp_inc_angle' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_alt' or 'sp_inc_angle' variables\")\n",
    "        if 'sp_rx_gain_copol' not in f.variables or 'sp_rx_gain_xpol' not in f.variables or 'ddm_snr' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_rx_gain_copol', 'sp_rx_gain_xpol' or 'ddm_snr' variables\")\n",
    "        if 'sp_lat' not in f.variables or 'sp_lon' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_lat' or 'sp_lon' variables\")\n",
    "        if 'sp_surface_type' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_surface_type' variable\")\n",
    "        if 'ac_alt' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'ac_alt' variable\")\n",
    "        if f.variables['raw_counts'].ndim != 4:\n",
    "            raise ValueError(\"The 'raw_counts' variable must have 4 dimensions\")\n",
    "        \n",
    "    def __init__(self, input_folder, output_folder, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the DDM Processor.\n",
    "        \n",
    "        Args:\n",
    "            input_folder (str): Path to folder containing input NetCDF files\n",
    "            output_folder (str): Path to folder where compressed files will be saved\n",
    "            device (torch.device): Device for computation (cuda/cpu)\n",
    "        \"\"\"\n",
    "        self.input_folder = Path(input_folder)\n",
    "        self.output_folder = Path(output_folder)\n",
    "        \n",
    "        # Create output folder if it doesn't exist\n",
    "        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.scaler_fitted = False\n",
    "        \n",
    "        # Placeholder for encoder model (to be loaded or set)\n",
    "        self.encoder = None\n",
    "        \n",
    "    def set_encoder(self, encoder_model):\n",
    "        \"\"\"\n",
    "        Set the encoder model for compression.\n",
    "        \n",
    "        Args:\n",
    "            encoder_model: PyTorch model for encoding/compression\n",
    "        \"\"\"\n",
    "        self.encoder = encoder_model.to(self.device)\n",
    "        self.encoder.eval()\n",
    "        \n",
    "    def load_encoder(self, model_path):\n",
    "        \"\"\"\n",
    "        Load a pre-trained encoder model.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the saved encoder model\n",
    "        \"\"\"\n",
    "        # Example implementation - adjust based on your model architecture\n",
    "        self.encoder = torch.load(model_path, map_location=self.device)\n",
    "        self.encoder.eval()\n",
    "        \n",
    "    def process_single_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Process a single NetCDF file and extract DDM data.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the NetCDF file\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Processed DDM data\n",
    "        \"\"\"\n",
    "        ddm_list = []\n",
    "        #kept_track_id = []\n",
    "        #discarded_track_id = []\n",
    "        #data_dict = {}\n",
    "        # Load NetCDF dataset\n",
    "        try:\n",
    "            ds = netCDF4.Dataset(file_path)\n",
    "            self.check_integrity(ds)\n",
    "            #data_dict['file_name'] = file_name\n",
    "            #L1a_power_ddm = ds[\"L1a_power_ddm\"][:]  # Shape (N, 20, 40, 5)\n",
    "\n",
    "            track_id = ds[\"track_id\"][:] # Shape (N,)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        # Calcola la stessa maschera usata dal preprocessor\n",
    "        #preprocessor.check_integrity(f)\n",
    "\n",
    "        ac_alt = ds['ac_alt'][:]\n",
    "        sp_alt = ds['sp_alt'][:]\n",
    "        sp_inc_angle = ds['sp_inc_angle'][:]\n",
    "        copol = ds['sp_rx_gain_copol'][:]\n",
    "        xpol = ds['sp_rx_gain_xpol'][:]\n",
    "        L1a_power_ddm = ds['L1a_power_ddm'][:]\n",
    "        surface = ds.variables[\"sp_surface_type\"][:]\n",
    "\n",
    "        try:\n",
    "            assert L1a_power_ddm.shape[0] == track_id.shape[0], \"DDM and track_id must have the same first dimension\" # type: ignore\n",
    "        except AssertionError as e:\n",
    "            print(e)\n",
    "\n",
    "        # Stessa logica del preprocessor per la distanza\n",
    "        distance_2d = (ac_alt[:, np.newaxis] - sp_alt) / np.cos(np.deg2rad(sp_inc_angle))\n",
    "\n",
    "        # Stessa maschera di filtering (senza SNR per unfiltered)\n",
    "        keep_mask = (\n",
    "            (copol >= 5) & \n",
    "            (xpol >= 5) & \n",
    "            (distance_2d >= 2000) & \n",
    "            (distance_2d <= 10000) &\n",
    "            ~np.isnan(copol) & \n",
    "            ~np.isnan(xpol) & \n",
    "            ~np.isnan(distance_2d) &\n",
    "            ~np.isnan(surface)  \n",
    "            \n",
    "        )\n",
    "\n",
    "        # Applica la maschera per creare l'array filtrato\n",
    "        output_array = np.full(L1a_power_ddm.shape, np.nan, dtype=np.float32)\n",
    "        i_indices, j_indices = np.where(keep_mask)\n",
    "        output_array[i_indices, j_indices] = L1a_power_ddm[i_indices, j_indices]\n",
    "\n",
    "        n_time, n_samples = L1a_power_ddm.shape[:2]\n",
    "        L1a_power_ddm_reshaped = output_array.reshape(n_time * n_samples, *L1a_power_ddm.shape[2:])\n",
    "\n",
    "        # Filter out NaN and zero-sum rows\n",
    "        valid_mask = ~np.any(np.isnan(L1a_power_ddm_reshaped), axis=(1, 2)) & (np.sum(L1a_power_ddm_reshaped, axis=(1, 2)) > 0)\n",
    "\n",
    "\n",
    "        surface_types = np.nan_to_num(surface, nan=0).ravel()\n",
    "        label_data = np.isin(surface_types, np.arange(1, 8)).astype(np.int32)\n",
    "        label_data = label_data[valid_mask]\n",
    "\n",
    "        # Process DDM data\n",
    "        label_data_check = []\n",
    "        for sample in range(output_array.shape[0]):\n",
    "            for channel in range(output_array.shape[1]):\n",
    "                ddm_sample = output_array[sample, channel].reshape(200,)  # Shape (200,)\n",
    "                if max(ddm_sample) > 0:\n",
    "                    ddm_list.append(ddm_sample)\n",
    "                    label = surface[sample, channel]\n",
    "                    if label in np.arange(1, 8):\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label = 0\n",
    "                \n",
    "                    label_data_check.append(label)\n",
    "                    #kept_track_id.append((sample, channel))\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    #discarded_track_id.append((sample, channel)) # type: ignore #\n",
    "                    continue\n",
    "        del output_array\n",
    "        \n",
    "        #ata_dict['kept_track_id'] = np.array(kept_track_id)\n",
    "        #data_dict['discarded_track_id'] = np.array(discarded_track_id)\n",
    "\n",
    "        if len(ddm_list) == 0:\n",
    "            print(f\"No valid DDM samples found in {file_path.name}\")\n",
    "            \n",
    "        # Stack all DDM samples\n",
    "        ddm_data_raw = np.stack(ddm_list)\n",
    "        #data_dict['ddm_data_raw'] = ddm_data_raw\n",
    "        #print(f\"  Extracted {ddm_data_raw.shape[0]} valid DDM samples\")\n",
    "        try:\n",
    "            assert label_data.shape[0] == ddm_data_raw.shape[0], \\\n",
    "                f\"Shape mismatch: label_data {label_data.shape[0]}, ddm_data_raw {ddm_data_raw.shape[0]}\"\n",
    "        except AssertionError as e:\n",
    "            print(f\"Error in {file_path.name}: {e}\")\n",
    "            return None, None\n",
    "            \n",
    "        \n",
    "        return ddm_data_raw, label_data\n",
    "    \n",
    "    def normalize_data(self, ddm_data_raw):\n",
    "        \"\"\"\n",
    "        Normalize DDM data to [0, 1] range.\n",
    "        \n",
    "        Args:\n",
    "            ddm_data_raw (np.ndarray): Raw DDM data\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Normalized DDM data\n",
    "        \"\"\"\n",
    "        # Scale the data\n",
    "        ddm_data = self.scaler.fit_transform(ddm_data_raw * 1e13)\n",
    "        self.scaler_fitted = True\n",
    "        \n",
    "        return ddm_data\n",
    "    \n",
    "    def compress_data(self, tensor_data):\n",
    "        \"\"\"\n",
    "        Compress data using the encoder model.\n",
    "        \n",
    "        Args:\n",
    "            tensor_data (torch.Tensor): Input tensor data\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Compressed data\n",
    "        \"\"\"\n",
    "        if self.encoder is None:\n",
    "            raise ValueError(\"Encoder model not set. Use set_encoder() or load_encoder() first.\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(tensor_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        compressed_data = []\n",
    "        \n",
    "        # Compress data in batches\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                inputs = batch[0].to(self.device)\n",
    "                compressed = self.encoder(inputs)\n",
    "                compressed_data.append(compressed.cpu().numpy())\n",
    "        \n",
    "        # Concatenate all compressed batches\n",
    "        compressed_array = np.concatenate(compressed_data, axis=0)\n",
    "        \n",
    "        return compressed_array\n",
    "    \n",
    "    def save_compressed_data(self, compressed_data, original_filename):\n",
    "        \"\"\"\n",
    "        Save compressed data to output folder.\n",
    "        \n",
    "        Args:\n",
    "            compressed_data (np.ndarray): Compressed data to save\n",
    "            original_filename (str): Original filename (without extension)\n",
    "        \"\"\"\n",
    "        output_filename = f\"{original_filename}_compressed.npz\"\n",
    "        output_path = self.output_folder / output_filename\n",
    "        \n",
    "        # Save compressed data using numpy compressed format\n",
    "        np.savez_compressed(output_path, data=compressed_data)\n",
    "        #print(f\"  Saved compressed data to {output_path}\")\n",
    "        \n",
    "    def process_all_files(self, file_extension='.nc', save_scaler=True):\n",
    "        \"\"\"\n",
    "        Process all NetCDF files in the input folder.\n",
    "        \n",
    "        Args:\n",
    "            file_extension (str): Extension of files to process (default: '.nc')\n",
    "            save_scaler (bool): Whether to save the scaler for future use\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "\n",
    "\n",
    "        # Get all files with specified extension\n",
    "        file_list = list(self.input_folder.glob(f'*{file_extension}'))[:100]# Limit to first 50 files for testing\n",
    "\n",
    "        \n",
    "        if len(file_list) == 0:\n",
    "            print(f\"No files with extension '{file_extension}' found in {self.input_folder}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(file_list)} files to process\")\n",
    "        full_data_dict = defaultdict(dict)\n",
    "        # Process each file\n",
    "        for file_path in tqdm(file_list, desc=\"Processing files\"):\n",
    "            if not file_path.is_file():\n",
    "                continue\n",
    "            # Step 1: Load and process DDM data\n",
    "            ddm_data_raw, label_data = self.process_single_file(file_path) # type: ignore\n",
    "            #full_data_dict[data_dict['file_name']] = data_dict\n",
    "            if ddm_data_raw is None:\n",
    "                continue\n",
    "            if label_data is None:\n",
    "                continue\n",
    "            \n",
    "            # Step 2: Normalize data\n",
    "            ddm_data_normalized = self.normalize_data(ddm_data_raw)\n",
    "            \n",
    "            # Step 3: Convert to tensor\n",
    "            tensor_data = torch.tensor(ddm_data_normalized, dtype=torch.float32)\n",
    "            \n",
    "            # Step 4: Compress data (if encoder is available)\n",
    "            if self.encoder is not None:\n",
    "                compressed_data = self.compress_data(tensor_data)\n",
    "                \n",
    "                # Step 5: Save compressed data\n",
    "                filename_without_ext = file_path.stem\n",
    "                self.save_compressed_data(compressed_data, filename_without_ext)\n",
    "                #print(f\"  Saving normalized data to {filename_without_ext}_normalized.npz\")\n",
    "\n",
    "                full_data_dict[str(filename_without_ext)]['compressed_data'] = compressed_data # type: ignore\n",
    "                full_data_dict[str(filename_without_ext)]['labels'] = label_data  # type: ignore\n",
    "\n",
    "\n",
    "            else:\n",
    "                # If no encoder, save normalized data\n",
    "                print(\"  No encoder set - saving normalized data instead\")\n",
    "                filename_without_ext = file_path.stem\n",
    "               \n",
    "                output_filename = f\"{filename_without_ext}_normalized.npz\"\n",
    "                output_path = self.output_folder / output_filename\n",
    "                np.savez_compressed(output_path, data=ddm_data_normalized)\n",
    "                print(f\"  Saved normalized data to {output_path}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Save scaler for future use\n",
    "        if save_scaler and self.scaler_fitted:\n",
    "            scaler_path = self.output_folder / \"scaler_encoder.pkl\"\n",
    "            joblib.dump(self.scaler, scaler_path)\n",
    "            print(f\"\\nScaler saved to {scaler_path}\")\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing complete! Output files saved to {self.output_folder}\")\n",
    "        return full_data_dict # type: ignore\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba16143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 20),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 200)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------\n",
    "# Load the saved models\n",
    "# ----------------------\n",
    "def load_model(model_class, path):\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c81dffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 100 files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   1%|          | 1/100 [00:00<01:04,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221026-100450_NZRO-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|▏         | 2/100 [00:01<00:53,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221026-112526_NZAA-NZTG_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   3%|▎         | 3/100 [00:01<00:53,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221026-122454_NZTG-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   4%|▍         | 4/100 [00:02<01:08,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221026-134535_NZAA-NZGS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   5%|▌         | 5/100 [00:03<01:20,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221026-150826_NZGS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   6%|▌         | 6/100 [00:04<01:19,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221026-165902_NZAA-NZAP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|▋         | 7/100 [00:05<01:17,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221026-181252_NZAP-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   8%|▊         | 8/100 [00:06<01:33,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221026-202809_NZAA-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   9%|▉         | 9/100 [00:08<01:41,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221027-061109_NZNS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  10%|█         | 10/100 [00:09<01:35,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221027-082948_NZAA-NZGS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  11%|█         | 11/100 [00:10<01:35,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221027-095646_NZGS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  12%|█▏        | 12/100 [00:11<01:41,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221027-123430_NZAA-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  13%|█▎        | 13/100 [00:12<01:31,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221027-143322_NZNS-NZCH_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  14%|█▍        | 14/100 [00:12<01:20,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221027-163735_NZCH-NZHK_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  15%|█▌        | 15/100 [00:13<01:07,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221027-174645_NZHK-NZCH_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  16%|█▌        | 16/100 [00:14<01:05,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221028-083519_NZCH-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  17%|█▋        | 17/100 [00:15<01:11,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221028-103953_NZNS-NZCH_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  18%|█▊        | 18/100 [00:16<01:08,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221028-125424_NZCH-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  19%|█▉        | 19/100 [00:16<00:57,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221028-144937_NZNS-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  20%|██        | 20/100 [00:17<01:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221028-161003_NZWN-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  21%|██        | 21/100 [00:17<00:50,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221028-173441_NZNS-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  22%|██▏       | 22/100 [00:18<00:59,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221028-185455_NZWN-NZGS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  23%|██▎       | 23/100 [00:19<01:04,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221029-063152_NZGS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  24%|██▍       | 24/100 [00:20<01:01,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221029-082733_NZAA-NZAP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  25%|██▌       | 25/100 [00:21<01:03,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221029-100254_NZAP-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  26%|██▌       | 26/100 [00:22<01:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221029-112638_NZAA-NZAP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  27%|██▋       | 27/100 [00:22<01:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221029-123547_NZAP-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  28%|██▊       | 28/100 [00:23<00:52,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221029-141744_NZAA-NZWR_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  29%|██▉       | 29/100 [00:23<00:45,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221029-151744_NZWR-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  30%|███       | 30/100 [00:24<00:41,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221029-165836_NZAA-NZWR_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  31%|███       | 31/100 [00:24<00:39,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-064735_NZAA-NZTG_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  32%|███▏      | 32/100 [00:26<00:49,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-074443_NZTG-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  33%|███▎      | 33/100 [00:27<00:58,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-092955_NZWN-NZTG_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  34%|███▍      | 34/100 [00:27<00:51,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-111110_NZTG-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  35%|███▌      | 35/100 [00:28<00:51,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-123329_NZAA-NZGS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  36%|███▌      | 36/100 [00:29<00:55,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-141321_NZGS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  37%|███▋      | 37/100 [00:31<01:08,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-154832_NZAA-NZWB_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  38%|███▊      | 38/100 [00:32<01:09,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-173952_NZWB-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  39%|███▉      | 39/100 [00:33<00:57,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221101-194320_NZAA-NZKK_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  40%|████      | 40/100 [00:33<00:51,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-060644_NZKK-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  41%|████      | 41/100 [00:34<00:49,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-072913_NZAA-NZNP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  42%|████▏     | 42/100 [00:35<00:45,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-084043_NZNP-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  43%|████▎     | 43/100 [00:35<00:45,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-105117_NZAA-NZNP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  44%|████▍     | 44/100 [00:36<00:43,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-120951_NZNP-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  45%|████▌     | 45/100 [00:36<00:34,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-141125_NZWN-NZWB_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  46%|████▌     | 46/100 [00:37<00:28,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-162321_NZWB-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  47%|████▋     | 47/100 [00:37<00:24,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-172207_NZWN-NZWB_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  48%|████▊     | 48/100 [00:37<00:21,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-181605_NZWB-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  49%|████▉     | 49/100 [00:39<00:35,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221102-195358_NZWN-NZTG_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 50/100 [00:40<00:46,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221103-065105_NZTG-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  51%|█████     | 51/100 [00:42<01:02,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221103-091459_NZWN-NZNV_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  52%|█████▏    | 52/100 [00:44<01:01,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221103-121416_NZNV-NZCH_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  53%|█████▎    | 53/100 [00:45<01:09,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221103-151412_NZCH-NZTG_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  54%|█████▍    | 54/100 [00:46<00:55,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221103-173530_NZTG-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  55%|█████▌    | 55/100 [00:47<00:49,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221103-185508_NZAA-NZNR_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  56%|█████▌    | 56/100 [00:48<00:50,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221104-071142_NZNR-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  57%|█████▋    | 57/100 [00:49<00:41,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221104-085735_NZAA-NZRO_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  58%|█████▊    | 58/100 [00:49<00:37,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221104-100424_NZRO-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  59%|█████▉    | 59/100 [00:51<00:39,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221104-123446_NZAA-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  60%|██████    | 60/100 [00:51<00:35,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221104-144000_NZNS-NZCH_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  61%|██████    | 61/100 [00:52<00:32,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221105-085244_NZCH-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  62%|██████▏   | 62/100 [00:53<00:28,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221105-101226_NZNS-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  63%|██████▎   | 63/100 [00:53<00:25,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221105-123553_NZWN-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  64%|██████▍   | 64/100 [00:54<00:29,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221105-143106_NZNS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  65%|██████▌   | 65/100 [00:55<00:28,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221105-165902_NZAA-NZGS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  66%|██████▌   | 66/100 [00:56<00:29,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221106-093628_NZGS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  67%|██████▋   | 67/100 [00:56<00:24,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221106-115743_NZAA-NZTG_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  68%|██████▊   | 68/100 [00:57<00:21,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221106-125610_NZTG-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  69%|██████▉   | 69/100 [00:58<00:23,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221106-151123_NZAA-NZGS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  70%|███████   | 70/100 [00:59<00:26,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221106-163130_NZGS-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  71%|███████   | 71/100 [01:00<00:22,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221106-183016_NZWN-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  72%|███████▏  | 72/100 [01:01<00:26,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-060333_NZNS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  73%|███████▎  | 73/100 [01:02<00:24,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-083011_NZAA-NZGS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  74%|███████▍  | 74/100 [01:03<00:24,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-095603_NZGS-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  75%|███████▌  | 75/100 [01:03<00:19,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-115912_NZAA-NZWR_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  76%|███████▌  | 76/100 [01:04<00:17,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-130025_NZWR-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  77%|███████▋  | 77/100 [01:04<00:15,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-143614_NZAA-NZNP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  78%|███████▊  | 78/100 [01:05<00:15,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-154457_NZNP-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  79%|███████▉  | 79/100 [01:06<00:13,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-172323_NZAA-NZTG_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  80%|████████  | 80/100 [01:06<00:13,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-181821_NZTG-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  81%|████████  | 81/100 [01:07<00:14,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221107-194029_NZAA-NZKK_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  82%|████████▏ | 82/100 [01:08<00:12,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-060603_NZKK-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  83%|████████▎ | 83/100 [01:09<00:12,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-073331_NZAA-NZNP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  84%|████████▍ | 84/100 [01:09<00:12,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-084629_NZNP-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  85%|████████▌ | 85/100 [01:10<00:10,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-100736_NZAA-NZRO_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  86%|████████▌ | 86/100 [01:11<00:09,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-110604_NZRO-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  87%|████████▋ | 87/100 [01:11<00:09,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-123914_NZAA-NZWR_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  88%|████████▊ | 88/100 [01:12<00:07,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-140534_NZWR-NZAA_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  89%|████████▉ | 89/100 [01:12<00:06,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-152653_NZAA-NZTG_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  90%|█████████ | 90/100 [01:14<00:10,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-163135_NZTG-NZCH_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  91%|█████████ | 91/100 [01:16<00:09,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221108-192429_NZCH-NZNP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  92%|█████████▏| 92/100 [01:17<00:08,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221109-070037_NZNP-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  93%|█████████▎| 93/100 [01:17<00:06,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221109-083828_NZWN-NZNP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  94%|█████████▍| 94/100 [01:18<00:05,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221109-103050_NZNP-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  95%|█████████▌| 95/100 [01:19<00:04,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221109-123628_NZWN-NZNP_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  96%|█████████▌| 96/100 [01:20<00:03,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221109-140730_NZNP-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  97%|█████████▋| 97/100 [01:20<00:02,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221109-160642_NZWN-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  98%|█████████▊| 98/100 [01:21<00:01,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221109-171054_NZNS-NZWN_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  99%|█████████▉| 99/100 [01:21<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221109-183125_NZWN-NZNS_L1_normalized.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 100/100 [01:22<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saving normalized data to 20221110-065522_NZNS-NZWN_L1_normalized.npz\n",
      "\n",
      "Scaler saved to E:\\data\\geo_k_compressed\\scaler_encoder.pkl\n",
      "\n",
      "==================================================\n",
      "Processing complete! Output files saved to E:\\data\\geo_k_compressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define input and output folders\n",
    "    input_folder = \"E:/data/RONGOWAI_L1_SDR_V1.0\"\n",
    "    output_folder = \"E:/data/geo_k_compressed\"\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = DDMProcessor(input_folder, output_folder)\n",
    "    \n",
    "    #Load a pre-trained encoder\n",
    "    encoder = load_model(Encoder, \"C:/Users/atogni/Desktop/rongowai/geo_k/encoder_all_surface2.pth\")\n",
    "    processor.set_encoder(encoder)\n",
    "    \n",
    "    # Process all NetCDF files in the input folder\n",
    "    full_data_dict = processor.process_all_files(file_extension='.nc', save_scaler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad622991",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:/data/geo_k_compressed/full_data_dict.json\", \"w\") as f:\n",
    "        json.dump(full_data_dict, f, indent=2, default=lambda x: x.tolist() if hasattr(x, \"tolist\") else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d478e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f97fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_path = r\"E:\\data\\geo_k_compressed\\full_data_dict.json\"\n",
    "with open(json_path, \"r\") as f:\n",
    "    loaded_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482e2b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['20221110-065522_NZNS-NZWN_L1.nc'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9cc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(loaded_data['20221026-150826_NZGS-NZAA_L1.nc'][\"compressed_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1570c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = loaded_data['20221026-150826_NZGS-NZAA_L1.nc'][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54251686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_dict['20221026-150826_NZGS-NZAA_L1.nc'][]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_dict['20221026-150826_NZGS-NZAA_L1.nc']\n",
    "    # Example: Access compressed data and labels for a specific file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc32e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save full_data_dict to a JSON file\n",
    "with open(\"full_data_dict.json\", \"w\") as f:\n",
    "    json.dump(full_data_dict, f, indent=2, default=lambda x: x.tolist() if hasattr(x, \"tolist\") else str(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39727683",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_data_dict['20221026-100450_NZRO-NZAA_L1.nc']['kept_track_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_dict['20221026-100450_NZRO-NZAA_L1.nc']['discarded_track_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98654907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
