{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a002bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, Union, List, Dict\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report,\n",
    "                           roc_curve, precision_recall_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "import warnings\n",
    "import netCDF4\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.fft import fft\n",
    "\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurazione per grafici migliori\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCDFPreprocessor:\n",
    "    def __init__(self, root_dir, preprocessing_method=str):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "        self.preprocessing_method = preprocessing_method\n",
    "        if self.preprocessing_method not in ['filtered', 'with_lat_lons', 'unfiltered']:\n",
    "            raise ValueError(\"Invalid preprocessing method. Choose from 'filtered', 'with_lat_lons', or 'unfiltered'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def check_integrity(f):\n",
    "        \"\"\"Check integrity of the netCDF file\"\"\"\n",
    "        if not isinstance(f, netCDF4.Dataset):\n",
    "            raise ValueError(\"Input must be a netCDF4.Dataset object\")\n",
    "        if 'raw_counts' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'raw_counts' variable\")\n",
    "        if 'sp_alt' not in f.variables or 'sp_inc_angle' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_alt' or 'sp_inc_angle' variables\")\n",
    "        if 'sp_rx_gain_copol' not in f.variables or 'sp_rx_gain_xpol' not in f.variables or 'ddm_snr' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_rx_gain_copol', 'sp_rx_gain_xpol' or 'ddm_snr' variables\")\n",
    "        if 'sp_lat' not in f.variables or 'sp_lon' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_lat' or 'sp_lon' variables\")\n",
    "        if 'sp_surface_type' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_surface_type' variable\")\n",
    "        if 'ac_alt' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'ac_alt' variable\")\n",
    "        if f.variables['raw_counts'].ndim != 4:\n",
    "            raise ValueError(\"The 'raw_counts' variable must have 4 dimensions\")\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        \"\"\" Preprocess the netCDF file and return fit data and labels \"\"\"\n",
    "        # Check integrity of the netCDF file\n",
    "        self.check_integrity(f)\n",
    "\n",
    "        raw_counts = f.variables['raw_counts'][:]\n",
    "        ac_alt = f.variables['ac_alt'][:]\n",
    "        sp_alt = f.variables['sp_alt'][:]\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        sp_inc_angle = f.variables['sp_inc_angle'][:]\n",
    "\n",
    "        distance_2d = (ac_alt[:, np.newaxis] - sp_alt) / np.cos(np.deg2rad(sp_inc_angle)) # Distance between the aircraft and the specular point\n",
    "\n",
    "        # Filtering mask\n",
    "        keep_mask = (\n",
    "            (copol >= 5) & # # SP copolarized gain\n",
    "            (xpol >= 5) & # SP cross-polarized gain\n",
    "            (snr > 0) & # Positive signal-to-Noise Ratio\n",
    "            (distance_2d >= 2000) & #SP distance min\n",
    "            (distance_2d <= 10000) & #SP distance max\n",
    "            ~np.isnan(copol) & \n",
    "            ~np.isnan(xpol) & \n",
    "            ~np.isnan(snr) & \n",
    "            ~np.isnan(distance_2d)\n",
    "        )\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "        i_indices, j_indices = np.where(keep_mask)\n",
    "        output_array[i_indices, j_indices] = raw_counts[i_indices, j_indices]\n",
    "\n",
    "        n_time, n_samples = raw_counts.shape[:2]\n",
    "        raw_counts_reshaped = output_array.reshape(n_time * n_samples, *raw_counts.shape[2:])\n",
    "        del output_array\n",
    "\n",
    "        # Filter out NaN and zero-sum rows\n",
    "        valid_mask = ~np.any(np.isnan(raw_counts_reshaped), axis=(1, 2)) & (np.sum(raw_counts_reshaped, axis=(1, 2)) > 0)\n",
    "        fit_data = raw_counts_reshaped[valid_mask].reshape(valid_mask.sum(), -1)\n",
    "\n",
    "        surface_types = np.nan_to_num(f.variables[\"sp_surface_type\"][:], nan=0).ravel()\n",
    "        label_data = np.isin(surface_types, np.arange(1, 8)).astype(np.int32)\n",
    "        label_data = label_data[valid_mask]\n",
    "\n",
    "        # Ensure that fit_data and label_data have the same length\n",
    "        assert fit_data.shape[0] == len(label_data), \\\n",
    "            f\"Shape mismatch: fit_data {fit_data.shape[0]}, label_data {len(label_data)}\"\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def preprocess_w_lat_lons(self, f):\n",
    "        \"\"\" Version of the preprocessing function returning latitude and longitudes of the specular points \"\"\"\n",
    "\n",
    "        self.check_integrity(f)\n",
    "        raw_counts = np.array(f.variables['raw_counts'])\n",
    "\n",
    "        # Distance between the aircraft and the specular point\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "        specular_point_lat = f.variables['sp_lat'][:]\n",
    "        specular_point_lon = f.variables['sp_lon'][:]\n",
    "\n",
    "        # Filtering with mask\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data) & ~np.isnan(specular_point_lat.data) & ~np.isnan(specular_point_lon.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "\n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        specular_point_lats = specular_point_lat[to_keep_indices[:, 0]]\n",
    "        specular_point_lons = specular_point_lon[to_keep_indices[:, 0]]\n",
    "\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "        # Reshape the output array to match the original dimensions\n",
    "            raw_counts_filtered = output_array.copy()\n",
    "\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "        del output_array\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "\n",
    "        specular_point_lats = specular_point_lat.ravel()[keep_indices]\n",
    "        specular_point_lons = specular_point_lon.ravel()[keep_indices]\n",
    "\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "        label_data = [1 if surface_type in np.arange(1, 8) else 0 for surface_type in surface_types_unravelled]\n",
    "        label_data = [label_data[lab] for lab in range(len(label_data)) if lab in keep_indices]\n",
    "\n",
    "        assert np.array(fit_data).shape[0] == len(label_data) == np.array(specular_point_lats).shape[0] == np.array(specular_point_lons).shape[0], \\\n",
    "            f\"Shape mismatch: fit_data {np.array(fit_data).shape[0]}, label_data {len(label_data)}, lats {np.array(specular_point_lats).shape[0]}, lons {np.array(specular_point_lons).shape[0]}\"\n",
    "\n",
    "\n",
    "        return fit_data, label_data, specular_point_lats, specular_point_lons\n",
    "\n",
    "    def preprocess_snr_unfiltered(self, f):\n",
    "        \"\"\" Preprocess the netCDF file and return fit data and labels without filtering on signal-to-noise ratio \"\"\"\n",
    "        # Check integrity of the netCDF file\n",
    "        self.check_integrity(f)\n",
    "\n",
    "        raw_counts = f.variables['raw_counts'][:]\n",
    "        ac_alt = f.variables['ac_alt'][:]\n",
    "        sp_alt = f.variables['sp_alt'][:]\n",
    "        sp_inc_angle = f.variables['sp_inc_angle'][:]\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        #snr = f.variables['ddm_snr'][:]\n",
    "        \n",
    "        #Distance between the aircraft and the specular point\n",
    "        distance_2d = (ac_alt[:, np.newaxis] - sp_alt) / np.cos(np.deg2rad(sp_inc_angle))\n",
    "        # Filtering mask without SNR\n",
    "        keep_mask = (\n",
    "            (copol >= 5) & \n",
    "            (xpol >= 5) & \n",
    "        #   (snr > 0)  &\n",
    "            (distance_2d >= 2000) & \n",
    "            (distance_2d <= 10000) &\n",
    "            ~np.isnan(copol) & \n",
    "            ~np.isnan(xpol) & \n",
    "            #~np.isnan(snr) & \n",
    "            ~np.isnan(distance_2d)\n",
    "        )\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "        i_indices, j_indices = np.where(keep_mask)\n",
    "        output_array[i_indices, j_indices] = raw_counts[i_indices, j_indices]\n",
    "\n",
    "        n_time, n_samples = raw_counts.shape[:2]\n",
    "        raw_counts_reshaped = output_array.reshape(n_time * n_samples, *raw_counts.shape[2:])\n",
    "        del output_array\n",
    "        valid_mask = ~np.any(np.isnan(raw_counts_reshaped), axis=(1, 2)) & (np.sum(raw_counts_reshaped, axis=(1, 2)) > 0)\n",
    "        fit_data = raw_counts_reshaped[valid_mask].reshape(valid_mask.sum(), -1)\n",
    "\n",
    "        surface_types = np.nan_to_num(f.variables[\"sp_surface_type\"][:], nan=0).ravel()\n",
    "        label_data = np.isin(surface_types, np.arange(1, 8)).astype(np.int32)\n",
    "        label_data = label_data[valid_mask]\n",
    "        # Ensure that fit_data and label_data have the same length\n",
    "        assert fit_data.shape[0] == len(label_data), \\\n",
    "            f\"Shape mismatch: fit_data {fit_data.shape[0]}, label_data {len(label_data)}\"\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "\n",
    "    def process_all_files_random_picked(self, chunk_size = int, sample_fraction = float, n_files_to_pick= int, remove_chunks= bool):\n",
    "        \"\"\" Process all netCDF files in the directory, randomly picking a specified number of files,\n",
    "        and save the processed data and labels in chunks.\"\"\"\n",
    "\n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        counter = 0\n",
    "\n",
    "        # Take a random number of netCDF files\n",
    "        if int(len(self.netcdf_file_list)) > n_files_to_pick: # type: ignore\n",
    "            np.random.seed(42)\n",
    "            random_netcdf_selected_files = np.random.choice(self.netcdf_file_list, n_files_to_pick, replace=False) # type: ignore\n",
    "            print('Selezionati 500 file netCDF casuali dalla lista')\n",
    "        else:\n",
    "            random_netcdf_selected_files = self.netcdf_file_list\n",
    "\n",
    "        for file_name in tqdm(random_netcdf_selected_files, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                continue\n",
    "            try:\n",
    "                f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                if self.preprocessing_method == 'unfiltered':\n",
    "                    data, labels = self.preprocess_snr_unfiltered(f)\n",
    "                elif self.preprocessing_method == 'with_lat_lons':\n",
    "                    data, labels, latitudes, longitudes = self.preprocess_w_lat_lons(f)\n",
    "                else:\n",
    "                    # Default to filtered preprocessing\n",
    "                    data, labels = self.preprocess(f)\n",
    "                assert (len(data) == len(labels)), f\"Data and labels length mismatch in file {file_name}: {len(data)} != {len(labels)}\"\n",
    "                full_data.append(data)\n",
    "                full_labels.append(labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "            counter += 1\n",
    "            if counter == n_files_to_pick:\n",
    "                break\n",
    "        print(f\"Processed {counter} files out of {len(random_netcdf_selected_files)} selected files.\")\n",
    "        # Filtering on data shape\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "        print(f\"Number of valid data arrays after filtering: {len(full_data_clean)}\")\n",
    "        # Chunking \n",
    "        os.makedirs('test_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))  # type: ignore\n",
    "        print(f\"Total number of chunks: {num_chunks}\")\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size # type: ignore\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean)) # type: ignore\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            if chunk_data.size == 0 or chunk_labels.size == 0:\n",
    "                print(f\"Skipping empty chunk {idx + 1}/{num_chunks}\")\n",
    "                continue\n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "\n",
    "            # Save each chunk to parquet files\n",
    "            if chunk_data.shape[0] == 0 or chunk_labels.shape[0] == 0:\n",
    "                print(f\"Skipping empty chunk {idx + 1}/{num_chunks}\")\n",
    "                continue\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'test_data/binary_classification/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'test_data/binary_classification/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            # Stratified sampling from each chunk\n",
    "            _, X_sampled, _, y_sampled = train_test_split(\n",
    "                chunk_data, chunk_labels,\n",
    "                test_size=sample_fraction,  # type: ignore\n",
    "                stratify=chunk_labels,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        del full_data, full_labels\n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "\n",
    "        del full_data_sampled, full_labels_sampled\n",
    "        print(f\"Shape of sampled data after chunking and sampling: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels after chunking and sampling: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "\n",
    "        # Save the final sampled data and labels in parquet format\n",
    "        if not os.path.exists('test_data/binary_classification'):\n",
    "            print(\"Creating directory test_data/binary_classification\")\n",
    "            os.makedirs('test_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        # Save fit_data \n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'test_data/binary_classification/fit_data_binary_test.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "        )\n",
    "        # Save labels\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'test_data/binary_classification/labels_binary_test.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "        )\n",
    "        # Clean up\n",
    "        del fit_data_df, labels_df, table_fit, table_labels\n",
    "\n",
    "        print(\"Data and labels saved in test_data/binary_classification directory.\")\n",
    "        # Remove all chunk parquet files if flag is set (to save space)\n",
    "        if remove_chunks:\n",
    "            try:\n",
    "                chunk_dir = 'test_data/binary_classification'\n",
    "                for fname in os.listdir(chunk_dir):\n",
    "                    if fname.startswith('fit_data_chunk_') or fname.startswith('labels_chunk_'):\n",
    "                        os.remove(os.path.join(chunk_dir, fname))\n",
    "                print(\"All chunk files removed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing chunk files: {e}\")\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccefe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def gini(self, array):\n",
    "            \"\"\"Calcola il coefficiente di Gini (disuguaglianza)\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))    \n",
    "    def extract_ddm_features(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"\n",
    "        Estrae features dettagliate da raw_counts DDM (shape: n_samples x 200)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float64) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. Statistiche base\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = self.gini(x)\n",
    "\n",
    "            # 2. Posizionali\n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentazione\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "            \n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivate e cambiamenti\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelazioni (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT (spettro frequenze)\n",
    "            spectrum = np.abs(fft(x))\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  # simmetrico\n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "            \n",
    "            #Aggiungi le statistiche dei quadranti e del centro\n",
    "            ddm = row.reshape(10, 20)  # 10x20\n",
    "\n",
    "            # Quadranti\n",
    "            q1 = ddm[:5, :10].ravel()\n",
    "            q2 = ddm[:5, 10:].ravel()\n",
    "            q3 = ddm[5:, :10].ravel()\n",
    "            q4 = ddm[5:, 10:].ravel()\n",
    "            # Quadrante centrale (4x8 centrale)\n",
    "            center = ddm[3:7, 6:14].ravel()\n",
    "            \n",
    "            # Statistiche dei quadranti \n",
    "            f['q1_mean'] = np.mean(q1)\n",
    "            f['q2_mean'] = np.mean(q2)      \n",
    "            f['q3_mean'] = np.mean(q3)\n",
    "            f['q4_mean'] = np.mean(q4)\n",
    "            f['center_mean'] = np.mean(center)\n",
    "            f['q1_std'] = np.std(q1)\n",
    "            f['q2_std'] = np.std(q2)\n",
    "            f['q3_std'] = np.std(q3)\n",
    "            f['q4_std'] = np.std(q4)\n",
    "            f['center_std'] = np.std(center)\n",
    "            f['q1_min'] = np.min(q1)\n",
    "            f['q2_min'] = np.min(q2)\n",
    "            f['q3_min'] = np.min(q3)\n",
    "            f['q4_min'] = np.min(q4)\n",
    "            f['center_min'] = np.min(center)\n",
    "            f['q1_max'] = np.max(q1)\n",
    "            f['q2_max'] = np.max(q2)\n",
    "            f['q3_max'] = np.max(q3)\n",
    "            f['q4_max'] = np.max(q4)\n",
    "            f['center_max'] = np.max(center)\n",
    "            f['q1_median'] = np.median(q1)\n",
    "            f['q2_median'] = np.median(q2)\n",
    "            f['q3_median'] = np.median(q3)\n",
    "            f['q4_median'] = np.median(q4)\n",
    "            f['center_median'] = np.median(center)\n",
    "            f['q1_range'] = np.max(q1) - np.min(q1)\n",
    "            f['q2_range'] = np.max(q2) - np.min(q2)\n",
    "            f['q3_range'] = np.max(q3) - np.min(q3)\n",
    "            f['q4_range'] = np.max(q4) - np.min(q4)\n",
    "            f['center_range'] = np.max(center) - np.min(center)\n",
    "            f['q1_skew'] = skew(q1)\n",
    "            f['q2_skew'] = skew(q2)\n",
    "            f['q3_skew'] = skew(q3)\n",
    "            f['q4_skew'] = skew(q4)\n",
    "            f['center_skew'] = skew(center)\n",
    "            f['q1_kurtosis'] = kurtosis(q1)\n",
    "            f['q2_kurtosis'] = kurtosis(q2)\n",
    "            f['q3_kurtosis'] = kurtosis(q3)\n",
    "            f['q4_kurtosis'] = kurtosis(q4)\n",
    "            f['center_kurtosis'] = kurtosis(center)\n",
    "            f['q1_entropy'] = entropy(q1 + 1e-10)\n",
    "            f['q2_entropy'] = entropy(q2 + 1e-10)\n",
    "            f['q3_entropy'] = entropy(q3 + 1e-10)\n",
    "            f['q4_entropy'] = entropy(q4 + 1e-10)\n",
    "            f['center_entropy'] = entropy(center + 1e-10)\n",
    "            f['q1_gini'] = self.gini(q1)\n",
    "            f['q2_gini'] = self.gini(q2)\n",
    "            f['q3_gini'] = self.gini(q3)\n",
    "            f['q4_gini'] = self.gini(q4)\n",
    "            f['center_gini'] = self.gini(center)\n",
    "\n",
    "            # Statistiche di confronto tra quadranti e centro\n",
    "            \n",
    "            # Differenze tra media dei quadranti e centro\n",
    "            f['q1_center_mean_diff'] = f['q1_mean'] - f['center_mean']\n",
    "            f['q2_center_mean_diff'] = f['q2_mean'] - f['center_mean']\n",
    "            f['q3_center_mean_diff'] = f['q3_mean'] - f['center_mean']\n",
    "            f['q4_center_mean_diff'] = f['q4_mean'] - f['center_mean']\n",
    "\n",
    "            # Differenze tra std dei quadranti e centro\n",
    "            f['q1_center_std_diff'] = f['q1_std'] - f['center_std']\n",
    "            f['q2_center_std_diff'] = f['q2_std'] - f['center_std']\n",
    "            f['q3_center_std_diff'] = f['q3_std'] - f['center_std']\n",
    "            f['q4_center_std_diff'] = f['q4_std'] - f['center_std']\n",
    "\n",
    "            # Differenze tra max dei quadranti e centro\n",
    "            f['q1_center_max_diff'] = f['q1_max'] - f['center_max']\n",
    "            f['q2_center_max_diff'] = f['q2_max'] - f['center_max']\n",
    "            f['q3_center_max_diff'] = f['q3_max'] - f['center_max']\n",
    "            f['q4_center_max_diff'] = f['q4_max'] - f['center_max']\n",
    "\n",
    "            # Differenze tra min dei quadranti e centro\n",
    "            f['q1_center_min_diff'] = f['q1_min'] - f['center_min']\n",
    "            f['q2_center_min_diff'] = f['q2_min'] - f['center_min']\n",
    "            f['q3_center_min_diff'] = f['q3_min'] - f['center_min']\n",
    "            f['q4_center_min_diff'] = f['q4_min'] - f['center_min']\n",
    "\n",
    "            # Differenze tra entropia dei quadranti e centro\n",
    "            f['q1_center_entropy_diff'] = f['q1_entropy'] - f['center_entropy']\n",
    "            f['q2_center_entropy_diff'] = f['q2_entropy'] - f['center_entropy']\n",
    "            f['q3_center_entropy_diff'] = f['q3_entropy'] - f['center_entropy']\n",
    "            f['q4_center_entropy_diff'] = f['q4_entropy'] - f['center_entropy']\n",
    "\n",
    "            # Differenze tra gini dei quadranti e centro\n",
    "            f['q1_center_gini_diff'] = f['q1_gini'] - f['center_gini']\n",
    "            f['q2_center_gini_diff'] = f['q2_gini'] - f['center_gini']\n",
    "            f['q3_center_gini_diff'] = f['q3_gini'] - f['center_gini']\n",
    "            f['q4_center_gini_diff'] = f['q4_gini'] - f['center_gini']\n",
    "\n",
    "            # Differenze tra skewness dei quadranti e centro\n",
    "            f['q1_center_skew_diff'] = f['q1_skew'] - f['center_skew']\n",
    "            f['q2_center_skew_diff'] = f['q2_skew'] - f['center_skew']\n",
    "            f['q3_center_skew_diff'] = f['q3_skew'] - f['center_skew']\n",
    "            f['q4_center_skew_diff'] = f['q4_skew'] - f['center_skew']\n",
    "\n",
    "            # Differenze tra kurtosis dei quadranti e centro\n",
    "            f['q1_center_kurtosis_diff'] = f['q1_kurtosis'] - f['center_kurtosis']\n",
    "            f['q2_center_kurtosis_diff'] = f['q2_kurtosis'] - f['center_kurtosis']\n",
    "            f['q3_center_kurtosis_diff'] = f['q3_kurtosis'] - f['center_kurtosis']\n",
    "            f['q4_center_kurtosis_diff'] = f['q4_kurtosis'] - f['center_kurtosis']\n",
    "\n",
    "            features.append(f)\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SampleConfig:\n",
    "    \"\"\"Sample configuration.\"\"\"\n",
    "    name: str\n",
    "    n_samples_per_class: int\n",
    "    random_state: int = 42\n",
    "\n",
    "@dataclass\n",
    "class DatasetInfo:\n",
    "    n_samples: int\n",
    "    n_features: int\n",
    "    class_distribution: Dict[int, int]\n",
    "    features_columns: List[str]\n",
    "    labels_columns: List[str]\n",
    "\n",
    "@dataclass\n",
    "class DataLoader:\n",
    "    features_path: Union[str, Path]\n",
    "    labels_path: Union[str, Path]\n",
    "    _features_df_cache: Optional[pd.DataFrame] = field(default=None, init=False, repr=False)\n",
    "    _labels_df_cache: Optional[pd.DataFrame] = field(default=None, init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.features_path = Path(self.features_path)\n",
    "        self.labels_path = Path(self.labels_path)\n",
    "\n",
    "        if not self.features_path.exists():\n",
    "            raise FileNotFoundError(f\"Features file not found: {self.features_path}\")\n",
    "        if not self.labels_path.exists():\n",
    "            raise FileNotFoundError(f\"Labels file not found: {self.labels_path}\")\n",
    "\n",
    "    def load_full_data(self, use_cache: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \n",
    "        if use_cache and self._features_df_cache is not None and self._labels_df_cache is not None:\n",
    "            return self._features_df_cache, self._labels_df_cache\n",
    "\n",
    "        features_df = pd.read_parquet(self.features_path)\n",
    "        labels_df = pd.read_parquet(self.labels_path)\n",
    "\n",
    "        # Dimensions check\n",
    "        if len(features_df) != len(labels_df):\n",
    "            raise ValueError(f\"Features and labels have different lengths: \"\n",
    "                           f\"{len(features_df)} vs {len(labels_df)}\")\n",
    "\n",
    "        if use_cache:\n",
    "            self._features_df_cache = features_df\n",
    "            self._labels_df_cache = labels_df\n",
    "\n",
    "        return features_df, labels_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_cache(self) -> None:\n",
    "        self._features_df_cache = None\n",
    "        self._labels_df_cache = None\n",
    "\n",
    "    def load_balanced_sample(self, \n",
    "                            n_samples_per_class: int, \n",
    "                            random_state: int = 42,\n",
    "                            use_cache: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \n",
    "        # Load full data (with caching option)\n",
    "        features_df_full, labels_df_full = self.load_full_data(use_cache=use_cache)\n",
    "\n",
    "        # Get sampled indices balanced across classes\n",
    "        sampled_indices = self._get_balanced_indices(\n",
    "            labels_df_full = labels_df_full,\n",
    "            n_samples_per_class = n_samples_per_class,\n",
    "            random_state =random_state\n",
    "        )\n",
    "        features_df = features_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = labels_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "\n",
    "        return features_df, labels_df\n",
    "\n",
    "    def load_balanced_sample_memory_efficient(self, n_samples_per_class: int, \n",
    "                                            random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \n",
    "        features_df_full = pd.read_parquet(self.features_path)\n",
    "        labels_df_full = pd.read_parquet(self.labels_path)\n",
    "\n",
    "        # Get sampled indices balanced across classes\n",
    "        sampled_indices = self._get_balanced_indices(\n",
    "            labels_df = labels_df_full, \n",
    "            n_samples_per_class = n_samples_per_class, \n",
    "            random_state = random_state\n",
    "        )\n",
    "\n",
    "        features_df = features_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = labels_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "\n",
    "        # Clean up memory\n",
    "        del features_df_full, labels_df_full, sampled_indices\n",
    "\n",
    "        return features_df, labels_df\n",
    "    \n",
    "    def _get_balanced_indices(self, labels_df: pd.DataFrame, \n",
    "                            n_samples_per_class: int, \n",
    "                            random_state: int = 42) -> pd.Index:\n",
    "        \"\"\"\n",
    "        Get indices of a balanced sample from the labels DataFrame.\n",
    "        \"\"\"\n",
    "        label_column = labels_df.iloc[:, 0]\n",
    "\n",
    "        # Check if all classes have enough samples\n",
    "        class_counts = label_column.value_counts()\n",
    "        for class_label, count in class_counts.items():\n",
    "            if count < n_samples_per_class:\n",
    "                raise ValueError(f\"Class {class_label} has only {count} samples, \"\n",
    "                               f\"but {n_samples_per_class} requested\")\n",
    "\n",
    "        sampled_indices = (\n",
    "            labels_df.groupby(label_column)\n",
    "            .apply(lambda x: x.sample(n=n_samples_per_class, random_state=random_state))\n",
    "            .index.get_level_values(1)\n",
    "        )\n",
    "        return sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"D:/DATA/\" # 'E:/data/RONGOWAI_L1_SDR_V1.0/' # Change this to your actual root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6402758",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_backup = False\n",
    "if read_from_backup:\n",
    "    \n",
    "    # Leggi i file parquet con polars\n",
    "    fit_data_pl = pd.read_parquet('./test_data/binary_classification/fit_data_binary_test.parquet')\n",
    "    labels_pl = pd.read_parquet('./test_data/binary_classification/labels_binary_test.parquet')\n",
    "\n",
    "    # Trasforma in numpy array\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()\n",
    "    del fit_data_pl, labels_pl\n",
    "else:\n",
    "    preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR, preprocessing_method='filtered')\n",
    "    fit_data, labels = preprocessor.process_all_files_random_picked(chunk_size=250, n_files_to_pick= 1000, sample_fraction=0.9,remove_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000 \n",
    "indices = np.random.choice(fit_data.shape[0], size=num_samples, replace=False)\n",
    "fit_data = fit_data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Istanzia il preprocessor e l'estrattore di features\n",
    "preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR, preprocessing_method='filtered')\n",
    "features_extractor = DDMFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estrazione delle features DDM\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_ddm_features_row(row):\n",
    "    return features_extractor.extract_ddm_features(np.array([row]))\n",
    "\n",
    "combined_features = Parallel(n_jobs=12, backend=\"loky\")(delayed(extract_ddm_features_row)(row) for row in tqdm(fit_data, desc=\"Estrazione features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e78a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_features = [row[0] if isinstance(row, list) and len(row) > 0 else row for row in combined_features]\n",
    "FEATURES = list(flat_features[0].keys())\n",
    "\n",
    "combined_features = np.array([[row[key] for key in FEATURES] for row in flat_features])\n",
    "combined_features.shape\n",
    "\n",
    "mask_finite = np.isfinite(combined_features).all(axis=1) & (np.abs(combined_features) < np.finfo(np.float64).max).all(axis=1)\n",
    "fit_data_with_features_clean = combined_features[mask_finite]\n",
    "labels_clean = labels[mask_finite]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672881e0",
   "metadata": {},
   "source": [
    "### VOTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this paths\n",
    "catboost_model_path = r\"./models/voters/catboost_voter.cbm\"\n",
    "catboost_scaler_path = \"./models/voters/catboost_scaler.joblib\"\n",
    "\n",
    "xg_boost_model_path = \"./models/voters/xgboost_voter.joblib\"\n",
    "xg_boost_scaler_path = \"./models/voters/xgboost_scaler.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_catboost_model(model_path=str):\n",
    "        try:\n",
    "            model = CatBoostClassifier()\n",
    "            model.load_model(str(model_path))\n",
    "            print(f\"Catboost model loaded from: {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error in loading catboost model: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading models and scalers \n",
    "print(\"Loading models and scalers...\")\n",
    "\n",
    "# Catboost\n",
    "catboost_model_final = _load_catboost_model(catboost_model_path)\n",
    "\n",
    "try:\n",
    "    catboost_scaler = joblib.load(catboost_scaler_path)\n",
    "    print(\"CatBoost scaler ok\")\n",
    "except FileNotFoundError:\n",
    "    print(\"CatBoost scaler not found\")\n",
    "\n",
    "# XGBoost \n",
    "xg_boost_model_final = None\n",
    "try:\n",
    "    xg_boost_model_final = joblib.load(xg_boost_model_path) \n",
    "    print(\"XGBoost model ok\")\n",
    "except FileNotFoundError:\n",
    "    print(\"XGBoost model not found\")\n",
    "    xg_boost_model_final = None\n",
    "\n",
    "xgboost_scaler = None\n",
    "if xg_boost_model_final is not None:\n",
    "    try:\n",
    "        xgboost_scaler = joblib.load(xg_boost_scaler_path)\n",
    "        print(\"XGBoost scaler ok\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"XGBoost scaler not found\")\n",
    "        xgboost_scaler = StandardScaler()\n",
    "\n",
    "# Sanity check\n",
    "def check_models_loaded():  \n",
    "    if catboost_model_final is None:\n",
    "        print(\"CatBoost model non caricato correttamente.\")\n",
    "    else:\n",
    "        print(\"CatBoost model caricato correttamente.\")\n",
    "\n",
    "    if xg_boost_model_final is None:\n",
    "        print(\"XGBoost model non caricato correttamente.\")\n",
    "    else:\n",
    "        print(\"XGBoost model caricato correttamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a582de",
   "metadata": {},
   "source": [
    "### VOTING CLASSIFIER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaxProbVotingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Voting Classifier che seleziona la classe con la probabilità più alta.\n",
    "    In caso di pareggio, privilegia CatBoost.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimators, scalers=None):\n",
    "        self.estimators = estimators\n",
    "        self.scalers = scalers if scalers else {}\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fit degli scaler se necessario\n",
    "        for name, scaler in self.scalers.items():\n",
    "            if scaler is not None and not hasattr(scaler, 'mean_'):\n",
    "                scaler.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Calcola le probabilità medie o massime per ogni classe\"\"\"\n",
    "        probas = []\n",
    "        \n",
    "        for name, model in self.estimators:\n",
    "            if model is None:\n",
    "                continue\n",
    "                \n",
    "            # Applica lo scaler se disponibile\n",
    "            X_scaled = X\n",
    "            if name in self.scalers and self.scalers[name] is not None:\n",
    "                X_scaled = self.scalers[name].transform(X)\n",
    "            \n",
    "            \n",
    "            proba = model.predict_proba(X_scaled)\n",
    "            probas.append((name, proba))\n",
    "        \n",
    "        if not probas:\n",
    "            raise ValueError(\"Nessun modello disponibile per le predizioni\")\n",
    "        \n",
    "        # Per ogni sample, prendi la probabilità massima per la classe positiva\n",
    "        n_samples = X.shape[0]\n",
    "        final_probas = np.zeros((n_samples, 2))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            max_prob_class_1 = -1\n",
    "            #best_model = None\n",
    "            \n",
    "            for name, proba in probas:\n",
    "                if proba[i, 1] > max_prob_class_1:\n",
    "                    max_prob_class_1 = proba[i, 1]\n",
    "                    #best_model = name\n",
    "                    final_probas[i] = proba[i]\n",
    "                elif proba[i, 1] == max_prob_class_1 and name == 'catboost':\n",
    "                    # In caso di pareggio, privilegia CatBoost\n",
    "                    #best_model = name\n",
    "                    final_probas[i] = proba[i]\n",
    "        \n",
    "        return final_probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predizione basata sulle probabilità\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 3. PREPARAZIONE DATI\n",
    "print(\"\\nPreparazione dati...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    fit_data_with_features_clean, \n",
    "    labels_clean,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels_clean\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# 4. CREAZIONE VOTING CLASSIFIER\n",
    "estimators = []\n",
    "scalers = {}\n",
    "\n",
    "if catboost_model_final is not None:\n",
    "    estimators.append(('catboost', catboost_model_final))\n",
    "    scalers['catboost'] = catboost_scaler\n",
    "\n",
    "if xg_boost_model_final is not None:\n",
    "    estimators.append(('xgboost', xg_boost_model_final))\n",
    "    scalers['xgboost'] = xgboost_scaler\n",
    "\n",
    "voting_clf = MaxProbVotingClassifier(estimators=estimators, scalers=scalers)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. FUNZIONE PER TROVARE SOGLIA OTTIMALE\n",
    "def find_optimal_threshold(model, X, y, metric='precision', conservative=False):\n",
    "    \"\"\"\n",
    "    Trova la soglia ottimale per massimizzare una metrica.\n",
    "    Se conservative=True, in caso di pareggio prende la soglia più bassa (più conservativa).\n",
    "    \"\"\"\n",
    "    probas = model.predict_proba(X)[:, 1]\n",
    "    thresholds = np.linspace(0.5, 0.6, 5)\n",
    "    print(f\"y_pred: {probas}\")\n",
    "    scores = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (probas >= threshold).astype(int)\n",
    "        print(f\"y_pred: {y_pred}\")\n",
    "        \n",
    "        if metric == 'recall':\n",
    "            score = recall_score(y, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y, y_pred)\n",
    "        elif metric == 'f1':\n",
    "            score = f1_score(y, y_pred)\n",
    "        else:\n",
    "            raise ValueError(f\"Metrica non supportata: {metric}\")\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    if conservative:\n",
    "        # Trova il primo threshold che massimizza la metrica\n",
    "        optimal_idx = np.where(scores == scores.max())[0][0]\n",
    "    else:\n",
    "        # Trova l'ultimo threshold che massimizza la metrica\n",
    "        optimal_idx = np.where(scores == scores.max())[0][-1]\n",
    "    \n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_score = scores[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, optimal_score, thresholds, scores\n",
    "\n",
    "# 6. VALUTAZIONE MODELLI\n",
    "def evaluate_model(model, X_test, y_test, model_name, threshold=0.5):\n",
    "    \"\"\"Valuta un modello con metriche multiple\"\"\"\n",
    "    probas = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (probas >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, probas)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} - Soglia: {threshold:.3f}\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"AUC-ROC:   {metrics['auc']:.4f}\")\n",
    "    \n",
    "    return metrics, y_pred, probas\n",
    "\n",
    "# Valutazione con soglia standard (0.5)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALUTAZIONE MODELLI CON SOGLIA STANDARD (0.5)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_standard = {}\n",
    "\n",
    "# CatBoost\n",
    "if catboost_model_final is not None:\n",
    "    X_test_scaled_cb = catboost_scaler.transform(X_test) if catboost_scaler else X_test\n",
    "    metrics, y_pred, probas = evaluate_model(\n",
    "        catboost_model_final, X_test_scaled_cb, y_test, \"CatBoost\"\n",
    "    )\n",
    "    results_standard['CatBoost'] = {'metrics': metrics, 'y_pred': y_pred, 'probas': probas}\n",
    "\n",
    "# XGBoost (se disponibile)\n",
    "if xg_boost_model_final is not None:\n",
    "    X_test_scaled_xgb = xgboost_scaler.transform(X_test) if xgboost_scaler else X_test\n",
    "    metrics, y_pred, probas = evaluate_model(\n",
    "        xg_boost_model_final, X_test_scaled_xgb, y_test, \"XGBoost\"\n",
    "    )\n",
    "    results_standard['XGBoost'] = {'metrics': metrics, 'y_pred': y_pred, 'probas': probas}\n",
    "\n",
    "# Voting Classifier\n",
    "metrics, y_pred, probas = evaluate_model(voting_clf, X_test, y_test, \"Voting Classifier\")\n",
    "results_standard['Voting'] = {'metrics': metrics, 'y_pred': y_pred, 'probas': probas}\n",
    "\n",
    "# 7. OTTIMIZZAZIONE SOGLIA PER f1\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OTTIMIZZAZIONE SOGLIA PER MASSIMIZZARE f1\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_optimized = {}\n",
    "\n",
    "# Trova soglie ottimali per ogni modello\n",
    "for model_name, model in [('CatBoost', catboost_model_final), \n",
    "                          ('XGBoost', xg_boost_model_final),\n",
    "                          ('Voting', voting_clf)]:\n",
    "    if model is None:\n",
    "        continue\n",
    "\n",
    "    if model_name == 'CatBoost':\n",
    "        X_val = catboost_scaler.transform(X_train) if catboost_scaler else X_train\n",
    "        X_test_model = catboost_scaler.transform(X_test) if catboost_scaler else X_test\n",
    "    elif model_name == 'XGBoost':\n",
    "        X_val = xgboost_scaler.transform(X_train) if xgboost_scaler else X_train\n",
    "        X_test_model = xgboost_scaler.transform(X_test) if xgboost_scaler else X_test\n",
    "    else:  # Voting\n",
    "        X_val = X_train\n",
    "        X_test_model = X_test\n",
    "\n",
    "    optimal_threshold, optimal_recall, thresholds, scores = find_optimal_threshold(\n",
    "        model, X_val, y_train, metric='f1', conservative=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Soglia ottimale: {optimal_threshold:.3f}\")\n",
    "    print(f\"f1 sul train: {optimal_recall:.4f}\")\n",
    "\n",
    "    # Valuta con soglia ottimale\n",
    "    metrics, y_pred, probas = evaluate_model(\n",
    "        model, X_test_model, y_test, f\"{model_name} (Ottimizzato)\", optimal_threshold # type: ignore\n",
    "    )\n",
    "    \n",
    "    results_optimized[model_name] = {\n",
    "        'metrics': metrics,\n",
    "        'y_pred': y_pred,\n",
    "        'probas': probas,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'threshold_scores': (thresholds, scores)\n",
    "    }\n",
    "\n",
    "# 8. VISUALIZZAZIONI\n",
    "\n",
    "# 8.1 Confronto metriche\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Confronto Performance Modelli', fontsize=16)\n",
    "\n",
    "# Prepara dati per confronto\n",
    "models = list(results_standard.keys())\n",
    "metrics_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "for idx, metric in enumerate(metrics_names):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    \n",
    "    standard_scores = [results_standard[m]['metrics'][metric] for m in models]\n",
    "    optimized_scores = [results_optimized[m]['metrics'][metric] \n",
    "                       for m in models if m in results_optimized]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, standard_scores, width, label='Soglia 0.5', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, optimized_scores, width, label='Soglia Ottimizzata', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Modello')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_title(f'{metric.capitalize()} per Modello')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Aggiungi valori sulle barre\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.2 Curve di ottimizzazione soglia\n",
    "fig, axes = plt.subplots(1, len(results_optimized), figsize=(15, 5))\n",
    "if len(results_optimized) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_optimized.items()):\n",
    "    thresholds, scores = result['threshold_scores']\n",
    "    optimal_threshold = result['optimal_threshold']\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.plot(thresholds, scores, 'b-', linewidth=2) # type: ignore\n",
    "    ax.axvline(optimal_threshold, color='red', linestyle='--',  # type: ignore\n",
    "               label=f'Soglia Ottimale: {optimal_threshold:.3f}')\n",
    "    ax.scatter([optimal_threshold], [scores[np.where(thresholds == optimal_threshold)[0][0]]],  # type: ignore\n",
    "               color='red', s=100, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Soglia') # type: ignore\n",
    "    ax.set_ylabel('Recall') # type: ignore\n",
    "    ax.set_title(f'Ottimizzazione Soglia - {model_name}')# type: ignore\n",
    "    ax.legend() # type: ignore\n",
    "    ax.grid(True, alpha=0.3) # type: ignore\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.3 ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in results_standard.keys():\n",
    "    if model_name in results_optimized:\n",
    "        probas = results_optimized[model_name]['probas']\n",
    "    else:\n",
    "        probas = results_standard[model_name]['probas']\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "    auc = roc_auc_score(y_test, probas)\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Confronto Modelli')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 8.4 Precision-Recall Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in results_standard.keys():\n",
    "    if model_name in results_optimized:\n",
    "        probas = results_optimized[model_name]['probas']\n",
    "    else:\n",
    "        probas = results_standard[model_name]['probas']\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, probas)\n",
    "    \n",
    "    plt.plot(recall, precision, linewidth=2, label=model_name)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - Confronto Modelli')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 8.5 Confusion Matrices\n",
    "n_models = len(results_optimized)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_optimized.items()):\n",
    "    cm = confusion_matrix(y_test, result['y_pred'])\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=True)\n",
    "    ax.set_title(f'Confusion Matrix - {model_name}\\n(Soglia: {result[\"optimal_threshold\"]:.3f})')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 9. REPORT FINALE\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REPORT FINALE - CONFRONTO MODELLI\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Tabella riassuntiva\n",
    "summary_data = []\n",
    "\n",
    "for model_name in results_standard.keys():\n",
    "    # Risultati con soglia standard\n",
    "    std_metrics = results_standard[model_name]['metrics']\n",
    "    row = {\n",
    "        'Modello': f\"{model_name} (Soglia 0.5)\",\n",
    "        'Accuracy': f\"{std_metrics['accuracy']:.4f}\",\n",
    "        'Precision': f\"{std_metrics['precision']:.4f}\",\n",
    "        'Recall': f\"{std_metrics['recall']:.4f}\",\n",
    "        'F1-Score': f\"{std_metrics['f1']:.4f}\",\n",
    "        'AUC-ROC': f\"{std_metrics['auc']:.4f}\"\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "    \n",
    "    # Risultati con soglia ottimizzata\n",
    "    if model_name in results_optimized:\n",
    "        opt_metrics = results_optimized[model_name]['metrics']\n",
    "        opt_threshold = results_optimized[model_name]['optimal_threshold']\n",
    "        row = {\n",
    "            'Modello': f\"{model_name} (Soglia {opt_threshold:.3f})\",\n",
    "            'Accuracy': f\"{opt_metrics['accuracy']:.4f}\",\n",
    "            'Precision': f\"{opt_metrics['precision']:.4f}\",\n",
    "            'Recall': f\"{opt_metrics['recall']:.4f}\",\n",
    "            'F1-Score': f\"{opt_metrics['f1']:.4f}\",\n",
    "            'AUC-ROC': f\"{opt_metrics['auc']:.4f}\"\n",
    "        }\n",
    "        summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\", summary_df.to_string(index=False))\n",
    "\n",
    "# Miglioramento del Recall\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MIGLIORAMENTO DEL RECALL CON OTTIMIZZAZIONE SOGLIA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name in results_standard.keys():\n",
    "    if model_name in results_optimized:\n",
    "        recall_before = results_standard[model_name]['metrics']['recall']\n",
    "        recall_after = results_optimized[model_name]['metrics']['recall']\n",
    "        improvement = (recall_after - recall_before) / recall_before * 100\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Recall prima: {recall_before:.4f}\")\n",
    "        print(f\"  Recall dopo:  {recall_after:.4f}\")\n",
    "        print(f\"  Miglioramento: {improvement:+.1f}%\")\n",
    "\n",
    "# 10. SALVATAGGIO MODELLO FINALE E CONFIGURAZIONE\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SALVATAGGIO CONFIGURAZIONE FINALE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Salva il voting classifier e la configurazione ottimale\n",
    "best_model_name = 'Voting'  # Puoi cambiare con il modello migliore\n",
    "best_threshold = results_optimized[best_model_name]['optimal_threshold']\n",
    "\n",
    "config = {\n",
    "    'model_type': 'voting_classifier',\n",
    "    'optimal_threshold': best_threshold,\n",
    "    'models_included': list(dict(voting_clf.estimators).keys()),\n",
    "    'performance_metrics': results_optimized[best_model_name]['metrics'],\n",
    "    'scalers': {name: scaler for name, scaler in scalers.items() if scaler is not None}\n",
    "}\n",
    "\n",
    "# Salva il modello\n",
    "joblib.dump(voting_clf, './models/voting/voting_classifier_final.pkl')\n",
    "joblib.dump(config, './models/voting/model_config.pkl')\n",
    "\n",
    "print(\"✓ Voting Classifier salvato in './models/voting/voting_classifier_final.pkl\")\n",
    "print(\"✓ Configurazione salvata in './models/voting/model_config.pkl\")\n",
    "print(f\"✓ Soglia ottimale per Recall: {best_threshold:.3f}\")\n",
    "print(f\"✓ Recall finale: {results_optimized[best_model_name]['metrics']['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8e467",
   "metadata": {},
   "source": [
    "### DEEP TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc400195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Crea 10 subset bilanciati di 50000 campioni ciascuno e controllo integrità e distribuzione delle classi\n",
    "num_subsets = 10\n",
    "subset_size = 50000\n",
    "\n",
    "subsets_X_filtered = []\n",
    "subsets_y_filtered = []\n",
    "\n",
    "# Trova gli indici delle due classi\n",
    "idx_0 = np.where(labels_clean == 0)[0]\n",
    "idx_1 = np.where(labels_clean == 1)[0]\n",
    "min_class_size = min(len(idx_0), len(idx_1), subset_size // 2)\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    idx_0_sample = np.random.choice(idx_0, size=min_class_size, replace=False)\n",
    "    idx_1_sample = np.random.choice(idx_1, size=min_class_size, replace=False)\n",
    "    idx_balanced = np.concatenate([idx_0_sample, idx_1_sample])\n",
    "    np.random.shuffle(idx_balanced)\n",
    "    subsets_X_filtered.append(fit_data_with_features_clean[idx_balanced])\n",
    "    subsets_y_filtered.append(labels_clean[idx_balanced])\n",
    "\n",
    "\n",
    "# Controlla la distribuzione delle classi in ogni subset\n",
    "for i, (X, y) in enumerate(zip(subsets_X_filtered, subsets_y_filtered)):\n",
    "    assert len(X) == subset_size, f\"Subset {i+1} size mismatch: {len(X)} != {subset_size}\"\n",
    "    assert len(y) == subset_size, f\"Subset {i+1} labels size mismatch: {len(y)} != {subset_size}\"\n",
    "    print(f\"Subset {i+1}: {Counter(y)}\")\n",
    "\n",
    "\n",
    "# Check if all elements in subsets_X_unfiltered are different\n",
    "all_unique = True\n",
    "for i in range(num_subsets):\n",
    "    for j in range(i + 1, num_subsets):\n",
    "        if np.array_equal(subsets_X_filtered[i], subsets_X_filtered[j]):\n",
    "            print(f\"Subset {i+1} and Subset {j+1} are identical!\")\n",
    "            all_unique = False\n",
    "if all_unique:\n",
    "    print(\"All subsets in subsets_X_filtered are different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10563059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SU DATASET BILANCIATO (50K SAMPLES)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SU DATASET BILANCIATO - 50.000 SAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Assegna i nuovi dati di test\n",
    "X_test_balanced = subsets_X_filtered\n",
    "y_test_balanced = subsets_y_filtered\n",
    "\n",
    "print(f\"\\nDimensioni test set bilanciato: {X_test_balanced.shape}\")\n",
    "#print(f\"Distribuzione classi: {np.bincount(y_test_balanced)}\")\n",
    "#print(f\"Bilanciamento: {np.bincount(y_test_balanced)[1] / len(y_test_balanced):.2%} classe positiva\")\n",
    "\n",
    "# Funzione per test esteso con analisi dettagliata\n",
    "def extended_test_evaluation(model, X_test, y_test, model_name, threshold, scaler=None):\n",
    "    \"\"\"Valutazione estesa con analisi per sottogruppi\"\"\"\n",
    "    \n",
    "    # Applica scaler se necessario\n",
    "    if scaler is not None:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_test_scaled = X_test\n",
    "    \n",
    "    # Calcola probabilità e predizioni\n",
    "    probas = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred = (probas >= threshold).astype(int)\n",
    "    \n",
    "    # Metriche generali\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, probas)\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    # Metriche aggiuntive\n",
    "    specificity = tn / (tn + fp)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{model_name} - Test Set Bilanciato (Soglia: {threshold:.3f})\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Accuracy:    {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision:   {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:      {metrics['recall']:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"F1-Score:    {metrics['f1']:.4f}\")\n",
    "    print(f\"AUC-ROC:     {metrics['auc']:.4f}\")\n",
    "    print(f\"NPV:         {npv:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"  TP: {tp:,}  FP: {fp:,}\")\n",
    "    print(f\"  FN: {fn:,}  TN: {tn:,}\")\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'probas': probas,\n",
    "        'y_pred': y_pred,\n",
    "        'confusion_matrix': (tn, fp, fn, tp),\n",
    "        'specificity': specificity,\n",
    "        'npv': npv\n",
    "    }\n",
    "\n",
    "# Test tutti i modelli con soglie ottimizzate\n",
    "results_balanced = {}\n",
    "\n",
    "# CatBoost\n",
    "if catboost_model_final is not None:\n",
    "    threshold = results_optimized['CatBoost']['optimal_threshold']\n",
    "    results = extended_test_evaluation(\n",
    "        catboost_model_final, X_test_balanced, y_test_balanced, \n",
    "        \"CatBoost\", threshold, catboost_scaler\n",
    "    )\n",
    "    results_balanced['CatBoost'] = results\n",
    "\n",
    "# XGBoost (se disponibile)\n",
    "if xg_boost_model_final is not None:\n",
    "    threshold = results_optimized['XGBoost']['optimal_threshold']\n",
    "    results = extended_test_evaluation(\n",
    "        xg_boost_model_final, X_test_balanced, y_test_balanced,\n",
    "        \"XGBoost\", threshold, xgboost_scaler\n",
    "    )\n",
    "    results_balanced['XGBoost'] = results\n",
    "\n",
    "# Voting Classifier\n",
    "threshold = results_optimized['Voting']['optimal_threshold']\n",
    "results = extended_test_evaluation(\n",
    "    voting_clf, X_test_balanced, y_test_balanced,\n",
    "    \"Voting Classifier\", threshold\n",
    ")\n",
    "results_balanced['Voting'] = results\n",
    "\n",
    "# 12. VISUALIZZAZIONI PER TEST SET BILANCIATO\n",
    "\n",
    "# 12.1 Confronto performance tra test set originale e bilanciato\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Confronto Performance: Test Set Originale vs Bilanciato (50K samples)', fontsize=16)\n",
    "\n",
    "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity']\n",
    "models_to_compare = ['CatBoost', 'Voting']  # Aggiungi 'XGBoost' se disponibile\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_compare):\n",
    "    ax = axes[idx//3, idx%3]\n",
    "    \n",
    "    x = np.arange(len(models_to_compare))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Valori dal test set originale\n",
    "    original_values = []\n",
    "    for model in models_to_compare:\n",
    "        if model in results_optimized:\n",
    "            if metric == 'specificity':\n",
    "                # Calcola specificity per test originale\n",
    "                cm = confusion_matrix(y_test, results_optimized[model]['y_pred'])\n",
    "                tn, fp = cm[0, 0], cm[0, 1]\n",
    "                spec = tn / (tn + fp)\n",
    "                original_values.append(spec)\n",
    "            else:\n",
    "                original_values.append(results_optimized[model]['metrics'].get(metric, 0))\n",
    "    \n",
    "    # Valori dal test set bilanciato\n",
    "    balanced_values = []\n",
    "    for model in models_to_compare:\n",
    "        if model in results_balanced:\n",
    "            if metric == 'specificity':\n",
    "                balanced_values.append(results_balanced[model]['specificity'])\n",
    "            else:\n",
    "                balanced_values.append(results_balanced[model]['metrics'].get(metric, 0))\n",
    "    \n",
    "    # Plot\n",
    "    bars1 = ax.bar(x - width/2, original_values, width, label='Test Originale', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, balanced_values, width, label='Test Bilanciato', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Modello')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_title(f'{metric.capitalize()}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models_to_compare)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Aggiungi valori\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 12.2 Distribuzione delle probabilità nel test set bilanciato\n",
    "fig, axes = plt.subplots(1, len(results_balanced), figsize=(15, 5))\n",
    "if len(results_balanced) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_balanced.items()):\n",
    "    probas = result['probas']\n",
    "    threshold = results_optimized[model_name]['optimal_threshold']\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Separa probabilità per classe\n",
    "    probas_class_0 = probas[y_test_balanced == 0]\n",
    "    probas_class_1 = probas[y_test_balanced == 1]\n",
    "    \n",
    "    # Box plot\n",
    "    data = [probas_class_0, probas_class_1]\n",
    "    bp = ax.boxplot(data, labels=['Classe 0', 'Classe 1'], patch_artist=True)\n",
    "    \n",
    "    # Colori\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    # Soglia\n",
    "    ax.axhline(threshold, color='green', linestyle='--', linewidth=2,\n",
    "               label=f'Soglia: {threshold:.3f}')\n",
    "    \n",
    "    ax.set_ylabel('Probabilità Classe 1')\n",
    "    ax.set_title(f'Distribuzione Probabilità - {model_name}\\n(Test Set Bilanciato)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 12.3 Analisi errori per intervalli di confidenza\n",
    "fig, axes = plt.subplots(1, len(results_balanced), figsize=(15, 6))\n",
    "if len(results_balanced) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_balanced.items()):\n",
    "    probas = result['probas']\n",
    "    y_pred = result['y_pred']\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Definisci intervalli di confidenza\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    bin_labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
    "    \n",
    "    # Calcola errori per intervallo\n",
    "    errors_by_bin = []\n",
    "    counts_by_bin = []\n",
    "    \n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (probas >= bins[i]) & (probas < bins[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            errors = (y_pred[mask] != y_test_balanced[mask]).mean()\n",
    "            errors_by_bin.append(errors)\n",
    "            counts_by_bin.append(mask.sum())\n",
    "        else:\n",
    "            errors_by_bin.append(0)\n",
    "            counts_by_bin.append(0)\n",
    "    \n",
    "    # Plot\n",
    "    x = np.arange(len(bin_labels))\n",
    "    bars = ax.bar(x, errors_by_bin, alpha=0.7)\n",
    "    \n",
    "    # Aggiungi count sopra le barre\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts_by_bin)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'n={count:,}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Intervallo di Probabilità')\n",
    "    ax.set_ylabel('Tasso di Errore')\n",
    "    ax.set_title(f'Errori per Intervallo di Confidenza - {model_name}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(bin_labels)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 12.4 Curve di calibrazione\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Curve di calibrazione\n",
    "ax1 = axes[0]\n",
    "for model_name, result in results_balanced.items():\n",
    "    probas = result['probas']\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_test_balanced, probas, n_bins=10\n",
    "    )\n",
    "    \n",
    "    ax1.plot(mean_predicted_value, fraction_of_positives, \n",
    "             marker='o', linewidth=2, label=model_name)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Perfettamente calibrato')\n",
    "ax1.set_xlabel('Probabilità Media Predetta')\n",
    "ax1.set_ylabel('Frazione di Positivi')\n",
    "ax1.set_title('Curve di Calibrazione - Test Set Bilanciato')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Istogramma delle predizioni\n",
    "ax2 = axes[1]\n",
    "for model_name, result in results_balanced.items():\n",
    "    probas = result['probas']\n",
    "    ax2.hist(probas, bins=20, alpha=0.5, label=model_name, density=True)\n",
    "\n",
    "ax2.set_xlabel('Probabilità Predetta')\n",
    "ax2.set_ylabel('Densità')\n",
    "ax2.set_title('Distribuzione delle Probabilità Predette')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 13. REPORT FINALE COMPARATIVO\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REPORT FINALE - CONFRONTO TEST SET ORIGINALE VS BILANCIATO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crea tabella comparativa\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in results_balanced.keys():\n",
    "    # Test originale\n",
    "    if model_name in results_optimized:\n",
    "        orig = results_optimized[model_name]['metrics']\n",
    "        comparison_data.append({\n",
    "            'Modello': model_name,\n",
    "            'Test Set': 'Originale',\n",
    "            'Samples': len(y_test),\n",
    "            'Accuracy': f\"{orig['accuracy']:.4f}\",\n",
    "            'Precision': f\"{orig['precision']:.4f}\",\n",
    "            'Recall': f\"{orig['recall']:.4f}\",\n",
    "            'F1-Score': f\"{orig['f1']:.4f}\",\n",
    "            'AUC-ROC': f\"{orig['auc']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Test bilanciato\n",
    "    bal = results_balanced[model_name]['metrics']\n",
    "    comparison_data.append({\n",
    "        'Modello': model_name,\n",
    "        'Test Set': 'Bilanciato',\n",
    "        'Samples': len(y_test_balanced),\n",
    "        'Accuracy': f\"{bal['accuracy']:.4f}\",\n",
    "        'Precision': f\"{bal['precision']:.4f}\",\n",
    "        'Recall': f\"{bal['recall']:.4f}\",\n",
    "        'F1-Score': f\"{bal['f1']:.4f}\",\n",
    "        'AUC-ROC': f\"{bal['auc']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# Analisi delle differenze\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALISI DELLE DIFFERENZE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name in results_balanced.keys():\n",
    "    if model_name in results_optimized:\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # Calcola differenze\n",
    "        for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "            orig_val = results_optimized[model_name]['metrics'][metric]\n",
    "            bal_val = results_balanced[model_name]['metrics'][metric]\n",
    "            diff = bal_val - orig_val\n",
    "            diff_pct = (diff / orig_val) * 100 if orig_val > 0 else 0\n",
    "            \n",
    "            print(f\"  {metric.capitalize():12} Δ = {diff:+.4f} ({diff_pct:+.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "best_recall = max(results_balanced.items(), key=lambda x: x[1]['metrics']['recall'])\n",
    "best_f1 = max(results_balanced.items(), key=lambda x: x[1]['metrics']['f1'])\n",
    "best_balanced = max(results_balanced.items(), \n",
    "                   key=lambda x: x[1]['metrics']['recall'] * x[1]['metrics']['precision'])\n",
    "\n",
    "print(f\"\\n✓ Miglior Recall: {best_recall[0]} ({best_recall[1]['metrics']['recall']:.4f})\")\n",
    "print(f\"✓ Miglior F1-Score: {best_f1[0]} ({best_f1[1]['metrics']['f1']:.4f})\")\n",
    "print(f\"✓ Miglior bilanciamento Precision-Recall: {best_balanced[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa760b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
