{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e257e919",
   "metadata": {},
   "source": [
    "### Routine per il fine tuning del modello "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a03794",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mlflow server --host 127.0.0.1 --port 8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7c2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import (train_test_split, RandomizedSearchCV, \n",
    "                                   StratifiedKFold, cross_val_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_curve, auc, precision_recall_curve,\n",
    "                           average_precision_score, confusion_matrix, \n",
    "                           classification_report)\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from xgboost import plot_importance\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# MLflow imports\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "\n",
    "from typing import Tuple, Optional, Union, List, Dict\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b6a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta il tracking URI di MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e123a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SampleConfig:\n",
    "    \"\"\"Configurazione per un singolo campionamento.\"\"\"\n",
    "    name: str\n",
    "    n_samples_per_class: int\n",
    "    random_state: int = 42\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetInfo:\n",
    "    \"\"\"Informazioni sul dataset.\"\"\"\n",
    "    n_samples: int\n",
    "    n_features: int\n",
    "    class_distribution: Dict[int, int]\n",
    "    features_columns: List[str]\n",
    "    labels_columns: List[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataLoader:\n",
    "    features_path: Union[str, Path]\n",
    "    labels_path: Union[str, Path]\n",
    "    _features_df_cache: Optional[pd.DataFrame] = field(default=None, init=False, repr=False)\n",
    "    _labels_df_cache: Optional[pd.DataFrame] = field(default=None, init=False, repr=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Converte i path in oggetti Path e verifica che esistano.\"\"\"\n",
    "        self.features_path = Path(self.features_path)\n",
    "        self.labels_path = Path(self.labels_path)\n",
    "        \n",
    "        if not self.features_path.exists():\n",
    "            raise FileNotFoundError(f\"Features file not found: {self.features_path}\")\n",
    "        if not self.labels_path.exists():\n",
    "            raise FileNotFoundError(f\"Labels file not found: {self.labels_path}\")\n",
    "    \n",
    "    def load_full_data(self, use_cache: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Carica tutti i dati senza campionamento.\n",
    "        \n",
    "        Args:\n",
    "            use_cache: Se True, usa i dati in cache se disponibili\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: features_df, labels_df\n",
    "        \"\"\"\n",
    "        if use_cache and self._features_df_cache is not None and self._labels_df_cache is not None:\n",
    "            return self._features_df_cache, self._labels_df_cache\n",
    "        \n",
    "        features_df = pd.read_parquet(self.features_path)\n",
    "        labels_df = pd.read_parquet(self.labels_path)\n",
    "        \n",
    "        # Verifica che abbiano lo stesso numero di righe\n",
    "        if len(features_df) != len(labels_df):\n",
    "            raise ValueError(f\"Features and labels have different lengths: \"\n",
    "                           f\"{len(features_df)} vs {len(labels_df)}\")\n",
    "        \n",
    "        if use_cache:\n",
    "            self._features_df_cache = features_df\n",
    "            self._labels_df_cache = labels_df\n",
    "        \n",
    "        return features_df, labels_df\n",
    "    \n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"Pulisce la cache dei dati.\"\"\"\n",
    "        self._features_df_cache = None\n",
    "        self._labels_df_cache = None\n",
    "    \n",
    "    def load_balanced_sample(self, n_samples_per_class: int, \n",
    "                           random_state: int = 42,\n",
    "                           use_cache: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Carica un campione bilanciato dei dati.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: features_df campionate, labels_df campionate\n",
    "        \"\"\"\n",
    "        # Carica i dati completi\n",
    "        features_df_full, labels_df_full = self.load_full_data(use_cache=use_cache)\n",
    "        \n",
    "        # Ottieni gli indici campionati in modo bilanciato\n",
    "        sampled_indices = self._get_balanced_indices(\n",
    "            labels_df_full, \n",
    "            n_samples_per_class, \n",
    "            random_state\n",
    "        )\n",
    "        \n",
    "        # Campiona i dati\n",
    "        features_df = features_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = labels_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        \n",
    "        return features_df, labels_df\n",
    "    \n",
    "    def load_balanced_sample_memory_efficient(self, n_samples_per_class: int, \n",
    "                                            random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Versione memory-efficient che libera la memoria dopo il campionamento.\n",
    "        \n",
    "        Args:\n",
    "            n_samples_per_class: Numero di campioni per ogni classe\n",
    "            random_state: Seed per la riproducibilit√†\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: features_df campionate, labels_df campionate\n",
    "        \"\"\"\n",
    "        # Carica i dati completi (senza cache)\n",
    "        features_df_full = pd.read_parquet(self.features_path)\n",
    "        labels_df_full = pd.read_parquet(self.labels_path)\n",
    "        \n",
    "        # Ottieni gli indici campionati\n",
    "        sampled_indices = self._get_balanced_indices(\n",
    "            labels_df_full, \n",
    "            n_samples_per_class, \n",
    "            random_state\n",
    "        )\n",
    "        \n",
    "        # Campiona i dati\n",
    "        features_df = features_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = labels_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Libera la memoria\n",
    "        del features_df_full, labels_df_full, sampled_indices\n",
    "        \n",
    "        return features_df, labels_df\n",
    "    \n",
    "\n",
    "    \n",
    "    def _get_balanced_indices(self, labels_df: pd.DataFrame, \n",
    "                            n_samples_per_class: int, \n",
    "                            random_state: int) -> pd.Index:\n",
    "        \"\"\"\n",
    "        Ottiene gli indici per un campionamento bilanciato.\n",
    "        \n",
    "        Args:\n",
    "            labels_df: DataFrame delle labels\n",
    "            n_samples_per_class: Numero di campioni per classe\n",
    "            random_state: Seed per la riproducibilit√†\n",
    "            \n",
    "        Returns:\n",
    "            pd.Index: Indici campionati\n",
    "        \"\"\"\n",
    "        # Usa la prima colonna per il groupby (assumendo sia la colonna delle classi)\n",
    "        label_column = labels_df.iloc[:, 0]\n",
    "        \n",
    "        # Verifica che ci siano abbastanza campioni per ogni classe\n",
    "        class_counts = label_column.value_counts()\n",
    "        for class_label, count in class_counts.items():\n",
    "            if count < n_samples_per_class:\n",
    "                raise ValueError(f\"Class {class_label} has only {count} samples, \"\n",
    "                               f\"but {n_samples_per_class} requested\")\n",
    "        \n",
    "        # Campiona gli indici\n",
    "        sampled_indices = (\n",
    "            labels_df.groupby(label_column)\n",
    "            .apply(lambda x: x.sample(n=n_samples_per_class, random_state=random_state))\n",
    "            .index.get_level_values(1)\n",
    "        )\n",
    "        \n",
    "        return sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0cee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: features=(40000, 134), labels=(40000, 1)\n",
      "Final set shape: features=(500000, 134), labels=(500000, 1)\n",
      "Class distribution in dev set set: 0\n",
      "0    20000\n",
      "1    20000\n",
      "Name: count, dtype: int64\n",
      "Class distribution in final set: 0\n",
      "0    250000\n",
      "1    250000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Carico un dataset pi√π piccolo a classi bilanciate per la ricerca degli iperparametri \n",
    "# e uno pi√π esteso per ri-trainare il modello con i parametri ottimali trovati\n",
    "\n",
    "loader = DataLoader(\n",
    "    features_path='processed_data/binary_classification/data_w_features/combined_features.parquet',\n",
    "    labels_path='processed_data/binary_classification/data_w_features/labels_binary_stats_features_only.parquet'\n",
    ")\n",
    "\n",
    "# Esempio 1: Carica 40k campioni bilanciati (20k per classe) per training\n",
    "features_df, labels_df = loader.load_balanced_sample_memory_efficient(n_samples_per_class=20000)\n",
    "print(f\"Training set shape: features={features_df.shape}, labels={labels_df.shape}\")\n",
    "\n",
    "# Esempio 2: Carica 500k campioni bilanciati (250k per classe) per fit finale\n",
    "features_df_to_finalize, labels_df_to_finalize = loader.load_balanced_sample_memory_efficient(\n",
    "    n_samples_per_class=250000\n",
    ")\n",
    "print(f\"Final set shape: features={features_df_to_finalize.shape}, labels={labels_df_to_finalize.shape}\")\n",
    "print(f\"Class distribution in dev set set: {labels_df.iloc[:, 0].value_counts()}\")\n",
    "print(f\"Class distribution in final set: {labels_df_to_finalize.iloc[:, 0].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2818a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostPipeline:\n",
    "    def __init__(self, features_df, labels_df, test_size=0.2, random_state=42, \n",
    "                 experiment_name=\"xgboost_binary_classification\", model_save_dir=\"./models\"):\n",
    "        \"\"\"\n",
    "        Inizializza la pipeline per XGBoost con integrazione MLflow\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features_df : pd.DataFrame\n",
    "            DataFrame contenente le features\n",
    "        labels_df : pd.DataFrame o pd.Series\n",
    "            DataFrame/Series contenente le labels\n",
    "        test_size : float\n",
    "            Percentuale del dataset da usare per il test set\n",
    "        random_state : int\n",
    "            Seed per la riproducibilit√†\n",
    "        experiment_name : str\n",
    "            Nome dell'esperimento MLflow\n",
    "        model_save_dir : str\n",
    "            Directory dove salvare modelli e scaler\n",
    "        \"\"\"\n",
    "        self.features_df = features_df\n",
    "        self.labels_df = labels_df.values.ravel() if isinstance(labels_df, pd.DataFrame) else labels_df\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.cv_results = None\n",
    "        self.scaler = None\n",
    "        \n",
    "        # MLflow setup\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.run_id = None\n",
    "        \n",
    "        # Crea directory per i modelli se non esiste\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "        \n",
    "        # Setup MLflow experiment\n",
    "        self._setup_mlflow_experiment()\n",
    "        \n",
    "    def _setup_mlflow_experiment(self):\n",
    "        \"\"\"Setup dell'esperimento MLflow\"\"\"\n",
    "        try:\n",
    "            # Verifica se l'esperimento esiste gi√†\n",
    "            experiment = mlflow.get_experiment_by_name(self.experiment_name)\n",
    "            if experiment is None:\n",
    "                mlflow.create_experiment(self.experiment_name)\n",
    "            \n",
    "            mlflow.set_experiment(self.experiment_name)\n",
    "            print(f\"‚úÖ MLflow experiment '{self.experiment_name}' configurato correttamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Errore nella configurazione MLflow: {e}\")\n",
    "            print(\"MLflow tracking sar√† disabilitato\")\n",
    "            \n",
    "    def _log_to_mlflow(self, key, value):\n",
    "        \"\"\"Helper per logging sicuro su MLflow\"\"\"\n",
    "        try:\n",
    "            if isinstance(value, (int, float)):\n",
    "                mlflow.log_metric(key, value)\n",
    "            else:\n",
    "                mlflow.log_param(key, value)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Errore nel logging MLflow per {key}: {e}\")\n",
    "            \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepara i dati dividendoli in train e test set\"\"\"\n",
    "        print(\"=== PREPARAZIONE DATI ===\")\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.features_df, self.labels_df, \n",
    "            test_size=self.test_size, \n",
    "            random_state=self.random_state,\n",
    "            stratify=self.labels_df\n",
    "        )\n",
    "        \n",
    "        # Normalizzazione delle features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        # Log info dataset su MLflow\n",
    "        try:\n",
    "            mlflow.log_param(\"dataset_total_samples\", len(self.features_df))\n",
    "            mlflow.log_param(\"train_samples\", self.X_train.shape[0])\n",
    "            mlflow.log_param(\"test_samples\", self.X_test.shape[0])\n",
    "            mlflow.log_param(\"n_features\", self.X_train.shape[1])\n",
    "            mlflow.log_param(\"test_size\", self.test_size)\n",
    "            mlflow.log_param(\"random_state\", self.random_state)\n",
    "            \n",
    "            # Log distribuzione classi\n",
    "            class_distribution = pd.Series(self.y_train).value_counts(normalize=True)\n",
    "            for class_label, proportion in class_distribution.items():\n",
    "                mlflow.log_metric(f\"class_{class_label}_proportion\", proportion)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Errore nel logging dei dati: {e}\")\n",
    "        \n",
    "        print(\"Dimensioni dataset:\")\n",
    "        print(f\"Train set: {self.X_train.shape[0]} campioni\")\n",
    "        print(f\"Test set: {self.X_test.shape[0]} campioni\")\n",
    "        print(f\"Numero di features: {self.X_train.shape[1]}\")\n",
    "        print(\"\\nDistribuzione classi nel training set:\")\n",
    "        print(pd.Series(self.y_train).value_counts(normalize=True))\n",
    "        \n",
    "    def hyperparameter_tuning(self, cv_folds=5, verbose=True, n_iter=250):\n",
    "        print(\"\\n=== HYPERPARAMETER TUNING ===\")\n",
    "        \n",
    "        self.param_distributions = {\n",
    "            'n_estimators': randint(500, 2000),\n",
    "            'max_depth': randint(3, 10),\n",
    "            'learning_rate': loguniform(0.01, 0.3),\n",
    "            'subsample': uniform(0.4, 0.6),     \n",
    "            'colsample_bytree': uniform(0.4, 0.6), \n",
    "            'colsample_bylevel': uniform(0.4, 0.6),\n",
    "            'colsample_bynode': uniform(0.4, 0.6), \n",
    "            'gamma': loguniform(1e-8, 1),\n",
    "            'reg_alpha': loguniform(1e-8, 10),   \n",
    "            'reg_lambda': loguniform(1e-8, 10),\n",
    "            'min_child_weight': randint(1, 10),\n",
    "            'scale_pos_weight': uniform(0.5, 5.0),\n",
    "            'booster': ['gbtree', 'dart'],\n",
    "        }\n",
    "        \n",
    "        # Log parametri di ricerca su MLflow\n",
    "        try:\n",
    "            mlflow.log_param(\"cv_folds\", cv_folds)\n",
    "            mlflow.log_param(\"search_iterations\", n_iter)\n",
    "            mlflow.log_param(\"search_scoring\", \"roc_auc\")\n",
    "            \n",
    "            # Log spazio di ricerca\n",
    "            for param, distribution in self.param_distributions.items():\n",
    "                if hasattr(distribution, 'args'):\n",
    "                    mlflow.log_param(f\"search_space_{param}\", str(distribution.args))\n",
    "                else:\n",
    "                    mlflow.log_param(f\"search_space_{param}\", str(distribution))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Errore nel logging parametri di ricerca: {e}\")\n",
    "\n",
    "        n_cores = multiprocessing.cpu_count()\n",
    "        n_jobs = np.floor(0.75 * n_cores).astype(int)\n",
    "        print(f\"Using {n_jobs} cores for hyperparameter tuning (75% of {n_cores} total cores).\")\n",
    "        \n",
    "        # Modello base\n",
    "        base_model = xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            tree_method='hist',\n",
    "            use_label_encoder=False,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "    \n",
    "        # Stratified K-Fold\n",
    "        stratified_kfold = StratifiedKFold(\n",
    "            n_splits=cv_folds, \n",
    "            shuffle=True, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # RandomizedSearchCV\n",
    "        self.grid_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=self.param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=stratified_kfold,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=2,\n",
    "            random_state=self.random_state,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Esegui la ricerca\n",
    "        print(\"Avvio ricerca iperparametri...\")\n",
    "        self.grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        self.best_params = self.grid_search.best_params_\n",
    "        self.cv_results = pd.DataFrame(self.grid_search.cv_results_)\n",
    "        \n",
    "        # Log risultati migliori su MLflow\n",
    "        try:\n",
    "            mlflow.log_metric(\"best_cv_score\", self.grid_search.best_score_)\n",
    "            \n",
    "            # Log migliori parametri\n",
    "            for param, value in self.best_params.items():\n",
    "                mlflow.log_param(f\"best_{param}\", value)\n",
    "                \n",
    "            # Salva risultati CV completi\n",
    "            cv_results_path = os.path.join(self.model_save_dir, \"cv_results.csv\")\n",
    "            self.cv_results.to_csv(cv_results_path, index=False)\n",
    "            mlflow.log_artifact(cv_results_path, \"hyperparameter_search\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Errore nel logging risultati: {e}\")\n",
    "        \n",
    "        print(\"\\nMigliori parametri trovati:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nMiglior score ROC-AUC (CV): {self.grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        # Visualizza i risultati della ricerca\n",
    "        self._plot_hyperparameter_results()\n",
    "        \n",
    "    def train_final_model(self):\n",
    "        \"\"\"Addestra il modello finale con i migliori parametri\"\"\"\n",
    "        print(\"\\n=== ADDESTRAMENTO MODELLO FINALE ===\")\n",
    "        \n",
    "        self.model = xgb.XGBClassifier(\n",
    "            **self.best_params,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        \n",
    "        # Addestra con early stopping\n",
    "        eval_set = [(self.X_train_scaled, self.y_train), (self.X_test_scaled, self.y_test)]\n",
    "        self.model.fit(\n",
    "            self.X_train_scaled, self.y_train,\n",
    "            eval_set=eval_set,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predizioni\n",
    "        self.y_pred = self.model.predict(self.X_test_scaled)\n",
    "        self.y_pred_proba = self.model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calcola tutte le metriche\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, self.y_pred),\n",
    "            'precision': precision_score(self.y_test, self.y_pred),\n",
    "            'recall': recall_score(self.y_test, self.y_pred),\n",
    "            'f1_score': f1_score(self.y_test, self.y_pred),\n",
    "            'roc_auc': auc(*roc_curve(self.y_test, self.y_pred_proba)[:2]),\n",
    "            'average_precision': average_precision_score(self.y_test, self.y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # Log metriche su MLflow\n",
    "        try:\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"test_{metric_name}\", metric_value)\n",
    "                \n",
    "            # Log numero di alberi utilizzati\n",
    "            mlflow.log_metric(\"n_estimators_used\", self.model.n_estimators)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Errore nel logging metriche: {e}\")\n",
    "        \n",
    "        # Stampa metriche\n",
    "        print(\"\\nMetriche sul test set:\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            print(f\"{metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "            \n",
    "    def save_model_and_scaler(self, model_name=None):\n",
    "        \"\"\"Salva il modello addestrato e lo scaler\"\"\"\n",
    "        print(\"\\n=== SALVATAGGIO MODELLO E SCALER ===\")\n",
    "        \n",
    "        if model_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_name = f\"xgboost_model_{timestamp}\"\n",
    "        \n",
    "        # Percorsi file\n",
    "        model_path = os.path.join(self.model_save_dir, f\"{model_name}.joblib\")\n",
    "        scaler_path = os.path.join(self.model_save_dir, f\"{model_name}_scaler.joblib\")\n",
    "        metadata_path = os.path.join(self.model_save_dir, f\"{model_name}_metadata.json\")\n",
    "        \n",
    "        try:\n",
    "            # Salva modello\n",
    "            joblib.dump(self.model, model_path)\n",
    "            print(f\"‚úÖ Modello salvato: {model_path}\")\n",
    "            \n",
    "            # Salva scaler\n",
    "            joblib.dump(self.scaler, scaler_path)\n",
    "            print(f\"‚úÖ Scaler salvato: {scaler_path}\")\n",
    "            \n",
    "            # Salva metadata\n",
    "            metadata = {\n",
    "                'model_name': model_name,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'best_params': self.best_params,\n",
    "                'cv_score': float(self.grid_search.best_score_),\n",
    "                'test_metrics': {\n",
    "                    'accuracy': float(accuracy_score(self.y_test, self.y_pred)),\n",
    "                    'precision': float(precision_score(self.y_test, self.y_pred)),\n",
    "                    'recall': float(recall_score(self.y_test, self.y_pred)),\n",
    "                    'f1_score': float(f1_score(self.y_test, self.y_pred)),\n",
    "                    'roc_auc': float(auc(*roc_curve(self.y_test, self.y_pred_proba)[:2]))\n",
    "                },\n",
    "                'feature_names': list(self.features_df.columns),\n",
    "                'n_features': len(self.features_df.columns),\n",
    "                'train_samples': len(self.y_train),\n",
    "                'test_samples': len(self.y_test)\n",
    "            }\n",
    "            \n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            print(f\"‚úÖ Metadata salvati: {metadata_path}\")\n",
    "            \n",
    "            # Log su MLflow\n",
    "            try:\n",
    "                # Log modello con MLflow (formato nativo XGBoost)\n",
    "                signature = infer_signature(self.X_train_scaled, self.y_pred_proba)\n",
    "                mlflow.xgboost.log_model(\n",
    "                    self.model, \n",
    "                    \"xgboost_model\",\n",
    "                    signature=signature\n",
    "                )\n",
    "                \n",
    "                # Log scaler come artifact\n",
    "                mlflow.log_artifact(scaler_path, \"preprocessing\")\n",
    "                mlflow.log_artifact(metadata_path, \"model_info\")\n",
    "                \n",
    "                # Log paths come parametri\n",
    "                mlflow.log_param(\"model_save_path\", model_path)\n",
    "                mlflow.log_param(\"scaler_save_path\", scaler_path)\n",
    "                mlflow.log_param(\"model_name\", model_name)\n",
    "                \n",
    "                print(\"‚úÖ Modello e scaler loggati su MLflow\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Errore nel logging MLflow: {e}\")\n",
    "            \n",
    "            return model_path, scaler_path, metadata_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore nel salvataggio: {e}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model_and_scaler(model_path, scaler_path):\n",
    "        \"\"\"Carica modello e scaler salvati\"\"\"\n",
    "        try:\n",
    "            model = joblib.load(model_path)\n",
    "            scaler = joblib.load(scaler_path)\n",
    "            print(f\"‚úÖ Modello caricato da: {model_path}\")\n",
    "            print(f\"‚úÖ Scaler caricato da: {scaler_path}\")\n",
    "            return model, scaler\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore nel caricamento: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def predict_new_data(self, new_data, model_path=None, scaler_path=None):\n",
    "        \"\"\"Predice su nuovi dati usando il modello addestrato\"\"\"\n",
    "        # Se i percorsi non sono forniti, usa il modello corrente\n",
    "        if model_path and scaler_path:\n",
    "            model, scaler = self.load_model_and_scaler(model_path, scaler_path)\n",
    "        else:\n",
    "            model, scaler = self.model, self.scaler\n",
    "            \n",
    "        if model is None or scaler is None:\n",
    "            print(\"‚ùå Modello o scaler non disponibili\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Scala i nuovi dati\n",
    "            new_data_scaled = scaler.transform(new_data)\n",
    "            \n",
    "            # Predizioni\n",
    "            predictions = model.predict(new_data_scaled)\n",
    "            probabilities = model.predict_proba(new_data_scaled)[:, 1]\n",
    "            \n",
    "            return predictions, probabilities\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore nella predizione: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def finalize_model(self, features_df_to_finalize, labels_df_to_finalize, \n",
    "                      model_path, scaler_path, \n",
    "                      refit_scaler=True, use_early_stopping=True,\n",
    "                      final_model_name=None, experiment_suffix=\"_finalized\"):\n",
    "        \"\"\"\n",
    "        Carica un modello esistente e lo riallena sui dati finali\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features_df_to_finalize : pd.DataFrame\n",
    "            Nuove features per il training finale\n",
    "        labels_df_to_finalize : pd.DataFrame o pd.Series\n",
    "            Nuove labels per il training finale\n",
    "        model_path : str\n",
    "            Percorso del modello salvato da caricare\n",
    "        scaler_path : str\n",
    "            Percorso dello scaler salvato da caricare\n",
    "        refit_scaler : bool\n",
    "            Se True, rifatta lo scaler sui nuovi dati. Se False, usa la trasformazione esistente\n",
    "        use_early_stopping : bool\n",
    "            Se utilizzare early stopping durante il riallenaemnto\n",
    "        final_model_name : str\n",
    "            Nome per il modello finalizzato (se None, auto-generato)\n",
    "        experiment_suffix : str\n",
    "            Suffisso da aggiungere al nome dell'esperimento MLflow\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (final_model, final_scaler, run_id)\n",
    "        \"\"\"\n",
    "        print(\"=== FINALIZZAZIONE MODELLO ===\")\n",
    "        \n",
    "        # Prepara i dati finali\n",
    "        labels_to_finalize = labels_df_to_finalize.values.ravel() if isinstance(labels_df_to_finalize, pd.DataFrame) else labels_df_to_finalize\n",
    "        \n",
    "        # Carica modello e scaler esistenti\n",
    "        print(f\"üìÇ Caricamento modello da: {model_path}\")\n",
    "        print(f\"üìÇ Caricamento scaler da: {scaler_path}\")\n",
    "        \n",
    "        base_model, base_scaler = self.load_model_and_scaler(model_path, scaler_path)\n",
    "        if base_model is None or base_scaler is None:\n",
    "            print(\"‚ùå Impossibile caricare modello o scaler\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Setup esperimento MLflow per finalizzazione\n",
    "        original_experiment_name = self.experiment_name\n",
    "        finalization_experiment_name = original_experiment_name + experiment_suffix\n",
    "        \n",
    "        try:\n",
    "            experiment = mlflow.get_experiment_by_name(finalization_experiment_name)\n",
    "            if experiment is None:\n",
    "                mlflow.create_experiment(finalization_experiment_name)\n",
    "            mlflow.set_experiment(finalization_experiment_name)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Errore setup MLflow: {e}\")\n",
    "        \n",
    "        # Avvia MLflow run per finalizzazione\n",
    "        with mlflow.start_run() as run:\n",
    "            finalization_run_id = run.info.run_id\n",
    "            print(f\"üöÄ MLflow Finalization Run ID: {finalization_run_id}\")\n",
    "            \n",
    "            try:\n",
    "                # Log informazioni sulla finalizzazione\n",
    "                mlflow.log_param(\"base_model_path\", model_path)\n",
    "                mlflow.log_param(\"base_scaler_path\", scaler_path)\n",
    "                mlflow.log_param(\"finalization_samples\", len(features_df_to_finalize))\n",
    "                mlflow.log_param(\"finalization_features\", len(features_df_to_finalize.columns))\n",
    "                mlflow.log_param(\"refit_scaler\", refit_scaler)\n",
    "                mlflow.log_param(\"use_early_stopping\", use_early_stopping)\n",
    "                \n",
    "                # Log distribuzione classi nei dati finali\n",
    "                class_distribution = pd.Series(labels_to_finalize).value_counts(normalize=True)\n",
    "                for class_label, proportion in class_distribution.items():\n",
    "                    mlflow.log_metric(f\"final_class_{class_label}_proportion\", proportion)\n",
    "                \n",
    "                print(f\"üìä Dati finali: {len(features_df_to_finalize)} campioni, {len(features_df_to_finalize.columns)} features\")\n",
    "                print(\"Distribuzione classi nei dati finali:\")\n",
    "                print(pd.Series(labels_to_finalize).value_counts(normalize=True))\n",
    "                \n",
    "                # Gestione scaler\n",
    "                if refit_scaler:\n",
    "                    print(\"üîÑ Riallenaemnto scaler sui nuovi dati...\")\n",
    "                    final_scaler = StandardScaler()\n",
    "                    features_scaled = final_scaler.fit_transform(features_df_to_finalize)\n",
    "                    mlflow.log_param(\"scaler_action\", \"refit_on_new_data\")\n",
    "                else:\n",
    "                    print(\"üìã Utilizzo scaler esistente...\")\n",
    "                    final_scaler = base_scaler\n",
    "                    features_scaled = final_scaler.transform(features_df_to_finalize)\n",
    "                    mlflow.log_param(\"scaler_action\", \"use_existing\")\n",
    "                \n",
    "                # Ottieni parametri del modello base\n",
    "                base_params = base_model.get_params()\n",
    "                print(f\"üéØ Parametri modello base: {len(base_params)} parametri\")\n",
    "                \n",
    "                # Log parametri del modello base\n",
    "                for param, value in base_params.items():\n",
    "                    if param not in ['random_state', 'n_jobs', 'objective', 'eval_metric', 'use_label_encoder']:\n",
    "                        mlflow.log_param(f\"base_{param}\", value)\n",
    "                \n",
    "                # Crea nuovo modello con stessi parametri\n",
    "                final_model = xgb.XGBClassifier(**base_params)\n",
    "                \n",
    "                # Training finale\n",
    "                print(\"üöÄ Avvio training finale...\")\n",
    "                \n",
    "                if use_early_stopping and len(features_df_to_finalize) > 100:\n",
    "                    # Se abbiamo abbastanza dati, usa una piccola porzione per validation\n",
    "                    val_size = min(0.1, 100 / len(features_df_to_finalize))\n",
    "                    X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "                        features_scaled, labels_to_finalize,\n",
    "                        test_size=val_size,\n",
    "                        random_state=self.random_state,\n",
    "                        stratify=labels_to_finalize\n",
    "                    )\n",
    "                    \n",
    "                    eval_set = [(X_train_final, y_train_final), (X_val_final, y_val_final)]\n",
    "                    final_model.fit(\n",
    "                        X_train_final, y_train_final,\n",
    "                        eval_set=eval_set,\n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    \n",
    "                    mlflow.log_param(\"validation_split_used\", True)\n",
    "                    mlflow.log_param(\"validation_size\", val_size)\n",
    "                    \n",
    "                    # Predizioni finali su validation set\n",
    "                    y_val_pred = final_model.predict(X_val_final)\n",
    "                    y_val_pred_proba = final_model.predict_proba(X_val_final)[:, 1]\n",
    "                    \n",
    "                    # Metriche finali\n",
    "                    final_metrics = {\n",
    "                        'final_accuracy': accuracy_score(y_val_final, y_val_pred),\n",
    "                        'final_precision': precision_score(y_val_final, y_val_pred),\n",
    "                        'final_recall': recall_score(y_val_final, y_val_pred),\n",
    "                        'final_f1_score': f1_score(y_val_final, y_val_pred),\n",
    "                        'final_roc_auc': auc(*roc_curve(y_val_final, y_val_pred_proba)[:2])\n",
    "                    }\n",
    "                    \n",
    "                else:\n",
    "                    # Training su tutti i dati senza early stopping\n",
    "                    final_model.fit(features_scaled, labels_to_finalize, verbose=False)\n",
    "                    mlflow.log_param(\"validation_split_used\", False)\n",
    "                    \n",
    "                    # Metriche sui dati di training (training accuracy)\n",
    "                    y_train_pred = final_model.predict(features_scaled)\n",
    "                    y_train_pred_proba = final_model.predict_proba(features_scaled)[:, 1]\n",
    "                    \n",
    "                    final_metrics = {\n",
    "                        'final_train_accuracy': accuracy_score(labels_to_finalize, y_train_pred),\n",
    "                        'final_train_precision': precision_score(labels_to_finalize, y_train_pred),\n",
    "                        'final_train_recall': recall_score(labels_to_finalize, y_train_pred),\n",
    "                        'final_train_f1_score': f1_score(labels_to_finalize, y_train_pred),\n",
    "                        'final_train_roc_auc': auc(*roc_curve(labels_to_finalize, y_train_pred_proba)[:2])\n",
    "                    }\n",
    "                \n",
    "                # Log metriche finali\n",
    "                for metric_name, metric_value in final_metrics.items():\n",
    "                    mlflow.log_metric(metric_name, metric_value)\n",
    "                \n",
    "                print(\"\\nüìà Metriche finali:\")\n",
    "                for metric_name, metric_value in final_metrics.items():\n",
    "                    print(f\"  {metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "                \n",
    "                # Confronto importanza features\n",
    "                print(\"\\nüîç Analisi feature importance...\")\n",
    "                base_importance = pd.DataFrame({\n",
    "                    'feature': features_df_to_finalize.columns,\n",
    "                    'base_importance': base_model.feature_importances_\n",
    "                }).sort_values('base_importance', ascending=False)\n",
    "                \n",
    "                final_importance = pd.DataFrame({\n",
    "                    'feature': features_df_to_finalize.columns,\n",
    "                    'final_importance': final_model.feature_importances_\n",
    "                }).sort_values('final_importance', ascending=False)\n",
    "                \n",
    "                # Merge e analisi differenze\n",
    "                importance_comparison = base_importance.merge(\n",
    "                    final_importance, on='feature', how='inner'\n",
    "                )\n",
    "                importance_comparison['importance_change'] = (\n",
    "                    importance_comparison['final_importance'] - importance_comparison['base_importance']\n",
    "                )\n",
    "                \n",
    "                # Log top features che sono cambiate di pi√π\n",
    "                top_changes = importance_comparison.reindex(\n",
    "                    importance_comparison['importance_change'].abs().sort_values(ascending=False).index\n",
    "                ).head(10)\n",
    "                \n",
    "                print(\"Top 10 feature con maggiori cambiamenti in importanza:\")\n",
    "                for _, row in top_changes.iterrows():\n",
    "                    change_pct = (row['importance_change'] / (row['base_importance'] + 1e-8)) * 100\n",
    "                    print(f\"  {row['feature']}: {row['importance_change']:+.4f} ({change_pct:+.1f}%)\")\n",
    "                \n",
    "                # Salva modello e scaler finalizzati\n",
    "                if final_model_name is None:\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    final_model_name = f\"finalized_model_{timestamp}\"\n",
    "                \n",
    "                final_model_path = os.path.join(self.model_save_dir, f\"{final_model_name}.joblib\")\n",
    "                final_scaler_path = os.path.join(self.model_save_dir, f\"{final_model_name}_scaler.joblib\")\n",
    "                final_metadata_path = os.path.join(self.model_save_dir, f\"{final_model_name}_metadata.json\")\n",
    "                importance_comparison_path = os.path.join(self.model_save_dir, f\"{final_model_name}_importance_comparison.csv\")\n",
    "                \n",
    "                # Salva tutto\n",
    "                joblib.dump(final_model, final_model_path)\n",
    "                joblib.dump(final_scaler, final_scaler_path)\n",
    "                importance_comparison.to_csv(importance_comparison_path, index=False)\n",
    "                \n",
    "                # Metadata finali\n",
    "                final_metadata = {\n",
    "                    'model_name': final_model_name,\n",
    "                    'finalization_timestamp': datetime.now().isoformat(),\n",
    "                    'base_model_path': model_path,\n",
    "                    'base_scaler_path': scaler_path,\n",
    "                    'finalization_samples': len(features_df_to_finalize),\n",
    "                    'finalization_features': len(features_df_to_finalize.columns),\n",
    "                    'refit_scaler': refit_scaler,\n",
    "                    'use_early_stopping': use_early_stopping,\n",
    "                    'final_metrics': {k: float(v) for k, v in final_metrics.items()},\n",
    "                    'model_params': {k: v for k, v in base_params.items() if k not in ['random_state', 'n_jobs']},\n",
    "                    'feature_names': list(features_df_to_finalize.columns),\n",
    "                    'class_distribution': {str(k): float(v) for k, v in class_distribution.items()}\n",
    "                }\n",
    "                \n",
    "                with open(final_metadata_path, 'w') as f:\n",
    "                    json.dump(final_metadata, f, indent=2)\n",
    "                \n",
    "                # MLflow logging\n",
    "                try:\n",
    "                    # Log modello finalizzato\n",
    "                    signature = infer_signature(features_scaled, final_model.predict_proba(features_scaled))\n",
    "                    mlflow.xgboost.log_model(\n",
    "                        final_model, \n",
    "                        \"finalized_xgboost_model\",\n",
    "                        signature=signature\n",
    "                    )\n",
    "                    \n",
    "                    # Log artifacts\n",
    "                    mlflow.log_artifact(final_scaler_path, \"preprocessing\")\n",
    "                    mlflow.log_artifact(final_metadata_path, \"model_info\")\n",
    "                    mlflow.log_artifact(importance_comparison_path, \"analysis\")\n",
    "                    \n",
    "                    # Log parametri finali\n",
    "                    mlflow.log_param(\"final_model_path\", final_model_path)\n",
    "                    mlflow.log_param(\"final_scaler_path\", final_scaler_path)\n",
    "                    mlflow.log_param(\"final_model_name\", final_model_name)\n",
    "                    \n",
    "                    # Grafico confronto feature importance\n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    top_15_comparison = importance_comparison.head(15)\n",
    "                    \n",
    "                    x = np.arange(len(top_15_comparison))\n",
    "                    width = 0.35\n",
    "                    \n",
    "                    ax.bar(x - width/2, top_15_comparison['base_importance'], width, \n",
    "                          label='Modello Base', alpha=0.8)\n",
    "                    ax.bar(x + width/2, top_15_comparison['final_importance'], width, \n",
    "                          label='Modello Finalizzato', alpha=0.8)\n",
    "                    \n",
    "                    ax.set_xlabel('Features')\n",
    "                    ax.set_ylabel('Importance Score')\n",
    "                    ax.set_title('Confronto Feature Importance: Base vs Finalizzato')\n",
    "                    ax.set_xticks(x)\n",
    "                    ax.set_xticklabels(top_15_comparison['feature'], rotation=45, ha='right')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    # Salva e log grafico\n",
    "                    plots_dir = os.path.join(self.model_save_dir, \"finalization_plots\")\n",
    "                    os.makedirs(plots_dir, exist_ok=True)\n",
    "                    comparison_plot_path = os.path.join(plots_dir, f\"{final_model_name}_importance_comparison.png\")\n",
    "                    plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "                    mlflow.log_artifact(comparison_plot_path, \"plots\")\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print(\"‚úÖ Artefatti salvati su MLflow\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Errore nel logging MLflow: {e}\")\n",
    "                \n",
    "                # Tag del run\n",
    "                mlflow.set_tag(\"pipeline_status\", \"finalized\")\n",
    "                mlflow.set_tag(\"model_type\", \"xgboost_finalized\")\n",
    "                mlflow.set_tag(\"base_model_source\", model_path)\n",
    "                \n",
    "                print(\"\\n=== FINALIZZAZIONE COMPLETATA ===\")\n",
    "                print(f\"üéØ MLflow Run: {finalization_run_id}\")\n",
    "                print(f\"üíæ Modello finalizzato: {final_model_path}\")\n",
    "                print(f\"‚öôÔ∏è  Scaler finalizzato: {final_scaler_path}\")\n",
    "                print(f\"üìä Analisi importance: {importance_comparison_path}\")\n",
    "                \n",
    "                # Aggiorna attributi della classe\n",
    "                self.model = final_model\n",
    "                self.scaler = final_scaler\n",
    "                \n",
    "                return final_model, final_scaler, finalization_run_id\n",
    "                \n",
    "            except Exception as e:\n",
    "                mlflow.set_tag(\"pipeline_status\", \"failed\")\n",
    "                mlflow.log_param(\"error_message\", str(e))\n",
    "                print(f\"‚ùå Errore nella finalizzazione: {e}\")\n",
    "                raise\n",
    "                \n",
    "        # Ripristina esperimento originale\n",
    "        try:\n",
    "            mlflow.set_experiment(original_experiment_name)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    def plot_all_visualizations(self):\n",
    "        \"\"\"Genera tutte le visualizzazioni e le salva su MLflow\"\"\"\n",
    "        print(\"\\n=== GENERAZIONE VISUALIZZAZIONI ===\")\n",
    "        \n",
    "        try:\n",
    "            # Crea directory temporanea per i grafici\n",
    "            plots_dir = os.path.join(self.model_save_dir, \"plots\")\n",
    "            os.makedirs(plots_dir, exist_ok=True)\n",
    "            \n",
    "            # 1. Feature Importance\n",
    "            self._plot_feature_importance(save_dir=plots_dir)\n",
    "            \n",
    "            # 2. Curva ROC\n",
    "            self._plot_roc_curve(save_dir=plots_dir)\n",
    "            \n",
    "            # 3. Curva Precision-Recall\n",
    "            self._plot_pr_curve(save_dir=plots_dir)\n",
    "            \n",
    "            # 4. Matrice di Confusione\n",
    "            self._plot_confusion_matrix(save_dir=plots_dir)\n",
    "            \n",
    "            # 5. Distribuzione delle probabilit√† predette\n",
    "            self._plot_probability_distribution(save_dir=plots_dir)\n",
    "            \n",
    "            # 6. Learning Curves\n",
    "            self._plot_learning_curves(save_dir=plots_dir)\n",
    "            \n",
    "            # 7. Calibration Plot\n",
    "            self._plot_calibration_curve(save_dir=plots_dir)\n",
    "            \n",
    "            # 8. Classification Report Heatmap\n",
    "            self._plot_classification_report(save_dir=plots_dir)\n",
    "            \n",
    "            # Log tutti i grafici su MLflow\n",
    "            try:\n",
    "                for plot_file in os.listdir(plots_dir):\n",
    "                    if plot_file.endswith('.png'):\n",
    "                        mlflow.log_artifact(os.path.join(plots_dir, plot_file), \"plots\")\n",
    "                print(\"‚úÖ Grafici salvati su MLflow\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Errore nel logging grafici: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Errore nella generazione grafici: {e}\")\n",
    "        \n",
    "    def _plot_hyperparameter_results(self, save_dir=None):\n",
    "        \"\"\"Visualizza i risultati della ricerca degli iperparametri\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Top 20 combinazioni\n",
    "        top_results = self.cv_results.nlargest(20, 'mean_test_score')\n",
    "        \n",
    "        # 1. Convergenza durante la ricerca\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(range(len(self.cv_results)), self.cv_results['mean_test_score'])\n",
    "        ax.axhline(y=self.grid_search.best_score_, color='r', linestyle='--', label='Best Score')\n",
    "        ax.set_xlabel('Iterazione')\n",
    "        ax.set_ylabel('ROC-AUC Score')\n",
    "        ax.set_title('Convergenza della Grid Search')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Importanza relativa dei parametri\n",
    "        ax = axes[0, 1]\n",
    "        param_importance = {}\n",
    "        for param in self.best_params.keys():\n",
    "            col = f'param_{param}'\n",
    "            if col in self.cv_results.columns:\n",
    "                # Per parametri categorici, usiamo varianza\n",
    "                if self.cv_results[col].dtype == 'object':\n",
    "                    unique_vals = self.cv_results[col].nunique()\n",
    "                    param_importance[param] = unique_vals / len(self.cv_results)\n",
    "                else:\n",
    "                    correlation = self.cv_results[[col, 'mean_test_score']].corr().iloc[0, 1]\n",
    "                    if not np.isnan(correlation):\n",
    "                        param_importance[param] = abs(correlation)\n",
    "        \n",
    "        if param_importance:\n",
    "            pd.Series(param_importance).sort_values().plot(kind='barh', ax=ax)\n",
    "            ax.set_title('Importanza Relativa dei Parametri')\n",
    "            ax.set_xlabel('Score di Importanza')\n",
    "        \n",
    "        # 3. Distribuzione dei migliori score\n",
    "        ax = axes[1, 0]\n",
    "        ax.hist(self.cv_results['mean_test_score'], bins=30, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(self.grid_search.best_score_, color='red', linestyle='--', \n",
    "                  label=f'Best: {self.grid_search.best_score_:.4f}')\n",
    "        ax.set_xlabel('ROC-AUC Score')\n",
    "        ax.set_ylabel('Frequenza')\n",
    "        ax.set_title('Distribuzione degli Score CV')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Tempo di training vs performance\n",
    "        ax = axes[1, 1]\n",
    "        if 'mean_fit_time' in self.cv_results.columns:\n",
    "            scatter = ax.scatter(self.cv_results['mean_fit_time'], \n",
    "                               self.cv_results['mean_test_score'],\n",
    "                               c=self.cv_results['mean_test_score'],\n",
    "                               cmap='viridis', alpha=0.6)\n",
    "            ax.set_xlabel('Tempo di Training (secondi)')\n",
    "            ax.set_ylabel('ROC-AUC Score')\n",
    "            ax.set_title('Trade-off Tempo vs Performance')\n",
    "            plt.colorbar(scatter, ax=ax)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'hyperparameter_results.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_feature_importance(self, save_dir=None):\n",
    "        \"\"\"Visualizza l'importanza delle features\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "        \n",
    "        # 1. XGBoost native importance\n",
    "        ax = axes[0]\n",
    "        plot_importance(self.model, max_num_features=20, ax=ax, importance_type='weight')\n",
    "        ax.set_title('Feature Importance - Weight (Frequenza di utilizzo)')\n",
    "        \n",
    "        # 2. Gain importance\n",
    "        ax = axes[1]\n",
    "        plot_importance(self.model, max_num_features=20, ax=ax, importance_type='gain')\n",
    "        ax.set_title('Feature Importance - Gain (Miglioramento medio)')\n",
    "        \n",
    "        # 3. Cover importance\n",
    "        ax = axes[2]\n",
    "        plot_importance(self.model, max_num_features=20, ax=ax, importance_type='cover')\n",
    "        ax.set_title('Feature Importance - Cover (Copertura media)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'feature_importance_types.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Grafico combinato delle top features\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': self.features_df.columns,\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "        \n",
    "        sns.barplot(data=feature_importance, y='feature', x='importance', ax=ax)\n",
    "        ax.set_title('Top 15 Features pi√π Importanti')\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'top_features_importance.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_roc_curve(self, save_dir=None):\n",
    "        \"\"\"Visualizza la curva ROC\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, self.y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot della curva ROC\n",
    "        ax.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "        \n",
    "        # Trova il punto ottimale\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        ax.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, \n",
    "                  label=f'Optimal point (threshold={optimal_threshold:.3f})')\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'roc_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_pr_curve(self, save_dir=None):\n",
    "        \"\"\"Visualizza la curva Precision-Recall\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        precision, recall, thresholds = precision_recall_curve(self.y_test, self.y_pred_proba)\n",
    "        average_precision = average_precision_score(self.y_test, self.y_pred_proba)\n",
    "        \n",
    "        # Plot della curva PR\n",
    "        ax.plot(recall, precision, color='blue', lw=2,\n",
    "                label=f'PR curve (AP = {average_precision:.3f})')\n",
    "        \n",
    "        # Baseline (proporzione della classe positiva)\n",
    "        baseline = np.sum(self.y_test) / len(self.y_test)\n",
    "        ax.axhline(y=baseline, color='red', linestyle='--', \n",
    "                  label=f'Baseline (Random) = {baseline:.3f}')\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_title('Precision-Recall Curve')\n",
    "        ax.legend(loc=\"lower left\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'precision_recall_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_confusion_matrix(self, save_dir=None):\n",
    "        \"\"\"Visualizza la matrice di confusione\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "        \n",
    "        # 1. Matrice di confusione con valori assoluti\n",
    "        ax = axes[0]\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_title('Confusion Matrix - Valori Assoluti')\n",
    "        \n",
    "        # 2. Matrice di confusione normalizzata\n",
    "        ax = axes[1]\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=ax)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_title('Confusion Matrix - Normalizzata per Riga')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_probability_distribution(self, save_dir=None):\n",
    "        \"\"\"Visualizza la distribuzione delle probabilit√† predette\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 1. Istogramma delle probabilit√† per classe\n",
    "        ax = axes[0]\n",
    "        ax.hist(self.y_pred_proba[self.y_test == 0], bins=50, alpha=0.5, \n",
    "                label='Classe 0', color='blue', density=True)\n",
    "        ax.hist(self.y_pred_proba[self.y_test == 1], bins=50, alpha=0.5, \n",
    "                label='Classe 1', color='red', density=True)\n",
    "        ax.axvline(x=0.5, color='black', linestyle='--', label='Threshold=0.5')\n",
    "        ax.set_xlabel('Probabilit√† Predetta')\n",
    "        ax.set_ylabel('Densit√†')\n",
    "        ax.set_title('Distribuzione delle Probabilit√† per Classe')\n",
    "        ax.legend()\n",
    "        \n",
    "        # 2. Box plot delle probabilit√†\n",
    "        ax = axes[1]\n",
    "        data_to_plot = [self.y_pred_proba[self.y_test == 0], \n",
    "                       self.y_pred_proba[self.y_test == 1]]\n",
    "        ax.boxplot(data_to_plot, labels=['Classe 0', 'Classe 1'])\n",
    "        ax.set_ylabel('Probabilit√† Predetta')\n",
    "        ax.set_title('Box Plot delle Probabilit√† per Classe')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'probability_distribution.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_learning_curves(self, save_dir=None):\n",
    "        \"\"\"Visualizza le learning curves\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Estrai i risultati della valutazione durante il training\n",
    "        results = self.model.evals_result()\n",
    "        \n",
    "        if results:\n",
    "            epochs = len(results['validation_0']['logloss'])\n",
    "            x_axis = range(0, epochs)\n",
    "            \n",
    "            # Plot training & validation loss\n",
    "            ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "            ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "            ax.set_xlabel('Epoca')\n",
    "            ax.set_ylabel('Log Loss')\n",
    "            ax.set_title('Learning Curves - Log Loss durante il Training')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'learning_curves.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_calibration_curve(self, save_dir=None):\n",
    "        \"\"\"Visualizza la curva di calibrazione\"\"\"\n",
    "        from sklearn.calibration import calibration_curve\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Calcola la curva di calibrazione\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            self.y_test, self.y_pred_proba, n_bins=10\n",
    "        )\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(mean_predicted_value, fraction_of_positives, 's-', label='XGBoost')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "        \n",
    "        ax.set_xlabel('Mean Predicted Probability')\n",
    "        ax.set_ylabel('Fraction of Positives')\n",
    "        ax.set_title('Calibration Plot (Reliability Diagram)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'calibration_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_classification_report(self, save_dir=None):\n",
    "        \"\"\"Visualizza il classification report come heatmap\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # Genera il classification report\n",
    "        report = classification_report(self.y_test, self.y_pred, output_dict=True)\n",
    "        \n",
    "        # Converti in DataFrame per la visualizzazione\n",
    "        df_report = pd.DataFrame(report).transpose()\n",
    "        df_report = df_report.iloc[:-1, :-1]  # Rimuovi 'accuracy' e 'support'\n",
    "        \n",
    "        # Heatmap\n",
    "        sns.heatmap(df_report, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax)\n",
    "        ax.set_title('Classification Report Heatmap')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'classification_report.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def run_complete_pipeline(self, model_name=None):\n",
    "        \"\"\"Esegue l'intera pipeline con MLflow tracking\"\"\"\n",
    "        print(\"=== AVVIO PIPELINE COMPLETA ===\\n\")\n",
    "        \n",
    "        # Avvia MLflow run\n",
    "        with mlflow.start_run() as run:\n",
    "            self.run_id = run.info.run_id\n",
    "            print(f\"üöÄ MLflow Run ID: {self.run_id}\")\n",
    "            \n",
    "            try:\n",
    "                # 1. Preparazione dati\n",
    "                self.prepare_data()\n",
    "                \n",
    "                # 2. Ricerca iperparametri\n",
    "                self.hyperparameter_tuning()\n",
    "                \n",
    "                # 3. Training modello finale\n",
    "                self.train_final_model()\n",
    "                \n",
    "                # 4. Salvataggio modello e scaler\n",
    "                model_path, scaler_path, metadata_path = self.save_model_and_scaler(model_name)\n",
    "                \n",
    "                # 5. Visualizzazioni\n",
    "                self.plot_all_visualizations()\n",
    "                \n",
    "                # 6. Tag del run\n",
    "                mlflow.set_tag(\"pipeline_status\", \"completed\")\n",
    "                mlflow.set_tag(\"model_type\", \"xgboost_binary_classifier\")\n",
    "                \n",
    "                print(\"\\n=== PIPELINE COMPLETATA ===\")\n",
    "                print(f\"üéØ MLflow Run: {self.run_id}\")\n",
    "                print(f\"üíæ Modello salvato: {model_path}\")\n",
    "                print(f\"‚öôÔ∏è  Scaler salvato: {scaler_path}\")\n",
    "                \n",
    "                return self.model, self.best_params, self.run_id\n",
    "                \n",
    "            except Exception as e:\n",
    "                mlflow.set_tag(\"pipeline_status\", \"failed\")\n",
    "                mlflow.log_param(\"error_message\", str(e))\n",
    "                print(f\"‚ùå Errore nella pipeline: {e}\")\n",
    "                raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d0b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLflow experiment 'xgboost_experiment_130625_1611' configurato correttamente\n",
      "=== AVVIO PIPELINE COMPLETA ===\n",
      "\n",
      "üöÄ MLflow Run ID: 7f04706acb994e71873eb428103e02cc\n",
      "=== PREPARAZIONE DATI ===\n",
      "Dimensioni dataset:\n",
      "Train set: 32000 campioni\n",
      "Test set: 8000 campioni\n",
      "Numero di features: 134\n",
      "\n",
      "Distribuzione classi nel training set:\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== HYPERPARAMETER TUNING ===\n",
      "Using 12 cores for hyperparameter tuning (75% of 16 total cores).\n",
      "Avvio ricerca iperparametri...\n",
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n"
     ]
    }
   ],
   "source": [
    "date = datetime.now().strftime(\"%d%m%y_%H%M\")\n",
    "\n",
    "# Inizializza la pipeline\n",
    "pipeline = XGBoostPipeline(\n",
    "    features_df=features_df, \n",
    "    labels_df=labels_df,\n",
    "    experiment_name=f\"xgboost_experiment_{date}\",\n",
    "    model_save_dir=\"./models\"\n",
    ")\n",
    "\n",
    "# Esegui la pipeline completa\n",
    "model, best_params, run_id = pipeline.run_complete_pipeline(\n",
    "    model_name=\"xgboost_dev_model_v1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84507f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================================\n",
    "# SCENARIO 2: Finalizzazione del modello\n",
    "# ========================================\n",
    "\n",
    "# Carica dati completi per finalizzazione\n",
    "# features_df_final = pd.read_csv(\"features_full_dataset.csv\")\n",
    "# labels_df_final = pd.read_csv(\"labels_full_dataset.csv\")\n",
    "\n",
    "# Finalizza il modello sui dati completi\n",
    "final_model, final_scaler, final_run_id = pipeline.finalize_model(\n",
    "     features_df_to_finalize=features_df_to_finalize ,\n",
    "     labels_df_to_finalize=labels_df_to_finalize,\n",
    "     model_path=\"./my_models/xgboost_finalized_model_v1.joblib\",\n",
    "     scaler_path=\"./my_models/xgboost_finalized_model_v1_scaler.joblib\",\n",
    "     refit_scaler=True,  # Riallena scaler sui dati finali\n",
    "          final_model_name=\"xgboost_production_model_v1\"\n",
    " )\n",
    "\n",
    "# ========================================\n",
    "# SCENARIO 3: Predizioni con modello salvato\n",
    "# ========================================\n",
    "\n",
    "# Per predire su nuovi dati\n",
    "# new_predictions, new_probabilities = pipeline.predict_new_data(new_data_df)\n",
    "\n",
    "# Per caricare un modello salvato\n",
    "# model, scaler = XGBoostPipeline.load_model_and_scaler(\n",
    "#     \"./my_models/production_model_v1.joblib\", \n",
    "#     \"./my_models/production_model_v1_scaler.joblib\"\n",
    "# )\n",
    "\n",
    "# ========================================\n",
    "# SCENARIO 4: Workflow completo dev->prod\n",
    "# ========================================\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0588645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1282f9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
