{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f7c3c6",
   "metadata": {},
   "source": [
    "### Routine to train and fine-tune Catboost Binary Classifier\n",
    "\n",
    "##### TODOs:\n",
    "- Set MlFlow tracking URI\n",
    "- Start mlflow server: mlflow server --host 127.0.0.1 --port 8080 (LOCAL)\n",
    "- Change folders if needee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fccb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import (train_test_split, RandomizedSearchCV, \n",
    "                                   StratifiedKFold, cross_val_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_curve, auc, precision_recall_curve,\n",
    "                           average_precision_score, confusion_matrix, \n",
    "                           classification_report)\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# MLflow imports\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "import multiprocessing\n",
    "\n",
    "from typing import Tuple, Optional, Union, List, Dict\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MlFlow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\")  # Check your MLflow server URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SampleConfig:\n",
    "    \"\"\"Sample configuration.\"\"\"\n",
    "    name: str\n",
    "    n_samples_per_class: int\n",
    "    random_state: int = 42\n",
    "\n",
    "@dataclass\n",
    "class DatasetInfo:\n",
    "    n_samples: int\n",
    "    n_features: int\n",
    "    class_distribution: Dict[int, int]\n",
    "    features_columns: List[str]\n",
    "    labels_columns: List[str]\n",
    "\n",
    "@dataclass\n",
    "class DataLoader:\n",
    "    features_path: Union[str, Path]\n",
    "    labels_path: Union[str, Path]\n",
    "    _features_df_cache: Optional[pd.DataFrame] = field(default=None, init=False, repr=False)\n",
    "    _labels_df_cache: Optional[pd.DataFrame] = field(default=None, init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.features_path = Path(self.features_path)\n",
    "        self.labels_path = Path(self.labels_path)\n",
    "\n",
    "        if not self.features_path.exists():\n",
    "            raise FileNotFoundError(f\"Features file not found: {self.features_path}\")\n",
    "        if not self.labels_path.exists():\n",
    "            raise FileNotFoundError(f\"Labels file not found: {self.labels_path}\")\n",
    "\n",
    "    def load_full_data(self, use_cache: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \n",
    "        if use_cache and self._features_df_cache is not None and self._labels_df_cache is not None:\n",
    "            return self._features_df_cache, self._labels_df_cache\n",
    "\n",
    "        features_df = pd.read_parquet(self.features_path)\n",
    "        labels_df = pd.read_parquet(self.labels_path)\n",
    "\n",
    "        # Dimensions check\n",
    "        if len(features_df) != len(labels_df):\n",
    "            raise ValueError(f\"Features and labels have different lengths: \"\n",
    "                           f\"{len(features_df)} vs {len(labels_df)}\")\n",
    "\n",
    "        if use_cache:\n",
    "            self._features_df_cache = features_df\n",
    "            self._labels_df_cache = labels_df\n",
    "\n",
    "        return features_df, labels_df\n",
    "    \n",
    "    def clear_cache(self) -> None:\n",
    "        self._features_df_cache = None\n",
    "        self._labels_df_cache = None\n",
    "\n",
    "    def load_balanced_sample(self, \n",
    "                            n_samples_per_class: int, \n",
    "                            random_state: int = 42,\n",
    "                            use_cache: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \n",
    "        # Load full data (with caching option)\n",
    "        features_df_full, labels_df_full = self.load_full_data(use_cache=use_cache)\n",
    "\n",
    "        # Get sampled indices balanced across classes\n",
    "        sampled_indices = self._get_balanced_indices(\n",
    "            labels_df_full = labels_df_full,\n",
    "            n_samples_per_class = n_samples_per_class,\n",
    "            random_state =random_state\n",
    "        )\n",
    "        features_df = features_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = labels_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "\n",
    "        return features_df, labels_df\n",
    "\n",
    "    def load_balanced_sample_memory_efficient(self, n_samples_per_class: int, \n",
    "                                            random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \n",
    "        features_df_full = pd.read_parquet(self.features_path)\n",
    "        labels_df_full = pd.read_parquet(self.labels_path)\n",
    "\n",
    "        # Get sampled indices balanced across classes\n",
    "        sampled_indices = self._get_balanced_indices(\n",
    "            labels_df = labels_df_full, \n",
    "            n_samples_per_class = n_samples_per_class, \n",
    "            random_state = random_state\n",
    "        )\n",
    "\n",
    "        features_df = features_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = labels_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "\n",
    "        # Clean up memory\n",
    "        del features_df_full, labels_df_full, sampled_indices\n",
    "\n",
    "        return features_df, labels_df\n",
    "    \n",
    "    def _get_balanced_indices(self, labels_df: pd.DataFrame, \n",
    "                            n_samples_per_class: int, \n",
    "                            random_state: int = 42) -> pd.Index:\n",
    "        \"\"\"\n",
    "        Get indices of a balanced sample from the labels DataFrame.\n",
    "        \"\"\"\n",
    "        label_column = labels_df.iloc[:, 0]\n",
    "\n",
    "        # Check if all classes have enough samples\n",
    "        class_counts = label_column.value_counts()\n",
    "        for class_label, count in class_counts.items():\n",
    "            if count < n_samples_per_class:\n",
    "                raise ValueError(f\"Class {class_label} has only {count} samples, \"\n",
    "                               f\"but {n_samples_per_class} requested\")\n",
    "\n",
    "        sampled_indices = (\n",
    "            labels_df.groupby(label_column)\n",
    "            .apply(lambda x: x.sample(n=n_samples_per_class, random_state=random_state))\n",
    "            .index.get_level_values(1)\n",
    "        )\n",
    "        return sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb013fe9",
   "metadata": {},
   "source": [
    "### Training\n",
    "#### Load a smaller balanced dataset for hyperparameter tuning and a larger one for final retraining with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    features_path='processed_data/binary_classification/data_w_features/combined_features.parquet',\n",
    "    labels_path='processed_data/binary_classification/data_w_features/labels_binary.parquet'\n",
    ")\n",
    "\n",
    "# Create a smaller balanced sample for hyperparameter tuning\n",
    "features_df, labels_df = loader.load_balanced_sample_memory_efficient(n_samples_per_class=25000)\n",
    "print(f\"Training set shape: features={features_df.shape}, labels={labels_df.shape}\")\n",
    "\n",
    "# Create dataset to fininalize training\n",
    "features_df_to_finalize, labels_df_to_finalize = loader.load_balanced_sample_memory_efficient(\n",
    "    n_samples_per_class=250000\n",
    ")\n",
    "print(f\"Final set shape: features={features_df_to_finalize.shape}, labels={labels_df_to_finalize.shape}\")\n",
    "print(f\"Class distribution in dev set set: {labels_df.iloc[:, 0].value_counts()}\")\n",
    "print(f\"Class distribution in final set: {labels_df_to_finalize.iloc[:, 0].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df478220",
   "metadata": {},
   "source": [
    "### Ricerca iperparametri e training del modello\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostPipeline:\n",
    "    def __init__(self, features_df, labels_df, test_size=0.2, random_state=42,\n",
    "                 experiment_name=\"catboost_binary_classification\", model_save_dir=\"./models\", save_voter=True):\n",
    "        \n",
    "        self.features_df = features_df\n",
    "        self.labels_df = labels_df.values.ravel() if isinstance(labels_df, pd.DataFrame) else labels_df\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.cv_results = None\n",
    "        self.scaler = None\n",
    "        self.save_voter = save_voter\n",
    "        \n",
    "        # MLflow setup\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.run_id = None\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "        \n",
    "        # Setup MLflow experiment\n",
    "        self._setup_mlflow_experiment()\n",
    " \n",
    "    def _setup_mlflow_experiment(self):\n",
    "        \n",
    "        try:\n",
    "            experiment = mlflow.get_experiment_by_name(self.experiment_name)\n",
    "            if experiment is None:\n",
    "                mlflow.create_experiment(self.experiment_name)\n",
    "            \n",
    "            mlflow.set_experiment(self.experiment_name)\n",
    "            print(f\"MLflow experiment '{self.experiment_name}' correctly set up.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mlflow configuration: {e}\")\n",
    "    \n",
    "    def _log_to_mlflow(self, key, value):\n",
    "        \"\"\"Helper per logging sicuro su MLflow\"\"\"\n",
    "        try:\n",
    "            if isinstance(value, (int, float)):\n",
    "                mlflow.log_metric(key, value)\n",
    "            else:\n",
    "                mlflow.log_param(key, value)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel logging MLflow per {key}: {e}\")\n",
    "\n",
    "           \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Split data into train and test sets\"\"\"\n",
    "        print(\"=== Data preparation ===\")\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.features_df, self.labels_df, \n",
    "            test_size=self.test_size, \n",
    "            random_state=self.random_state,\n",
    "            stratify=self.labels_df\n",
    "        )\n",
    "        \n",
    "        # Features scaling\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        # Log info dataset su MLflow\n",
    "        try:\n",
    "            mlflow.log_param(\"dataset_total_samples\", len(self.features_df))\n",
    "            mlflow.log_param(\"train_samples\", self.X_train.shape[0])\n",
    "            mlflow.log_param(\"test_samples\", self.X_test.shape[0])\n",
    "            mlflow.log_param(\"n_features\", self.X_train.shape[1])\n",
    "            mlflow.log_param(\"test_size\", self.test_size)\n",
    "            mlflow.log_param(\"random_state\", self.random_state)\n",
    "            \n",
    "            class_distribution = pd.Series(self.y_train).value_counts(normalize=True)\n",
    "            for class_label, proportion in class_distribution.items():\n",
    "                mlflow.log_metric(f\"class_{class_label}_proportion\", proportion)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error mlflow: {e}\")\n",
    "        \n",
    "        print(\"Dataset dimensions:\")\n",
    "        print(f\"Train set: {self.X_train.shape[0]} campioni\")\n",
    "        print(f\"Test set: {self.X_test.shape[0]} campioni\")\n",
    "        print(f\"Features: {self.X_train.shape[1]}\")\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(pd.Series(self.y_train).value_counts(normalize=True))\n",
    "        \n",
    "    def hyperparameter_tuning(self, cv_folds=5, verbose=True, n_iter=250):\n",
    "        print(\"\\n=== HYPERPARAMETER TUNING ===\")\n",
    "        \n",
    "        self.param_distributions = {\n",
    "            'iterations': randint(500, 2000),\n",
    "            'depth': randint(4, 10),\n",
    "            'learning_rate': loguniform(0.01, 0.3),\n",
    "            'l2_leaf_reg': loguniform(1, 50),\n",
    "            'bagging_temperature': uniform(0.0, 1.0),\n",
    "            'random_strength': uniform(1, 5.0),\n",
    "            'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS'],\n",
    "            'one_hot_max_size': randint(2, 255),\n",
    "            'max_ctr_complexity': randint(1, 4),\n",
    "            'min_data_in_leaf': randint(1, 50),\n",
    "            'leaf_estimation_iterations': randint(1, 20),\n",
    "            'subsample': uniform(0.5, 1.0),  \n",
    "            'rsm': uniform(0.5, 1.0),  \n",
    "            'border_count': randint(128, 255),\n",
    "            'feature_border_type': ['Median', 'Uniform', 'UniformAndQuantiles', 'MaxLogSum', 'MinEntropy', 'GreedyLogSum'],\n",
    "            'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            mlflow.log_param(\"cv_folds\", cv_folds)\n",
    "            mlflow.log_param(\"search_iterations\", n_iter)\n",
    "            mlflow.log_param(\"search_scoring\", \"roc_auc\")\n",
    "            \n",
    "            # Log hyperparameter search space\n",
    "            for param, distribution in self.param_distributions.items():\n",
    "                if hasattr(distribution, 'args'):\n",
    "                    mlflow.log_param(f\"search_space_{param}\", str(distribution.args))\n",
    "                else:\n",
    "                    mlflow.log_param(f\"search_space_{param}\", str(distribution))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"{e}\")\n",
    "        \n",
    "        # Modello base\n",
    "        base_model = CatBoostClassifier(\n",
    "            objective='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            random_seed=self.random_state,\n",
    "            verbose=False,\n",
    "            thread_count=-1,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "    \n",
    "        # Stratified K-Fold\n",
    "        stratified_kfold = StratifiedKFold(\n",
    "            n_splits=cv_folds, \n",
    "            shuffle=True, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # RandomizedSearchCV\n",
    "        n_cores = multiprocessing.cpu_count()\n",
    "        n_jobs = np.floor(0.75 * n_cores).astype(int)\n",
    "        print(f\"Using {n_jobs} cores for hyperparameter tuning (75% of {n_cores} total cores).\")\n",
    "        \n",
    "        self.grid_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=self.param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=stratified_kfold,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=2,\n",
    "            random_state=self.random_state,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        print(\"Searching for hyperparameters!\")\n",
    "        self.grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        self.best_params = self.grid_search.best_params_\n",
    "        self.cv_results = pd.DataFrame(self.grid_search.cv_results_)\n",
    "        \n",
    "        try:\n",
    "            mlflow.log_metric(\"best_cv_score\", self.grid_search.best_score_)\n",
    "            \n",
    "            for param, value in self.best_params.items():\n",
    "                mlflow.log_param(f\"best_{param}\", value)\n",
    "                \n",
    "            cv_results_path = os.path.join(self.model_save_dir, \"cv_results.csv\")\n",
    "            self.cv_results.to_csv(cv_results_path, index=False)\n",
    "            mlflow.log_artifact(cv_results_path, \"hyperparameter_search\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mlflow logging: {e}\")\n",
    "        \n",
    "        print(\"\\nBest hyperparameters found:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nBest  ROC-AUC (CV): {self.grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        # Visualize results\n",
    "        self._plot_hyperparameter_results(save_dir=self.model_save_dir)\n",
    "        \n",
    "    def train_final_model(self):\n",
    "        \"\"\"Finalize the model training with the best hyperparameters found\"\"\"\n",
    "        print(\"\\n=== Finalizing model ===\")\n",
    "        \n",
    "        self.model = CatBoostClassifier(\n",
    "            **self.best_params,\n",
    "            objective='Logloss',\n",
    "            eval_metric='AUC',\n",
    "            random_seed=self.random_state,\n",
    "            verbose=False,\n",
    "            thread_count=-1,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        \n",
    "        # Create Pool for CatBoost \n",
    "        train_pool = Pool(self.X_train_scaled, self.y_train)\n",
    "        test_pool = Pool(self.X_test_scaled, self.y_test)\n",
    "        \n",
    "        self.model.fit(\n",
    "            train_pool,\n",
    "            eval_set=test_pool,\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True\n",
    "        )\n",
    "        self.y_pred = self.model.predict(self.X_test_scaled)\n",
    "        self.y_pred_proba = self.model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, self.y_pred),\n",
    "            'precision': precision_score(self.y_test, self.y_pred),\n",
    "            'recall': recall_score(self.y_test, self.y_pred),\n",
    "            'f1_score': f1_score(self.y_test, self.y_pred),\n",
    "            'roc_auc': auc(*roc_curve(self.y_test, self.y_pred_proba)[:2]),\n",
    "            'average_precision': average_precision_score(self.y_test, self.y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # Log metrics MLflow\n",
    "        try:\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"test_{metric_name}\", metric_value)\n",
    "                \n",
    "            \n",
    "            mlflow.log_metric(\"iterations_used\", self.model.tree_count_)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error logging metrics: {e}\")\n",
    "        \n",
    "        print(\"\\nMetrics on test set:\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            print(f\"{metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "\n",
    "    def save_model_and_scaler(self, model_name=None):\n",
    "        print(\"\\n=== Saving finalized model and scaler ===\")\n",
    "        \n",
    "        if model_name is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_name = f\"catboost_model_{timestamp}\"\n",
    "        \n",
    "        model_path = os.path.join(self.model_save_dir, f\"{model_name}.cbm\")  # CatBoost format\n",
    "        model_joblib_path = os.path.join(self.model_save_dir, f\"{model_name}.joblib\")  # Fallback\n",
    "        scaler_path = os.path.join(self.model_save_dir, f\"{model_name}_scaler.joblib\")\n",
    "        metadata_path = os.path.join(self.model_save_dir, f\"{model_name}_metadata.json\")\n",
    "        \n",
    "        try:\n",
    "            # Save model in native CatBoost format and joblib\n",
    "            self.model.save_model(model_path)\n",
    "            print(f\"Model saved with native format in: {model_path}\")\n",
    "            joblib.dump(self.model, model_joblib_path)\n",
    "            print(f\"Model saved (Joblib) in: {model_joblib_path}\")\n",
    "            \n",
    "            # Save scaler\n",
    "            joblib.dump(self.scaler, scaler_path)\n",
    "            print(f\"Scaler saved in: {scaler_path}\")\n",
    "            joblib.dump(self.scaler, 'models/voters/catboost_scaler.joblib')\n",
    "\n",
    "            if self.save_voter:\n",
    "                print(\"Saving voter...\")\n",
    "                self.model.save_model('models/voters/catboost_voter.cbm')\n",
    "                joblib.dump(self.model, 'models/voters/catboost_voter.joblib')\n",
    "                joblib.dump(self.scaler, 'models/voters/catboost_scaler.joblib')\n",
    "\n",
    "            \n",
    "            # Save metadata\n",
    "            metadata = {\n",
    "                'model_name': model_name,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'best_params': self.best_params,\n",
    "                'cv_score': float(self.grid_search.best_score_),\n",
    "                'test_metrics': {\n",
    "                'accuracy': float(accuracy_score(self.y_test, self.y_pred)),\n",
    "                'precision': float(precision_score(self.y_test, self.y_pred)),\n",
    "                'recall': float(recall_score(self.y_test, self.y_pred)),\n",
    "                'f1_score': float(f1_score(self.y_test, self.y_pred)),\n",
    "                'roc_auc': float(auc(*roc_curve(self.y_test, self.y_pred_proba)[:2]))\n",
    "                },\n",
    "                'feature_names': list(self.features_df.columns),\n",
    "                'n_features': len(self.features_df.columns),\n",
    "                'train_samples': len(self.y_train),\n",
    "                'test_samples': len(self.y_test),\n",
    "                'tree_count': int(self.model.tree_count_)\n",
    "            }\n",
    "            \n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            print(f\"Metadata saved: {metadata_path}\")\n",
    "            \n",
    "            try:\n",
    "                signature = infer_signature(self.X_train_scaled, self.y_pred_proba)\n",
    "                mlflow.catboost.log_model(\n",
    "                    self.model, \n",
    "                    \"catboost_model\",\n",
    "                    signature=signature\n",
    "                )\n",
    "                \n",
    "                mlflow.log_artifact(scaler_path, \"preprocessing\")\n",
    "                mlflow.log_artifact(metadata_path, \"model_info\")\n",
    "                \n",
    "                mlflow.log_param(\"model_save_path\", model_path)\n",
    "                mlflow.log_param(\"scaler_save_path\", scaler_path)\n",
    "                mlflow.log_param(\"model_name\", model_name)\n",
    "                \n",
    "                print(\"Model and scaler logged to MLflow successfully.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error logging scaler or model on MLflow: {e}\")\n",
    "            \n",
    "            return model_path, scaler_path, metadata_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving: {e}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model_and_scaler(model_path, scaler_path):\n",
    "        try:\n",
    "            #Load model using CatBoost native format or fallback to joblib\n",
    "            if model_path.endswith('.cbm'):\n",
    "                model = CatBoostClassifier()\n",
    "                model.load_model(model_path)\n",
    "            else:\n",
    "                model = joblib.load(model_path)\n",
    "                \n",
    "            scaler = joblib.load(scaler_path)\n",
    "            print(f\"Model loaded from: {model_path}\")\n",
    "            print(f\"Scaler loaded from: {scaler_path}\")\n",
    "            return model, scaler\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading scaler or model: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def predict_new_data(self, new_data, model_path=None, scaler_path=None):\n",
    "        if model_path and scaler_path:\n",
    "            model, scaler = self.load_model_and_scaler(model_path, scaler_path)\n",
    "        else:\n",
    "            model, scaler = self.model, self.scaler\n",
    "            \n",
    "        if model is None or scaler is None:\n",
    "            print(\"Model or scaler not loaded. Cannot predict.\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "            predictions = model.predict(new_data_scaled)\n",
    "            probabilities = model.predict_proba(new_data_scaled)[:, 1]\n",
    "            \n",
    "            return predictions, probabilities\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def finalize_model(self, features_df_to_finalize, labels_df_to_finalize, \n",
    "                      model_path, scaler_path, \n",
    "                      refit_scaler=bool,\n",
    "                      use_early_stopping=bool,\n",
    "                      final_model_name=None, experiment_suffix=\"_finalized\"):\n",
    "        \"\"\"\n",
    "        Loads a preexisting model and retrains it on the final data.\n",
    "        \"\"\"\n",
    "        print(\"=== Finalizing model ===\")\n",
    "        \n",
    "        labels_to_finalize = labels_df_to_finalize.values.ravel() if isinstance(labels_df_to_finalize, pd.DataFrame) else labels_df_to_finalize\n",
    "        \n",
    "\n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        print(f\"Loading scaler from: {scaler_path}\")\n",
    "        base_model, base_scaler = self.load_model_and_scaler(model_path, scaler_path)\n",
    "        if base_model is None or base_scaler is None:\n",
    "            print(\"Error in loading model or scaler\")\n",
    "            return None, None, None\n",
    "        \n",
    "        original_experiment_name = self.experiment_name\n",
    "        finalization_experiment_name = original_experiment_name + experiment_suffix\n",
    "        \n",
    "        try:\n",
    "            experiment = mlflow.get_experiment_by_name(finalization_experiment_name)\n",
    "            if experiment is None:\n",
    "                mlflow.create_experiment(finalization_experiment_name)\n",
    "            mlflow.set_experiment(finalization_experiment_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error setup MLflow: {e}\")\n",
    "         \n",
    "        with mlflow.start_run() as run:\n",
    "            finalization_run_id = run.info.run_id\n",
    "            print(f\"MLflow Finalization Run ID: {finalization_run_id}\")\n",
    "            \n",
    "            try:\n",
    "                mlflow.log_param(\"base_model_path\", model_path)\n",
    "                mlflow.log_param(\"base_scaler_path\", scaler_path)\n",
    "                mlflow.log_param(\"finalization_samples\", len(features_df_to_finalize))\n",
    "                mlflow.log_param(\"finalization_features\", len(features_df_to_finalize.columns))\n",
    "                mlflow.log_param(\"use_early_stopping\", use_early_stopping)\n",
    "                mlflow.log_param(\"refit_scaler\", refit_scaler)\n",
    "                \n",
    "                class_distribution = pd.Series(labels_to_finalize).value_counts(normalize=True)\n",
    "                for class_label, proportion in class_distribution.items():\n",
    "                    mlflow.log_metric(f\"final_class_{class_label}_proportion\", proportion)\n",
    "                \n",
    "                print(f\"Final data: {len(features_df_to_finalize)} samples, {len(features_df_to_finalize.columns)} features\")\n",
    "                print(\"Class distribution in final data:\")\n",
    "                print(pd.Series(labels_to_finalize).value_counts(normalize=True))\n",
    "\n",
    "                # Scaler refitting logic\n",
    "                if refit_scaler:\n",
    "                    print(\"Re-fitting scaler on new data...\")\n",
    "                    final_scaler = StandardScaler()\n",
    "                    features_scaled = final_scaler.fit_transform(features_df_to_finalize)\n",
    "                    mlflow.log_param(\"scaler_action\", \"refit_on_new_data\")\n",
    "                else:\n",
    "                    print(\"Using existing scaler...\")\n",
    "                    final_scaler = base_scaler\n",
    "                    features_scaled = final_scaler.transform(features_df_to_finalize)\n",
    "                    mlflow.log_param(\"scaler_action\", \"use_existing\")\n",
    "                \n",
    "                base_params = base_model.get_params()\n",
    "                print(f\"Base model parameters: {len(base_params)}\")\n",
    "                \n",
    "                # Log parameters\n",
    "                for param, value in base_params.items():\n",
    "                    if param not in ['random_seed', 'thread_count', 'objective', 'eval_metric', 'verbose', 'allow_writing_files']:\n",
    "                        mlflow.log_param(f\"base_{param}\", value)\n",
    "                \n",
    "                # Instantiate the final model with base parameters\n",
    "                final_model = CatBoostClassifier(**base_params)\n",
    "                \n",
    "                # Training finale\n",
    "                print(\"Starting final training...\")\n",
    "                \n",
    "                if use_early_stopping and len(features_df_to_finalize) > 100:\n",
    "                    val_size= 0.15\n",
    "                    X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "                        features_scaled, labels_to_finalize,\n",
    "                        test_size=val_size,\n",
    "                        random_state=self.random_state,\n",
    "                        stratify=labels_to_finalize\n",
    "                    )\n",
    "                    \n",
    "                    # Create Pool for CatBoost\n",
    "                    train_pool = Pool(X_train_final, y_train_final)\n",
    "                    val_pool = Pool(X_val_final, y_val_final)\n",
    "                    \n",
    "                    final_model.fit(\n",
    "                        train_pool,\n",
    "                        eval_set=val_pool,\n",
    "                        early_stopping_rounds=200,\n",
    "                        use_best_model=True\n",
    "                    )\n",
    "                    \n",
    "                    mlflow.log_param(\"validation_split_used\", True)\n",
    "                    mlflow.log_param(\"validation_size\", val_size)\n",
    "\n",
    "                    y_val_pred = final_model.predict(X_val_final)\n",
    "                    y_val_pred_proba = final_model.predict_proba(X_val_final)[:, 1]\n",
    "                \n",
    "                    final_metrics = {\n",
    "                        'final_accuracy': accuracy_score(y_val_final, y_val_pred),\n",
    "                        'final_precision': precision_score(y_val_final, y_val_pred),\n",
    "                        'final_recall': recall_score(y_val_final, y_val_pred),\n",
    "                        'final_f1_score': f1_score(y_val_final, y_val_pred),\n",
    "                        'final_roc_auc': auc(*roc_curve(y_val_final, y_val_pred_proba)[:2])\n",
    "                    }\n",
    "                    \n",
    "                else:\n",
    "                    train_pool = Pool(features_scaled, labels_to_finalize)\n",
    "                    final_model.fit(train_pool)\n",
    "                    mlflow.log_param(\"validation_split_used\", False)\n",
    "                    \n",
    "                    y_train_pred = final_model.predict(features_scaled)\n",
    "                    y_train_pred_proba = final_model.predict_proba(features_scaled)[:, 1]\n",
    "                    \n",
    "                    final_metrics = {\n",
    "                        'final_train_accuracy': accuracy_score(labels_to_finalize, y_train_pred),\n",
    "                        'final_train_precision': precision_score(labels_to_finalize, y_train_pred),\n",
    "                        'final_train_recall': recall_score(labels_to_finalize, y_train_pred),\n",
    "                        'final_train_f1_score': f1_score(labels_to_finalize, y_train_pred),\n",
    "                        'final_train_roc_auc': auc(*roc_curve(labels_to_finalize, y_train_pred_proba)[:2])\n",
    "                    }\n",
    "                \n",
    "                for metric_name, metric_value in final_metrics.items():\n",
    "                    mlflow.log_metric(metric_name, metric_value)\n",
    "                \n",
    "                print(\"\\n Finalized model metrics:\")\n",
    "                for metric_name, metric_value in final_metrics.items():\n",
    "                    print(f\"  {metric_name.replace('_', ' ').title()}: {metric_value:.4f}\")\n",
    "                \n",
    "                print(\"\\n Feature importance study...\")\n",
    "                base_importance = pd.DataFrame({\n",
    "                    'feature': features_df_to_finalize.columns,\n",
    "                    'base_importance': base_model.get_feature_importance()\n",
    "                }).sort_values('base_importance', ascending=False)\n",
    "\n",
    "                final_importance = pd.DataFrame({\n",
    "                    'feature': features_df_to_finalize.columns,\n",
    "                    'final_importance': final_model.get_feature_importance()\n",
    "                }).sort_values('final_importance', ascending=False)\n",
    "\n",
    "                importance_comparison = base_importance.merge(\n",
    "                    final_importance, on='feature', how='inner'\n",
    "                )\n",
    "                importance_comparison['importance_change'] = (\n",
    "                    importance_comparison['final_importance'] - importance_comparison['base_importance']\n",
    "                )\n",
    "\n",
    "                top_changes = importance_comparison.reindex(\n",
    "                    importance_comparison['importance_change'].abs().sort_values(ascending=False).index\n",
    "                ).head(10)\n",
    "                \n",
    "                print(\"Top 10 feature with greater change in importance between partial and full training data:\")\n",
    "                \n",
    "                try:\n",
    "                    for _, row in top_changes.iterrows():\n",
    "                        change_pct = (row['importance_change'] / (row['base_importance'] + 1e-8)) * 100\n",
    "                        print(f\"  {row['feature']}: {row['importance_change']:+.4f} ({change_pct:+.1f}%)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during print of importances change {e}\")\n",
    "                \n",
    "                # Save final model, scaler, metadata and importance comparison\n",
    "                if final_model_name is None:\n",
    "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    final_model_name = f\"finalized_model_{timestamp}\"\n",
    "                    print(f\"Final model name: {final_model_name}\")\n",
    "                \n",
    "                final_model_path = os.path.join(self.model_save_dir, f\"{final_model_name}.cbm\")\n",
    "                print(f\"Final model will be saved as: {final_model_path}\")\n",
    "\n",
    "                final_model_joblib_path = os.path.join(self.model_save_dir, f\"{final_model_name}.joblib\")\n",
    "                print(f\"Final scaler will be saved as: {final_scaler_path}\")\n",
    "\n",
    "                final_scaler_path = os.path.join(self.model_save_dir, f\"{final_model_name}_scaler.joblib\")\n",
    "                print(f\"Final metadata will be saved as: {final_metadata_path}\")\n",
    "\n",
    "                final_metadata_path = os.path.join(self.model_save_dir, f\"{final_model_name}_metadata.json\")\n",
    "                \n",
    "                importance_comparison_path = os.path.join(self.model_save_dir, f\"{final_model_name}_importance_comparison.csv\")\n",
    "                print(f\"Importance comparison will be saved as: {importance_comparison_path}\")\n",
    "\n",
    "                final_model.save_model(final_model_path)\n",
    "                joblib.dump(final_model, final_model_joblib_path)\n",
    "                joblib.dump(final_scaler, final_scaler_path)\n",
    "                importance_comparison.to_csv(importance_comparison_path, index=False)\n",
    "                \n",
    "                final_metadata = {\n",
    "                    'model_name': final_model_name,\n",
    "                    'finalization_timestamp': datetime.now().isoformat(),\n",
    "                    'base_model_path': model_path,\n",
    "                    'base_scaler_path': scaler_path,\n",
    "                    'finalization_samples': len(features_df_to_finalize),\n",
    "                    'finalization_features': len(features_df_to_finalize.columns),\n",
    "                    'use_early_stopping': use_early_stopping,\n",
    "                    'final_metrics': {k: float(v) for k, v in final_metrics.items()},\n",
    "                    'model_params': {k: v for k, v in base_params.items() if k not in ['random_seed', 'thread_count']},\n",
    "                    'feature_names': list(features_df_to_finalize.columns),\n",
    "                    'class_distribution': {str(k): float(v) for k, v in class_distribution.items()},\n",
    "                    'tree_count': int(final_model.tree_count_)\n",
    "                }\n",
    "                \n",
    "                with open(final_metadata_path, 'w') as f:\n",
    "                    json.dump(final_metadata, f, indent=2)\n",
    "                \n",
    "                # MLflow logging\n",
    "                try:\n",
    "\n",
    "                    signature = infer_signature(features_scaled, final_model.predict_proba(features_scaled))\n",
    "                    mlflow.catboost.log_model(\n",
    "                        final_model, \n",
    "                        \"finalized_catboost_model\",\n",
    "                        signature=signature\n",
    "                    )\n",
    "                    \n",
    "                    # Log artifacts\n",
    "                    mlflow.log_artifact(final_scaler_path, \"preprocessing\")\n",
    "                    mlflow.log_artifact(final_metadata_path, \"model_info\")\n",
    "                    mlflow.log_artifact(importance_comparison_path, \"analysis\")\n",
    "                    \n",
    "                    mlflow.log_param(\"final_model_path\", final_model_path)\n",
    "                    mlflow.log_param(\"final_scaler_path\", final_scaler_path)\n",
    "                    mlflow.log_param(\"final_model_name\", final_model_name)\n",
    "                    \n",
    "                    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                    top_15_comparison = importance_comparison.head(15)\n",
    "                    \n",
    "                    x = np.arange(len(top_15_comparison))\n",
    "                    width = 0.35\n",
    "                    \n",
    "                    ax.bar(x - width/2, top_15_comparison['base_importance'], width, \n",
    "                          label='Modello Base', alpha=0.8)\n",
    "                    ax.bar(x + width/2, top_15_comparison['final_importance'], width, \n",
    "                          label='Modello Finalizzato', alpha=0.8)\n",
    "                    \n",
    "                    ax.set_xlabel('Features')\n",
    "                    ax.set_ylabel('Importance Score')\n",
    "                    ax.set_title('Confronto Feature Importance: Base vs Finalizzato')\n",
    "                    ax.set_xticks(x)\n",
    "                    ax.set_xticklabels(top_15_comparison['feature'], rotation=45, ha='right')\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    plots_dir = os.path.join(self.model_save_dir, \"finalization_plots\")\n",
    "                    os.makedirs(plots_dir, exist_ok=True)\n",
    "                    comparison_plot_path = os.path.join(plots_dir, f\"{final_model_name}_importance_comparison.png\")\n",
    "                    plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "                    mlflow.log_artifact(comparison_plot_path, \"plots\")\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print(\"Artifacts saved on MLflow\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Errore logging MLflow: {e}\")\n",
    "                \n",
    "                # Tag del run\n",
    "                mlflow.set_tag(\"pipeline_status\", \"finalized\")\n",
    "                mlflow.set_tag(\"model_type\", \"catboost_finalized\")\n",
    "                mlflow.set_tag(\"base_model_source\", model_path)\n",
    "                \n",
    "                print(\"\\n=== FINALIZZAZIONE COMPLETATA ===\")\n",
    "                print(f\" MLflow Run: {finalization_run_id}\")\n",
    "                print(f\" Finalized model: {final_model_path}\")\n",
    "                print(f\" Finalized scaler: {final_scaler_path}\")\n",
    "                print(f\" Importance analysis: {importance_comparison_path}\")\n",
    "                \n",
    "                self.model = final_model\n",
    "                self.scaler = final_scaler\n",
    "                \n",
    "                return final_model, final_scaler, finalization_run_id\n",
    "                \n",
    "            except Exception as e:\n",
    "                mlflow.set_tag(\"pipeline_status\", \"failed\")\n",
    "                mlflow.log_param(\"error_message\", str(e))\n",
    "                print(f\"Errore in finalization: {e}\")\n",
    "                raise\n",
    "                \n",
    "        try:\n",
    "            mlflow.set_experiment(original_experiment_name)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    def plot_all_visualizations(self):\n",
    "        print(\"\\n=== Data Viz ===\")\n",
    "        \n",
    "        try:\n",
    "            # Create directory for plots\n",
    "            plots_dir = os.path.join(self.model_save_dir, \"plots\")\n",
    "            os.makedirs(plots_dir, exist_ok=True)\n",
    "            \n",
    "            # 1. Feature Importance\n",
    "            self._plot_feature_importance(save_dir=plots_dir)\n",
    "            \n",
    "            # 2. ROC\n",
    "            self._plot_roc_curve(save_dir=plots_dir)\n",
    "            \n",
    "            # 3. Precision-Recall\n",
    "            self._plot_pr_curve(save_dir=plots_dir)\n",
    "            \n",
    "            # 4. Confusion Matrix\n",
    "            self._plot_confusion_matrix(save_dir=plots_dir)\n",
    "            \n",
    "            # 5. Predicted proba Distribution\n",
    "            self._plot_probability_distribution(save_dir=plots_dir)\n",
    "            \n",
    "            # 6. Learning Curves\n",
    "            self._plot_learning_curves(save_dir=plots_dir)\n",
    "            \n",
    "            # 7. Calibration Plot\n",
    "            self._plot_calibration_curve(save_dir=plots_dir)\n",
    "            \n",
    "            # 8. Classification Report Heatmap\n",
    "            self._plot_classification_report(save_dir=plots_dir)\n",
    "\n",
    "            try:\n",
    "                for plot_file in os.listdir(plots_dir):\n",
    "                    if plot_file.endswith('.png'):\n",
    "                        mlflow.log_artifact(os.path.join(plots_dir, plot_file), \"plots\")\n",
    "                print(\"Plots saved on MLflow\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving plots on MlFlow: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating plots {e}\")\n",
    "        \n",
    "    def _plot_hyperparameter_results(self, save_dir=None):\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Grid Search Convergence\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(range(len(self.cv_results)), self.cv_results['mean_test_score'])\n",
    "        ax.axhline(y=self.grid_search.best_score_, color='r', linestyle='--', label='Best Score')\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('ROC-AUC Score')\n",
    "        ax.set_title('Grid Search Convergence')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Relative importance of parameters\n",
    "        ax = axes[0, 1]\n",
    "        param_importance = {}\n",
    "        for param in self.best_params.keys():\n",
    "            col = f'param_{param}'\n",
    "            if col in self.cv_results.columns:\n",
    "                if self.cv_results[col].dtype == 'object':\n",
    "                    unique_vals = self.cv_results[col].nunique()\n",
    "                    param_importance[param] = unique_vals / len(self.cv_results)\n",
    "                else:\n",
    "                    correlation = self.cv_results[[col, 'mean_test_score']].corr().iloc[0, 1]\n",
    "                    if not np.isnan(correlation):\n",
    "                        param_importance[param] = abs(correlation)\n",
    "        \n",
    "        if param_importance:\n",
    "            pd.Series(param_importance).sort_values().plot(kind='barh', ax=ax)\n",
    "            ax.set_title('Relative Importance ')\n",
    "            ax.set_xlabel('Importance score')\n",
    "        \n",
    "        # Best score distribution\n",
    "        ax = axes[1, 0]\n",
    "        ax.hist(self.cv_results['mean_test_score'], bins=30, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(self.grid_search.best_score_, color='red', linestyle='--', \n",
    "                  label=f'Best: {self.grid_search.best_score_:.4f}')\n",
    "        ax.set_xlabel('ROC-AUC Score')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Scores CV distribution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training time vs performance\n",
    "        ax = axes[1, 1]\n",
    "        if 'mean_fit_time' in self.cv_results.columns:\n",
    "            scatter = ax.scatter(self.cv_results['mean_fit_time'], \n",
    "                               self.cv_results['mean_test_score'],\n",
    "                               c=self.cv_results['mean_test_score'],\n",
    "                               cmap='viridis', alpha=0.6)\n",
    "            ax.set_xlabel('Training time (seconds)')\n",
    "            ax.set_ylabel('ROC-AUC Score')\n",
    "            ax.set_title('Trade-off Time vs Performance')\n",
    "            plt.colorbar(scatter, ax=ax)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'hyperparameter_results.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_feature_importance(self, save_dir=None):\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # CatBoost feature importance\n",
    "        ax = axes[0]\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': self.features_df.columns,\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(20)\n",
    "        \n",
    "        sns.barplot(data=feature_importance, x='importance', y='feature', ax=ax)\n",
    "        ax.set_title('Top 20 Features')\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        \n",
    "        # CatBoost object importance \n",
    "        ax = axes[1]\n",
    "        try:\n",
    "            obj_importance = self.model.get_feature_importance(prettified=True)\n",
    "            if len(obj_importance) > 0:\n",
    "                top_obj_importance = obj_importance.head(15)\n",
    "                sns.barplot(data=top_obj_importance, x='Importances', y='Feature Id', ax=ax)\n",
    "                ax.set_title('Top 15 Features (Object Importance)')\n",
    "            else:\n",
    "                # Fallback: show only 15 most important features\n",
    "                top_15 = feature_importance.head(15)\n",
    "                sns.barplot(data=top_15, x='importance', y='feature', ax=ax)\n",
    "                ax.set_title('Top 15 Features ')\n",
    "        except:\n",
    "            # Fallback: show only 15 most important features\n",
    "            top_15 = feature_importance.head(15)\n",
    "            sns.barplot(data=top_15, x='importance', y='feature', ax=ax)\n",
    "            ax.set_title('Top 15 Features')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'feature_importance.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_roc_curve(self, save_dir=None):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, self.y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        ax.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "        \n",
    "        # Find optimal threshold\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        ax.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, \n",
    "                  label=f'Optimal point (threshold={optimal_threshold:.3f})')\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'roc_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_pr_curve(self, save_dir=None):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        precision, recall, thresholds = precision_recall_curve(self.y_test, self.y_pred_proba)\n",
    "        average_precision = average_precision_score(self.y_test, self.y_pred_proba)\n",
    "        \n",
    "        # Plot PR\n",
    "        ax.plot(recall, precision, color='blue', lw=2,\n",
    "                label=f'PR curve (AP = {average_precision:.3f})')\n",
    "        \n",
    "        \n",
    "        baseline = np.sum(self.y_test) / len(self.y_test)\n",
    "        ax.axhline(y=baseline, color='red', linestyle='--', \n",
    "                  label=f'Baseline (Random) = {baseline:.3f}')\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_title('Precision-Recall Curve')\n",
    "        ax.legend(loc=\"lower left\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'precision_recall_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_confusion_matrix(self, save_dir=None):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "        \n",
    "        # 1. Confusion matrix with absolute values\n",
    "        ax = axes[0]\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_title('Confusion Matrix - Valori Assoluti')\n",
    "        \n",
    "        # 2. Normalized confusion matrix\n",
    "        ax = axes[1]\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=ax)\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_title('Confusion Matrix - Normalized')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_probability_distribution(self, save_dir=None):\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 1. Class probability distribution\n",
    "        ax = axes[0]\n",
    "        ax.hist(self.y_pred_proba[self.y_test == 0], bins=50, alpha=0.5, \n",
    "                label='Classe 0', color='blue', density=True)\n",
    "        ax.hist(self.y_pred_proba[self.y_test == 1], bins=50, alpha=0.5, \n",
    "                label='Classe 1', color='red', density=True)\n",
    "        ax.axvline(x=0.5, color='black', linestyle='--', label='Threshold=0.5')\n",
    "        ax.set_xlabel('Probabilit Predetta')\n",
    "        ax.set_ylabel('Densit')\n",
    "        ax.set_title('Distribuzione delle Probabilit per Classe')\n",
    "        ax.legend()\n",
    "        \n",
    "        # 2. Probability box plot\n",
    "        ax = axes[1]\n",
    "        data_to_plot = [self.y_pred_proba[self.y_test == 0], \n",
    "                       self.y_pred_proba[self.y_test == 1]]\n",
    "        ax.boxplot(data_to_plot, labels=['Classe 0', 'Classe 1'])\n",
    "        ax.set_ylabel('Predicted Probability')\n",
    "        ax.set_title('Probability Box Plot per Class')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'probability_distribution.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "  \n",
    "    def _plot_learning_curves(self, save_dir=None):\n",
    "        _, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            evals_result = self.model.get_evals_result()\n",
    "            \n",
    "            if evals_result and 'learn' in evals_result:\n",
    "                if 'AUC' in evals_result['learn']:\n",
    "                    train_auc = evals_result['learn']['AUC']\n",
    "                    epochs = range(len(train_auc))\n",
    "                    ax.plot(epochs, train_auc, label='Train AUC', color='blue')\n",
    "                \n",
    "                if 'validation' in evals_result and 'AUC' in evals_result['validation']:\n",
    "                    val_auc = evals_result['validation']['AUC']\n",
    "                    epochs = range(len(val_auc))\n",
    "                    ax.plot(epochs, val_auc, label='Validation AUC', color='orange')\n",
    "                \n",
    "                ax.set_xlabel('Iteration')\n",
    "                ax.set_ylabel('AUC Score')\n",
    "                ax.set_title('Learning Curves - AUC during Training')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Learning curves non disponibili\\n(modello non trainato con eval_set)', \n",
    "                       ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "                ax.set_title('Learning Curves')\n",
    "                \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f'Error retriving learning curves:\\n{str(e)}', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "            ax.set_title('Learning Curves')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'learning_curves.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    " \n",
    "    def _plot_calibration_curve(self, save_dir=None):\n",
    "        \"\"\"Visualizza la curva di calibrazione\"\"\"\n",
    "        from sklearn.calibration import calibration_curve\n",
    "        _, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            self.y_test, self.y_pred_proba, n_bins=10\n",
    "        )\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(mean_predicted_value, fraction_of_positives, 's-', label='CatBoost')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "        \n",
    "        ax.set_xlabel('Mean Predicted Probability')\n",
    "        ax.set_ylabel('Fraction of Positives')\n",
    "        ax.set_title('Calibration Plot (Reliability Diagram)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'calibration_curve.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def _plot_classification_report(self, save_dir=None):\n",
    "        \"\"\"Visualizza il classification report come heatmap\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # Genera il classification report\n",
    "        report = classification_report(self.y_test, self.y_pred, output_dict=True)\n",
    "        \n",
    "        df_report = pd.DataFrame(report).transpose()\n",
    "        df_report = df_report.iloc[:-1, :-1] \n",
    "        \n",
    "        sns.heatmap(df_report, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax)\n",
    "        ax.set_title('Classification Report Heatmap')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'classification_report.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def run_complete_pipeline(self, model_name=None):\n",
    "        print(\"=== Starting complete pipeline ===\\n\")\n",
    "        with mlflow.start_run() as run:\n",
    "            self.run_id = run.info.run_id\n",
    "            print(f\"MLflow Run ID: {self.run_id}\")\n",
    "            \n",
    "            try:\n",
    "                self.prepare_data()\n",
    "                self.hyperparameter_tuning()\n",
    "                self.train_final_model()\n",
    "                model_path, scaler_path, metadata_path = self.save_model_and_scaler(model_name)\n",
    "        \n",
    "                self.plot_all_visualizations()\n",
    "\n",
    "                mlflow.set_tag(\"pipeline_status\", \"completed\")\n",
    "                mlflow.set_tag(\"model_type\", \"catboost_binary_classifier\")\n",
    "                \n",
    "                print(\"\\n=== PIPELINE COMPLETED ===\")\n",
    "                print(f\"MLflow Run: {self.run_id}\")\n",
    "                print(f\"Modello saved in: {model_path}\")\n",
    "                print(f\"Scaler saved in: {scaler_path}\")\n",
    "                \n",
    "                return self.model, self.best_params, self.run_id\n",
    "                \n",
    "            except Exception as e:\n",
    "                mlflow.set_tag(\"pipeline_status\", \"failed\")\n",
    "                mlflow.log_param(\"error_message\", str(e))\n",
    "                print(f\"Errore in pipeline: {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef817923",
   "metadata": {},
   "source": [
    "### Main\n",
    "#### Experiment will be named after the datetime of the run\n",
    "#### Model and scaler will be save in ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d53292",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%d%m%y_%H%M\")\n",
    "pipeline = CatBoostPipeline(\n",
    "    features_df=features_df, \n",
    "    labels_df=labels_df,\n",
    "    experiment_name=f\"catboost_experiment_{date}\",\n",
    "    model_save_dir=f\"./models/catboost_run_{date}\",\n",
    "    save_voter=True\n",
    ")\n",
    "\n",
    "# Esegui la pipeline completa\n",
    "model, best_params, run_id = pipeline.run_complete_pipeline(\n",
    "    model_name=f\"catboost_dev_model_{date}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de148dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize model with full data\n",
    "final_model, final_scaler, final_run_id = pipeline.finalize_model(\n",
    "     features_df_to_finalize=features_df_to_finalize,\n",
    "     labels_df_to_finalize=labels_df_to_finalize,\n",
    "     model_path=\"./models/catboost_run_{date}/catboost_dev_model_{date}.cbm\",\n",
    "     scaler_path=\"./models/catboost_run_{date}/catboost_dev_model_v1_scaler.joblib\",\n",
    "     final_model_name=f\"catboost_trained_on_full_data_model_{date}\",\n",
    "     use_early_stopping=True\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07eeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Predizioni con modello salvato\n",
    "\n",
    "# new_predictions, new_probabilities = pipeline.predict_new_data(new_data_df)\n",
    "\n",
    "# Per caricare un modello salvato\n",
    "# model, scaler = CatBoostPipeline.load_model_and_scaler(\n",
    "#     \"./my_models/production_model_v1.cbm\", \n",
    "#     \"./my_models/production_model_v1_scaler.joblib\"\n",
    "# )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
