{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22870bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pycaret\n",
    "from datetime import timezone, datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "\n",
    "class SurfaceTypeUtils:\n",
    "    surface_type_dict = {\n",
    "        -1: \"Ocean\",\n",
    "        0: \"NaN\",\n",
    "        1: \"Artifical\",\n",
    "        2: \"Barely vegetated\",\n",
    "        3: \"Inland water\",\n",
    "        4: \"Crop\",\n",
    "        5: \"Grass\",\n",
    "        6: \"Shrub\",\n",
    "        7: \"Forest\"\n",
    "    }\n",
    "    ddm_antennas = {\n",
    "        0: 'None',\n",
    "        1: 'Zenith',\n",
    "        2: 'LHCP',\n",
    "        3: 'RHCP',\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_shapefile_path = gpd.read_file('D:/GREAT/machine_learning/data/land_vs_water_country_borders/ne_110m_admin_0_countries.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc26790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoUtils:\n",
    "    def __init__(self, world_shapefile_path):\n",
    "        self.world = gpd.read_file(world_shapefile_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def is_land(self, lat, lon):\n",
    "        point = Point(lon, lat)\n",
    "        return any(self.world.contains(point))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_ocean_and_land(lst):\n",
    "        has_ocean = -1 in lst\n",
    "        has_land = any(1 <= num <= 7 for num in lst)\n",
    "        return has_ocean and has_land\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeoHelper:\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCDFPreprocessor:\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        \n",
    "        raw_counts = f.variables['raw_counts']\n",
    "        raw_counts = np.array(raw_counts)\n",
    "\n",
    "        # Calcolo distanza tra il punto speculare e l'aereo\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        # Seleziona gli indici dove sp_rx_gain_copol > 5, sp_rx_gain_xpol > 5 e ddm_snr > 0 e distanza tra punto speculare e antenna > 2000 e < 10000\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "        #discard_indices =  np.argwhere(~keep_mask)\n",
    "\n",
    "\n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        #filtered_raw_counts_arr = np.array(filtered_raw_counts)\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Inserisci i dati filtrati nelle posizioni di to_keep_indices\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "\n",
    "        label_data = surface_types_unravelled\n",
    "        label_data = [label_data[l] for l in range(len(label_data)) if l in keep_indices]\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def process_all_files(self, load_and_save_data = bool):\n",
    "        if load_and_save_data:\n",
    "            full_data = []\n",
    "            full_labels = []\n",
    "            for file_name in tqdm(self.netcdf_file_list, desc=\"Processing files\"):\n",
    "                if not file_name.endswith('.nc'):\n",
    "                    continue\n",
    "                f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                data, labels = self.preprocess(f)\n",
    "                full_data.append(data)\n",
    "                full_labels.append(labels)\n",
    "            \n",
    "            # Trova gli indici degli elementi di full_data con seconda dimensione uguale a 200\n",
    "            valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "\n",
    "            # Applica la selezione a full_data e full_labels\n",
    "            full_data_clean = [full_data[i] for i in valid_indices]\n",
    "            full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "\n",
    "            full_data_clean_flat = np.vstack(full_data_clean)\n",
    "            full_labels_clean_flat = np.hstack(full_labels_clean)\n",
    "\n",
    "            import pyarrow as pa\n",
    "            import pyarrow.parquet as pq\n",
    "\n",
    "            # Crea la cartella processed_data se non esiste\n",
    "            os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "            # Salva fit_data in formato parquet ottimizzato\n",
    "            fit_data_df = pd.DataFrame(full_data_clean_flat)\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                'processed_data/fit_data_multiclass.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "                data_page_size=len(full_data_clean_flat)\n",
    "            )\n",
    "            del fit_data_df\n",
    "\n",
    "            # Salva labels in formato parquet ottimizzato\n",
    "            labels_df = pd.DataFrame(full_labels_clean_flat, columns=['label'])\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                'processed_data/labels_multiclass.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "                data_page_size=len(full_labels_clean_flat)\n",
    "            )\n",
    "\n",
    "            del labels_df\n",
    "        else: \n",
    "            import polars as pl\n",
    "            \n",
    "            # Leggi i file parquet con polars\n",
    "            full_data_pl = pl.read_parquet('processed_data/fit_data_multiclass.parquet')\n",
    "            full_labels_pl = pl.read_parquet('processed_data/labels_multiclass.parquet')\n",
    "\n",
    "            # Trasforma in numpy array\n",
    "            full_data_clean_flat = full_data_pl.to_numpy()\n",
    "            full_labels_clean_flat = full_labels_pl['label'].to_numpy()\n",
    "\n",
    "\n",
    "        return full_data_clean_flat, full_labels_clean_flat\n",
    "       \n",
    "            \n",
    "    \n",
    "class ModelTrainer:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.final_model = None\n",
    "\n",
    "    def train(self, model_search=True):\n",
    "        os.environ[\"PYCARET_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "        if model_search:\n",
    "            scaler = MinMaxScaler()\n",
    "            fit_data_scaled = scaler.fit_transform(self.data)\n",
    "            clf = setup(data=fit_data_scaled,\n",
    "                        target=self.labels,\n",
    "                        pca=True,\n",
    "                        pca_method='incremental',\n",
    "                        use_gpu=True\n",
    "                        )\n",
    "            best_models = compare_models(n_select=5)\n",
    "            best_model = best_models[0]\n",
    "            print(f\"Il modello migliore Ã¨: {best_model}\")\n",
    "            tuned_model = tune_model(best_model,\n",
    "                                    optimize='Recall',\n",
    "                                    n_iter=10,\n",
    "                                    search_library='optuna',\n",
    "                                    search_algorithm='tpe',\n",
    "                                    choose_better=True)\n",
    "            print(\"Valutazione del modello ottimizzato:\")\n",
    "            evaluate_model(tuned_model)\n",
    "            best_params = tuned_model.get_params()\n",
    "            print(\"Migliori iperparametri trovati:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "            self.final_model = finalize_model(tuned_model)\n",
    "            save_model(self.final_model, 'best_classification_model')\n",
    "            # loaded_model = load_model('best_classification_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_backup = False\n",
    "if read_from_backup:\n",
    "    import polars as pl\n",
    "\n",
    "    # Leggi i file parquet con polars\n",
    "    fit_data_pl = pl.read_parquet('processed_data/fit_data_multiclass.parquet')\n",
    "    labels_pl = pl.read_parquet('processed_data/labels_multiclass.parquet')\n",
    "\n",
    "    # Trasforma in numpy array\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()\n",
    "else:\n",
    "    preprocessor = NetCDFPreprocessor(root_dir='D:/GREAT/machine_learning/data/Rongowai/')\n",
    "    fit_data, labels = preprocessor.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad767f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Esegui il campionamento stratificato, prendi 1-test_size%  del campione\n",
    "fit_data_sample, _, labels_sample, _ = train_test_split(\n",
    "    fit_data, labels, test_size=0.90, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fit_data_sample), len(labels_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78048561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_trainer = ModelTrainer(data=fit_data_sample, labels=labels_sample)\n",
    "model_trainer.train(model_search=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
