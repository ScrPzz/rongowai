{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22870bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "from scipy.fft import fft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "from pycaret.classification import setup, compare_models, tune_model, finalize_model, save_model, plot_model, evaluate_model, dashboard, save_experiment, blend_models\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurfaceTypeUtils:\n",
    "    surface_type_dict = {\n",
    "        -1: \"Ocean\",\n",
    "        0: \"NaN\",\n",
    "        1: \"Artifical\",\n",
    "        2: \"Barely vegetated\",\n",
    "        3: \"Inland water\",\n",
    "        4: \"Crop\",\n",
    "        5: \"Grass\",\n",
    "        6: \"Shrub\",\n",
    "        7: \"Forest\"\n",
    "    }\n",
    "    ddm_antennas = {\n",
    "        0: 'None',\n",
    "        1: 'Zenith',\n",
    "        2: 'LHCP',\n",
    "        3: 'RHCP',\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dea071",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR= 'D:/data/RONGOWAI_L1_SDR_V1.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc26790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoUtils:\n",
    "    def __init__(self, world_shapefile_path):\n",
    "        self.world = gpd.read_file(world_shapefile_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def is_land(self, lat, lon):\n",
    "        point = Point(lon, lat)\n",
    "        return any(self.world.contains(point))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_ocean_and_land(lst):\n",
    "        has_ocean = -1 in lst\n",
    "        has_land = any(1 <= num <= 7 for num in lst)\n",
    "        return has_ocean and has_land\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeoHelper:\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCDFPreprocessor:\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        \n",
    "        raw_counts = f.variables['raw_counts']\n",
    "        raw_counts = np.array(raw_counts)\n",
    "\n",
    "        # Calcolo distanza tra il punto speculare e l'aereo\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        # Seleziona gli indici dove sp_rx_gain_copol > 5, sp_rx_gain_xpol > 5 e ddm_snr > 0 e distanza tra punto speculare e antenna > 2000 e < 10000\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "        \n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        #filtered_raw_counts_arr = np.array(filtered_raw_counts)\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Inserisci i dati filtrati nelle posizioni di to_keep_indices\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "        del output_array\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "\n",
    "        label_data = surface_types_unravelled\n",
    "        label_data = [label_data[lab] for lab in range(len(label_data)) if lab in keep_indices]\n",
    "\n",
    "        assert np.array(fit_data).shape[0] == len(label_data), \\\n",
    "            f\"Shape mismatch: fit_data {np.array(fit_data).shape[0]}, label_data {len(label_data)}\"\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def process_all_files(self, chunk_size = int, sample_fraction = float, remove_chunks= bool):\n",
    "        \n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        #counter = 0\n",
    "        for file_name in tqdm(self.netcdf_file_list, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                continue\n",
    "            try:\n",
    "                f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                data, labels = self.preprocess(f)\n",
    "                full_data.append(data)\n",
    "                full_labels.append(labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "            #counter += 1\n",
    "            #if counter == 100:  # Limita a 50 file per il caricamento\n",
    "            #    break\n",
    "        \n",
    "        # Trova gli indici degli elementi di full_data con seconda dimensione uguale a 200\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "\n",
    "        # Applica la selezione a full_data e full_labels\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "\n",
    "        del full_data, full_labels # Libera memoria\n",
    "\n",
    "        # Chunking \n",
    "        \n",
    "        os.makedirs('processed_data/multiclass', exist_ok=True)\n",
    "\n",
    "        chunk_size = chunk_size # dimensione del chunk in numero di campioni\n",
    "        sample_fraction = sample_fraction  # frazione di dati da campionare per ogni chunk\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean))\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            print('-' * 50)\n",
    "            \n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "            \n",
    "            # Salva ogni chunk come file parquet separato\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'processed_data/multiclass/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'processed_data/multiclass/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            del table_fit, table_labels, fit_data_df, labels_df\n",
    "\n",
    "            # Trova le classi più rare (meno rappresentate)\n",
    "            label_counts = Counter(chunk_labels)\n",
    "            min_count = min(label_counts.values())\n",
    "            rare_classes = [cls for cls, count in label_counts.items() if count == min_count]\n",
    "\n",
    "            # Seleziona tutte le occorrenze delle classi rare\n",
    "            rare_indices = np.isin(chunk_labels, rare_classes)\n",
    "            X_rare = chunk_data[rare_indices]\n",
    "            y_rare = chunk_labels[rare_indices]\n",
    "\n",
    "            # Per le altre classi, esegui un campionamento casuale per raggiungere la frazione desiderata\n",
    "            other_indices = ~rare_indices\n",
    "            X_other = chunk_data[other_indices]\n",
    "            y_other = chunk_labels[other_indices]\n",
    "\n",
    "            _, X_other_sampled, _, y_other_sampled = train_test_split(\n",
    "                X_other, y_other,\n",
    "                test_size=sample_fraction,\n",
    "                stratify=y_other,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            # Combina i dati delle classi rare con quelli campionati delle altre classi\n",
    "            X_sampled = np.vstack([X_rare, X_other_sampled])\n",
    "            y_sampled = np.hstack([y_rare, y_other_sampled])\n",
    "\n",
    "            print(f\"Distribuzione etichette prima del campionamento nel chunk {idx + 1}: {Counter(chunk_labels)}\")\n",
    "            print(f\"Distribuzione etichette dopo campionamento nel chunk {idx + 1}: {Counter(y_sampled)}\")\n",
    "\n",
    "            print('-' * 50)\n",
    "            \n",
    "            \n",
    "            \n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "\n",
    "        print(f\"Distribuzione totale etichette dopo stratificazione: {Counter(full_labels_sampled_stratified)}\")\n",
    "\n",
    "        \n",
    "        print(f\"Shape of sampled data after chunking and sampling: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels after chunking and sampling: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "        \n",
    "        # Crea la cartella processed_data se non esiste\n",
    "        os.makedirs('processed_data/multiclass', exist_ok=True)\n",
    "\n",
    "        # Salva fit_data in formato parquet ottimizzato\n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'processed_data/multiclass/fit_data_multiclass_raw_counts_only.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Salva labels in formato parquet ottimizzato\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'processed_data/multiclass/labels_multiclass_raw_counts_only.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        del fit_data_df, labels_df, table_fit, table_labels\n",
    "        # Remove all chunk parquet files if flag is set\n",
    "        if remove_chunks:\n",
    "            try:\n",
    "                chunk_dir = 'processed_data/multiclass'\n",
    "                for fname in os.listdir(chunk_dir):\n",
    "                    if fname.startswith('fit_data_chunk_') or fname.startswith('labels_chunk_'):\n",
    "                        os.remove(os.path.join(chunk_dir, fname))\n",
    "                print(\"All chunk files removed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing chunk files: {e}\")\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def gini(self, array):\n",
    "            \"\"\"Calcola il coefficiente di Gini (disuguaglianza)\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))    \n",
    "    def extract_ddm_features(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"\n",
    "        Estrae features dettagliate da raw_counts DDM (shape: n_samples x 200)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float64) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. Statistiche base\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = self.gini(x)\n",
    "\n",
    "            # 2. Posizionali\n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentazione\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "            \n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivate e cambiamenti\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelazioni (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT (spettro frequenze)\n",
    "            spectrum = np.abs(fft(x))\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  # simmetrico\n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "            \n",
    "            #Aggiungi le statistiche dei quadranti e del centro\n",
    "            ddm = row.reshape(10, 20)  # 10x20\n",
    "\n",
    "            # Quadranti\n",
    "            q1 = ddm[:5, :10].ravel()\n",
    "            q2 = ddm[:5, 10:].ravel()\n",
    "            q3 = ddm[5:, :10].ravel()\n",
    "            q4 = ddm[5:, 10:].ravel()\n",
    "            # Quadrante centrale (4x8 centrale)\n",
    "            center = ddm[3:7, 6:14].ravel()\n",
    "            \n",
    "            # Statistiche dei quadranti \n",
    "            f['q1_mean'] = np.mean(q1)\n",
    "            f['q2_mean'] = np.mean(q2)      \n",
    "            f['q3_mean'] = np.mean(q3)\n",
    "            f['q4_mean'] = np.mean(q4)\n",
    "            f['center_mean'] = np.mean(center)\n",
    "            f['q1_std'] = np.std(q1)\n",
    "            f['q2_std'] = np.std(q2)\n",
    "            f['q3_std'] = np.std(q3)\n",
    "            f['q4_std'] = np.std(q4)\n",
    "            f['center_std'] = np.std(center)\n",
    "            f['q1_min'] = np.min(q1)\n",
    "            f['q2_min'] = np.min(q2)\n",
    "            f['q3_min'] = np.min(q3)\n",
    "            f['q4_min'] = np.min(q4)\n",
    "            f['center_min'] = np.min(center)\n",
    "            f['q1_max'] = np.max(q1)\n",
    "            f['q2_max'] = np.max(q2)\n",
    "            f['q3_max'] = np.max(q3)\n",
    "            f['q4_max'] = np.max(q4)\n",
    "            f['center_max'] = np.max(center)\n",
    "            f['q1_median'] = np.median(q1)\n",
    "            f['q2_median'] = np.median(q2)\n",
    "            f['q3_median'] = np.median(q3)\n",
    "            f['q4_median'] = np.median(q4)\n",
    "            f['center_median'] = np.median(center)\n",
    "            f['q1_range'] = np.max(q1) - np.min(q1)\n",
    "            f['q2_range'] = np.max(q2) - np.min(q2)\n",
    "            f['q3_range'] = np.max(q3) - np.min(q3)\n",
    "            f['q4_range'] = np.max(q4) - np.min(q4)\n",
    "            f['center_range'] = np.max(center) - np.min(center)\n",
    "            f['q1_skew'] = skew(q1)\n",
    "            f['q2_skew'] = skew(q2)\n",
    "            f['q3_skew'] = skew(q3)\n",
    "            f['q4_skew'] = skew(q4)\n",
    "            f['center_skew'] = skew(center)\n",
    "            f['q1_kurtosis'] = kurtosis(q1)\n",
    "            f['q2_kurtosis'] = kurtosis(q2)\n",
    "            f['q3_kurtosis'] = kurtosis(q3)\n",
    "            f['q4_kurtosis'] = kurtosis(q4)\n",
    "            f['center_kurtosis'] = kurtosis(center)\n",
    "            f['q1_entropy'] = entropy(q1 + 1e-10)\n",
    "            f['q2_entropy'] = entropy(q2 + 1e-10)\n",
    "            f['q3_entropy'] = entropy(q3 + 1e-10)\n",
    "            f['q4_entropy'] = entropy(q4 + 1e-10)\n",
    "            f['center_entropy'] = entropy(center + 1e-10)\n",
    "            f['q1_gini'] = self.gini(q1)\n",
    "            f['q2_gini'] = self.gini(q2)\n",
    "            f['q3_gini'] = self.gini(q3)\n",
    "            f['q4_gini'] = self.gini(q4)\n",
    "            f['center_gini'] = self.gini(center)\n",
    "\n",
    "            # Statistiche di confronto tra quadranti e centro\n",
    "            \n",
    "            # Differenze tra media dei quadranti e centro\n",
    "            f['q1_center_mean_diff'] = f['q1_mean'] - f['center_mean']\n",
    "            f['q2_center_mean_diff'] = f['q2_mean'] - f['center_mean']\n",
    "            f['q3_center_mean_diff'] = f['q3_mean'] - f['center_mean']\n",
    "            f['q4_center_mean_diff'] = f['q4_mean'] - f['center_mean']\n",
    "\n",
    "            # Differenze tra std dei quadranti e centro\n",
    "            f['q1_center_std_diff'] = f['q1_std'] - f['center_std']\n",
    "            f['q2_center_std_diff'] = f['q2_std'] - f['center_std']\n",
    "            f['q3_center_std_diff'] = f['q3_std'] - f['center_std']\n",
    "            f['q4_center_std_diff'] = f['q4_std'] - f['center_std']\n",
    "\n",
    "            # Differenze tra max dei quadranti e centro\n",
    "            f['q1_center_max_diff'] = f['q1_max'] - f['center_max']\n",
    "            f['q2_center_max_diff'] = f['q2_max'] - f['center_max']\n",
    "            f['q3_center_max_diff'] = f['q3_max'] - f['center_max']\n",
    "            f['q4_center_max_diff'] = f['q4_max'] - f['center_max']\n",
    "\n",
    "            # Differenze tra min dei quadranti e centro\n",
    "            f['q1_center_min_diff'] = f['q1_min'] - f['center_min']\n",
    "            f['q2_center_min_diff'] = f['q2_min'] - f['center_min']\n",
    "            f['q3_center_min_diff'] = f['q3_min'] - f['center_min']\n",
    "            f['q4_center_min_diff'] = f['q4_min'] - f['center_min']\n",
    "\n",
    "            # Differenze tra entropia dei quadranti e centro\n",
    "            f['q1_center_entropy_diff'] = f['q1_entropy'] - f['center_entropy']\n",
    "            f['q2_center_entropy_diff'] = f['q2_entropy'] - f['center_entropy']\n",
    "            f['q3_center_entropy_diff'] = f['q3_entropy'] - f['center_entropy']\n",
    "            f['q4_center_entropy_diff'] = f['q4_entropy'] - f['center_entropy']\n",
    "\n",
    "            # Differenze tra gini dei quadranti e centro\n",
    "            f['q1_center_gini_diff'] = f['q1_gini'] - f['center_gini']\n",
    "            f['q2_center_gini_diff'] = f['q2_gini'] - f['center_gini']\n",
    "            f['q3_center_gini_diff'] = f['q3_gini'] - f['center_gini']\n",
    "            f['q4_center_gini_diff'] = f['q4_gini'] - f['center_gini']\n",
    "\n",
    "            # Differenze tra skewness dei quadranti e centro\n",
    "            f['q1_center_skew_diff'] = f['q1_skew'] - f['center_skew']\n",
    "            f['q2_center_skew_diff'] = f['q2_skew'] - f['center_skew']\n",
    "            f['q3_center_skew_diff'] = f['q3_skew'] - f['center_skew']\n",
    "            f['q4_center_skew_diff'] = f['q4_skew'] - f['center_skew']\n",
    "\n",
    "            # Differenze tra kurtosis dei quadranti e centro\n",
    "            f['q1_center_kurtosis_diff'] = f['q1_kurtosis'] - f['center_kurtosis']\n",
    "            f['q2_center_kurtosis_diff'] = f['q2_kurtosis'] - f['center_kurtosis']\n",
    "            f['q3_center_kurtosis_diff'] = f['q3_kurtosis'] - f['center_kurtosis']\n",
    "            f['q4_center_kurtosis_diff'] = f['q4_kurtosis'] - f['center_kurtosis']\n",
    "\n",
    "            features.append(f)\n",
    "            \n",
    "\n",
    "        return features\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5246e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.final_model = None\n",
    "\n",
    "    def visualize_model_performances(self, model):\n",
    "        #try:\n",
    "        #    print(\"Creazione del dashboard del modello...\")\n",
    "        #    dashboard(model)\n",
    "        #except Exception as e:\n",
    "        #    print(f\"Errore durante la creazione del dashboard: {e}\")\n",
    "\n",
    "        try:\n",
    "            print(\"Valutazione del modello...\")\n",
    "            evaluate_model(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante la valutazione del modello: {e}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Creazione della matrice di confusione del modello...\")\n",
    "            plot_model(model, plot='confusion_matrix', save=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante la creazione della matrice di confusione: {e}\")\n",
    "\n",
    "        try:\n",
    "            print(\"Creazione del grafico delle feature del modello...\")\n",
    "            plot_model(model, plot='feature_all', save=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante la creazione del grafico delle feature: {e}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Creazione del report di classificazione del modello...\")\n",
    "            plot_model(model, plot='class_report', save=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante la creazione del report di classificazione: {e}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Creazione del grafico PR del modello...\")\n",
    "            plot_model(model, plot='pr', save=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante la creazione del grafico PR: {e}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Calibrazione del modello...\")\n",
    "            plot_model(model, plot='calibration', save=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante la creazione del grafico di calibrazione: {e}\")\n",
    "\n",
    "\n",
    "    def search_and_train_model(self, model_search=True):\n",
    "        os.environ[\"PYCARET_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "        if model_search:\n",
    "            scaler = MinMaxScaler()\n",
    "            fit_data_scaled = scaler.fit_transform(self.data)\n",
    "            clf_exp = setup(data=fit_data_scaled,\n",
    "                        target=self.labels,\n",
    "                        #pca=True,\n",
    "                        #pca_method='incremental',\n",
    "                        use_gpu=True,\n",
    "                        feature_selection=True,\n",
    "                        #n_features_to_select=.4,\n",
    "                        )\n",
    "            \n",
    "            best_models = compare_models(n_select=3, \n",
    "                                         exclude=['gbc', 'dummy', 'qda', 'lda', 'nb', 'svm'],\n",
    "                                         sort='Accuracy',\n",
    "                                         )\n",
    "\n",
    "            best_model = best_models[0]\n",
    "\n",
    "            print(f\"Il modello migliore è: {best_model}\")\n",
    "            \n",
    "            print(\"Ottimizzazione degli iperparametri del modello migliore...\")\n",
    "            tuned_model = tune_model(best_model,\n",
    "                                    optimize='Accuracy',\n",
    "                                    n_iter=10,\n",
    "                                    search_library='optuna',\n",
    "                                    search_algorithm='tpe',\n",
    "                                    choose_better=True)\n",
    "            print(\"Valutazione del modello ottimizzato:\")\n",
    "\n",
    "            best_params = tuned_model.get_params()\n",
    "\n",
    "            print(\"Migliori iperparametri trovati:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "\n",
    "            self.final_model = finalize_model(tuned_model)\n",
    "\n",
    "            save_model(self.final_model, 'best_binary_classification_model', prep_pipeline=True)\n",
    "            print(\"Modello finale salvato come 'best_binary_classification_model'.\")\n",
    "            # Salva l'esperimento\n",
    "            save_experiment('binary_classification_experiment')\n",
    "            print(\"Esperimento salvato come 'binary_classification_experiment'.\")\n",
    "            self.visualize_model_performances(self.final_model)\n",
    "      \n",
    "        \n",
    "    def train_ensemble_model(self, n_sample_per_class=int):  \n",
    "        os.environ[\"PYCARET_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "        # Campiona  righe con distribuzione bilanciata tra le classi di labels_df  \n",
    "        sampled_indices = (\n",
    "            self.labels.groupby(self.labels.iloc[:, 0])\n",
    "            .apply(lambda x: x.sample(n=n_sample_per_class, random_state=42))\n",
    "            .index.get_level_values(1)\n",
    "        )\n",
    "        features_df = self.data.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = self.labels.loc[sampled_indices].reset_index(drop=True)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        fit_data_scaled = scaler.fit_transform(features_df)\n",
    "        clf_exp = setup(data=fit_data_scaled,\n",
    "                    target=labels_df['0'],\n",
    "                    #pca=True,\n",
    "                    #pca_method='incremental',\n",
    "                    use_gpu=True,\n",
    "                    feature_selection=True,\n",
    "                    n_features_to_select=.4,\n",
    "                    )\n",
    "        \n",
    "        best_models = compare_models(n_select=3, \n",
    "                                        exclude=['gbc', 'dummy', 'qda', 'lda', 'nb', 'svm'],\n",
    "                                        sort='Accuracy',\n",
    "                                        )  \n",
    "        \n",
    "        print(\"Ensembling dei migliori modelli...\")\n",
    "        best_models = [model for model in best_models if model is not None]\n",
    "        print(f\"Modelli selezionati per l'ensembling: {best_models}\")\n",
    "        ensembled_models = blend_models(best_models, \n",
    "                                        method='soft', \n",
    "                                        fold=5, \n",
    "                                        optimize='Accuracy', \n",
    "                                        )\n",
    "        if ensembled_models is None:\n",
    "            print(\"Nessun modello ensembled creato. Verifica i modelli selezionati.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Modelli ensembled creati: {ensembled_models}\")\n",
    "\n",
    "        self.visualize_model_performances(ensembled_models)\n",
    "\n",
    "        self.final_ensembled_model = finalize_model(ensembled_models)\n",
    "        save_model(self.final_ensembled_model, 'best_binary_classification_ensembled_model', prep_pipeline=True)\n",
    "        save_experiment('binary_classification_ensembled_experiment')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd89985",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'D:/data/RONGOWAI_L1_SDR_V1.0/'\n",
    "\n",
    "read_from_backup = False  \n",
    "if read_from_backup:\n",
    "    \n",
    "    fit_data_pl = pd.read_parquet('processed_data/multiclass/fit_data_multiclass_raw_counts_only.parquet')\n",
    "    labels_pl = pd.read_parquet('processed_data/multiclass/labels_multiclass_raw_counts_only.parquet')\n",
    "\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()\n",
    "else:\n",
    "    preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR)\n",
    "    fit_data, labels = preprocessor.process_all_files(chunk_size=250, sample_fraction=0.1, remove_chunks=True)\n",
    "\n",
    "try:\n",
    "    del fit_data_pl, labels_pl\n",
    "except NameError:\n",
    "    print(\"fit_data_pl and labels_pl not defined, skipping deletion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_data = fit_data[:3000000]\n",
    "labels = labels[:3000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94934c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR)\n",
    "features_extractor = DDMFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_ddm_features_row(row):\n",
    "    return features_extractor.extract_ddm_features(np.array([row]))\n",
    "\n",
    "combined_features = Parallel(n_jobs=12, backend=\"loky\")(delayed(extract_ddm_features_row)(row) for row in tqdm(fit_data, desc=\"Estrazione features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed837dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_features = [row[0] if isinstance(row, list) and len(row) > 0 else row for row in combined_features]\n",
    "del combined_features\n",
    "FEATURES = list(flat_features[0].keys())\n",
    "combined_features = np.array([[row[key] for key in FEATURES] for row in flat_features])\n",
    "del flat_features\n",
    "combined_features.shape\n",
    "\n",
    "# Controlla infiniti o valori troppo grandi per float64\n",
    "mask_finite = np.isfinite(combined_features).all(axis=1) & (np.abs(combined_features) < np.finfo(np.float64).max).all(axis=1)\n",
    "\n",
    "fit_data_with_features_clean = combined_features[mask_finite]\n",
    "labels_clean = labels[mask_finite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b0feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('processed_data/multiclass/data_w_features', exist_ok=True)\n",
    "pd.DataFrame(fit_data_with_features_clean, columns=FEATURES).to_parquet('processed_data/multiclass/data_w_features/fit_data_multiclass_stat_features_only.parquet', index=False)\n",
    "pd.DataFrame(labels_clean).to_parquet('processed_data/multiclass/data_w_features/labels_multiclass_stat_features_only.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa la lista FEATURES già creata sopra per i nomi delle colonne\n",
    "fit_data_with_features_df = pd.DataFrame(fit_data_with_features_clean, columns=FEATURES)\n",
    "\n",
    "del fit_data_with_features_clean\n",
    "fit_data_with_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e681a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(data=fit_data_with_features_df, labels=labels_clean)\n",
    "model_trainer.train(model_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b6173",
   "metadata": {},
   "source": [
    "TARGET MAPPING = -1.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65d6f1",
   "metadata": {},
   "source": [
    "2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
