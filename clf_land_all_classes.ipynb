{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22870bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "from scipy.fft import fft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SurfaceTypeUtils:\n",
    "    surface_type_dict = {\n",
    "        -1: \"Ocean\",\n",
    "        0: \"NaN\",\n",
    "        1: \"Artifical\",\n",
    "        2: \"Barely vegetated\",\n",
    "        3: \"Inland water\",\n",
    "        4: \"Crop\",\n",
    "        5: \"Grass\",\n",
    "        6: \"Shrub\",\n",
    "        7: \"Forest\"\n",
    "    }\n",
    "    ddm_antennas = {\n",
    "        0: 'None',\n",
    "        1: 'Zenith',\n",
    "        2: 'LHCP',\n",
    "        3: 'RHCP',\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dea071",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR= 'D:/data/RONGOWAI_L1_SDR_V1.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc26790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoUtils:\n",
    "    def __init__(self, world_shapefile_path):\n",
    "        self.world = gpd.read_file(world_shapefile_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def is_land(self, lat, lon):\n",
    "        point = Point(lon, lat)\n",
    "        return any(self.world.contains(point))\n",
    "\n",
    "    @staticmethod\n",
    "    def check_ocean_and_land(lst):\n",
    "        has_ocean = -1 in lst\n",
    "        has_land = any(1 <= num <= 7 for num in lst)\n",
    "        return has_ocean and has_land\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeoHelper:\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "\n",
    "    @staticmethod\n",
    "    def add_seconds(time, seconds):\n",
    "        timestamp = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "        new_timestamp = timestamp + timedelta(seconds=seconds)\n",
    "        return new_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_and_filter(arr):\n",
    "        mask_all_nan = np.all(np.isnan(arr), axis=(2, 3))\n",
    "        arr_filled = arr.copy()\n",
    "        for i in range(arr.shape[0]):\n",
    "            nan_indices = np.where(mask_all_nan[i])[0]\n",
    "            if len(nan_indices) > 0:\n",
    "                valid_indices = np.where(~mask_all_nan[i])[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    mean_matrix = np.nanmean(arr[i, valid_indices, :, :], axis=0)\n",
    "                    arr_filled[i, nan_indices, :, :] = mean_matrix\n",
    "        mask_discard = np.all(mask_all_nan, axis=1)\n",
    "        arr_filtered = arr_filled[~mask_discard]\n",
    "        return arr_filtered, list(np.where(mask_discard.astype(int) == 1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCDFPreprocessor:\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        \n",
    "        raw_counts = f.variables['raw_counts']\n",
    "        raw_counts = np.array(raw_counts)\n",
    "\n",
    "        # Calcolo distanza tra il punto speculare e l'aereo\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        # Seleziona gli indici dove sp_rx_gain_copol > 5, sp_rx_gain_xpol > 5 e ddm_snr > 0 e distanza tra punto speculare e antenna > 2000 e < 10000\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "        \n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        #filtered_raw_counts_arr = np.array(filtered_raw_counts)\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Inserisci i dati filtrati nelle posizioni di to_keep_indices\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "        del output_array\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "\n",
    "        label_data = surface_types_unravelled\n",
    "        label_data = [label_data[l] for l in range(len(label_data)) if l in keep_indices]\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def process_all_files(self, chunk_size = int, sample_fraction = float, remove_chunks= bool):\n",
    "        \n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        #counter = 0\n",
    "        for file_name in tqdm(self.netcdf_file_list, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                continue\n",
    "            try:\n",
    "                f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                data, labels = self.preprocess(f)\n",
    "                full_data.append(data)\n",
    "                full_labels.append(labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "            #counter += 1\n",
    "            #if counter == 100:  # Limita a 50 file per il caricamento\n",
    "            #    break\n",
    "        \n",
    "        # Trova gli indici degli elementi di full_data con seconda dimensione uguale a 200\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "\n",
    "        # Applica la selezione a full_data e full_labels\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "\n",
    "        # Chunking \n",
    "        \n",
    "        os.makedirs('processed_data/multiclass', exist_ok=True)\n",
    "\n",
    "        chunk_size = chunk_size # dimensione del chunk in numero di campioni\n",
    "        sample_fraction = sample_fraction  # frazione di dati da campionare per ogni chunk\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean))\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            print(f\"Distribuzione etichette nel chunk: {Counter(chunk_labels)}\")\n",
    "            \n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "            print(''-' * 50')\n",
    "            \n",
    "\n",
    "            # Salva ogni chunk come file parquet separato\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'processed_data/multiclass/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'processed_data/multiclass/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "\n",
    "        # Imposta la frazione di dati da campionare per ogni chunk (es: 0.2 per il 20%)\n",
    "        \n",
    "            #_, X_sampled, _, y_sampled = train_test_split(\n",
    "            #    chunk_data, chunk_labels, \n",
    "            #    test_size=sample_fraction, \n",
    "            #    stratify=chunk_labels, \n",
    "            #    random_state=42\n",
    "            #) \n",
    "\n",
    "            # Trova le classi più rare (meno rappresentate)\n",
    "            label_counts = Counter(chunk_labels)\n",
    "            min_count = min(label_counts.values())\n",
    "            rare_classes = [cls for cls, count in label_counts.items() if count == min_count]\n",
    "\n",
    "            # Seleziona tutte le occorrenze delle classi rare\n",
    "            rare_indices = np.isin(chunk_labels, rare_classes)\n",
    "            X_rare = chunk_data[rare_indices]\n",
    "            y_rare = chunk_labels[rare_indices]\n",
    "\n",
    "            # Per le altre classi, esegui un campionamento casuale per raggiungere la frazione desiderata\n",
    "            other_indices = ~rare_indices\n",
    "            X_other = chunk_data[other_indices]\n",
    "            y_other = chunk_labels[other_indices]\n",
    "\n",
    "            _, X_other_sampled, _, y_other_sampled = train_test_split(\n",
    "                X_other, y_other,\n",
    "                test_size=sample_fraction,\n",
    "                stratify=y_other,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            # Combina i dati delle classi rare con quelli campionati delle altre classi\n",
    "            X_sampled = np.vstack([X_rare, X_other_sampled])\n",
    "            y_sampled = np.hstack([y_rare, y_other_sampled])\n",
    "\n",
    "            print(f\"Distribuzione etichette prima del campionamento nel chunk {idx + 1}: {Counter(chunk_labels)}\")\n",
    "            print(f\"Distribuzione etichette dopo campionamento nel chunk {idx + 1}: {Counter(y_sampled)}\")\n",
    "            \n",
    "            del full_data, full_labels\n",
    "            \n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "\n",
    "        print(f\"Distribuzione totale etichette dopo stratificazione: {Counter(full_labels_sampled_stratified)}\")\n",
    "\n",
    "        \n",
    "        print(f\"Shape of sampled data after chunking and sampling: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels after chunking and sampling: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "        \n",
    "        # Crea la cartella processed_data se non esiste\n",
    "        os.makedirs('processed_data/multiclass', exist_ok=True)\n",
    "\n",
    "        # Salva fit_data in formato parquet ottimizzato\n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'processed_data/multiclass/fit_data_multiclass_raw_counts_only.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Salva labels in formato parquet ottimizzato\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'processed_data/multiclass/labels_multiclass_raw_counts_only.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        del fit_data_df, labels_df, table_fit, table_labels\n",
    "        # Remove all chunk parquet files if flag is set\n",
    "        if remove_chunks:\n",
    "            try:\n",
    "                chunk_dir = 'processed_data/multiclass'\n",
    "                for fname in os.listdir(chunk_dir):\n",
    "                    if fname.startswith('fit_data_chunk_') or fname.startswith('labels_chunk_'):\n",
    "                        os.remove(os.path.join(chunk_dir, fname))\n",
    "                print(\"All chunk files removed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing chunk files: {e}\")\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_ddm_features(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"\n",
    "        Estrae features dettagliate da raw_counts DDM (shape: n_samples x 200)\n",
    "        \"\"\"\n",
    "\n",
    "        def gini(array):\n",
    "            \"\"\"Calcola il coefficiente di Gini (disuguaglianza)\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))\n",
    "        \n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float64) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. Statistiche base\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = gini(x)\n",
    "\n",
    "            # 2. Posizionali\n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentazione\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "            \n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivate e cambiamenti\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelazioni (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT (spettro frequenze)\n",
    "            spectrum = np.abs(fft(x))\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  # simmetrico\n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "            features.append(f)\n",
    "\n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def extract_quadrant_features(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Estrae features statistiche da ciascuno dei 4 quadranti e dal quadrante centrale della DDM (shape: n_samples x 200).\n",
    "        Ritorna un DataFrame con feature statistiche per ogni quadrante e per il centro.\n",
    "        \"\"\"\n",
    "        def stats(x, prefix):\n",
    "            return {\n",
    "                f'{prefix}_mean': np.mean(x),\n",
    "                f'{prefix}_std': np.std(x),\n",
    "                f'{prefix}_min': np.min(x),\n",
    "                f'{prefix}_max': np.max(x),\n",
    "                f'{prefix}_median': np.median(x),\n",
    "                f'{prefix}_range': np.max(x) - np.min(x),\n",
    "                f'{prefix}_skew': skew(x),\n",
    "                f'{prefix}_kurtosis': kurtosis(x),\n",
    "                f'{prefix}_entropy': entropy(x + 1e-10),\n",
    "                f'{prefix}_gini': self.gini(x),\n",
    "            }\n",
    "\n",
    "        features = []\n",
    "        for row in tqdm(fit_data, desc=\"Extracting quadrant features\"):\n",
    "            row = np.array(row, dtype=np.float64)\n",
    "            ddm = row.reshape(10, 20)  # 10x20\n",
    "\n",
    "            # Quadranti\n",
    "            q1 = ddm[:5, :10].ravel()\n",
    "            q2 = ddm[:5, 10:].ravel()\n",
    "            q3 = ddm[5:, :10].ravel()\n",
    "            q4 = ddm[5:, 10:].ravel()\n",
    "            # Quadrante centrale (4x8 centrale)\n",
    "            center = ddm[3:7, 6:14].ravel()\n",
    "\n",
    "            f = {}\n",
    "            f.update(stats(q1, 'q1'))\n",
    "            f.update(stats(q2, 'q2'))\n",
    "            f.update(stats(q3, 'q3'))\n",
    "            f.update(stats(q4, 'q4'))\n",
    "            f.update(stats(center, 'center'))\n",
    "\n",
    "            features.append(f)\n",
    "        return pd.DataFrame(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5246e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "class ModelTrainer:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.final_model = None\n",
    "\n",
    "    def train(self, model_search=True):\n",
    "        os.environ[\"_CUSTOM_LOGGING_LEVEL\"] = \"CRITICAL\"\n",
    "        if model_search:\n",
    "            scaler = MinMaxScaler()\n",
    "            fit_data_scaled = scaler.fit_transform(self.data)\n",
    "            clf = setup(data=fit_data_scaled,\n",
    "                        target=self.labels,\n",
    "                        pca=True,\n",
    "                        pca_method='linear',\n",
    "                        use_gpu=True,\n",
    "                        feature_selection=True,\n",
    "                        n_features_to_select=0.8,\n",
    "                        session_id=42\n",
    "                        )\n",
    "            best_models = compare_models(n_select=5)\n",
    "            best_model = best_models[0]\n",
    "            print(f\"Il modello migliore è: {best_model}\")\n",
    "            tuned_model = tune_model(best_model,\n",
    "                                    optimize='Accuracy',\n",
    "                                    n_iter=10,\n",
    "                                    search_library='optuna',\n",
    "                                    search_algorithm='tpe',\n",
    "                                    choose_better=True)\n",
    "            \n",
    "            print(\"Valutazione del modello ottimizzato:\")\n",
    "            \n",
    "            try:\n",
    "                dashboard(tuned_model)\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante la creazione del dashboard: {e}\")\n",
    "\n",
    "            try:\n",
    "                evaluate_model(tuned_model, verbose=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante la valutazione del modello: {e}\")\n",
    "            \n",
    "            best_params = tuned_model.get_params()\n",
    "\n",
    "            try:\n",
    "                plot_model(tuned_model, plot='confusion_matrix', save=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante la creazione della matrice di confusione: {e}\")\n",
    "\n",
    "            print(\"Valutazione del modello ottimizzato:\")\n",
    "            evaluate_model(tuned_model)\n",
    "            best_params = tuned_model.get_params()\n",
    "            print(\"Migliori iperparametri trovati:\")\n",
    "            for param, value in best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "            self.final_model = finalize_model(tuned_model)\n",
    "            save_model(self.final_model, 'best_multiclass_classification_model')\n",
    "            # loaded_model = load_model('best_classification_model')\n",
    "            try:\n",
    "                save_experiment('multiclass_classification_experiment')\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante il salvataggio dell'esperimento: {e}\")\n",
    "\n",
    "    def plot_feature_importance(self, feature_names=None, top_n=20):\n",
    "        \"\"\"\n",
    "        Plotta la feature importance del modello finale se disponibile.\n",
    "        Args:\n",
    "            feature_names (list or None): nomi delle feature, se disponibili.\n",
    "            top_n (int): mostra solo le top_n feature.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        importances = None\n",
    "\n",
    "        if self.final_model is None:\n",
    "            print(\"Nessun modello finale disponibile.\")\n",
    "            return\n",
    "\n",
    "        # Tree-based models\n",
    "        if hasattr(self.final_model, \"feature_importances_\"):\n",
    "            importances = self.final_model.feature_importances_\n",
    "        # Linear models\n",
    "        elif hasattr(self.final_model, \"coef_\"):\n",
    "            importances = self.final_model.coef_.ravel()\n",
    "        else:\n",
    "            print(\"Il modello non supporta la feature importance.\")\n",
    "            return\n",
    "\n",
    "        if feature_names is None:\n",
    "            feature_names = [f\"f{i}\" for i in range(len(importances))]\n",
    "\n",
    "        # Ordina per importanza\n",
    "        indices = importances.argsort()[::-1][:top_n]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(top_n), importances[indices][::-1], align=\"center\")\n",
    "        plt.yticks(range(top_n), [feature_names[i] for i in indices][::-1])\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.title(\"Top Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd89985",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_backup = False  # Imposta a True per leggere i dati da file parquet pre-processati \n",
    "if read_from_backup:\n",
    "    \n",
    "    fit_data_pl = pd.read_parquet('processed_data/multiclass/fit_data_multiclass_raw_counts_only.parquet')\n",
    "    labels_pl = pd.read_parquet('processed_data/multiclass/labels_multiclass_raw_counts_only.parquet')\n",
    "\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()\n",
    "else:\n",
    "    preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR)\n",
    "    fit_data, labels = preprocessor.process_all_files(chunk_size=250, sample_fraction=0.1, remove_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94934c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR)\n",
    "features_extractor = DDMFeatureExtractor()\n",
    "ddm_features = features_extractor.extract_ddm_features(fit_data)\n",
    "quadrant_features = features_extractor.extract_quadrant_features(fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2301173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatena ddm_features e quadrant_features lungo le colonne\n",
    "combined_features = pd.concat([ddm_features, quadrant_features], axis=1)\n",
    "combined_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm_features_colnames = list(combined_features.columns)\n",
    "FEATURES = ddm_features_colnames\n",
    "\n",
    "fit_data_with_features = combined_features\n",
    "fit_data_with_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66880f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlla infiniti o valori troppo grandi per float64\n",
    "mask_finite = np.isfinite(fit_data_with_features).all(axis=1) & (np.abs(fit_data_with_features) < np.finfo(np.float64).max).all(axis=1)\n",
    "\n",
    "fit_data_with_features_clean = fit_data_with_features[mask_finite]\n",
    "\n",
    "labels_clean = labels[mask_finite]\n",
    "\n",
    "\n",
    "fit_data_with_features_clean.shape, labels_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcad97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('processed_data/binary_classification/data_w_features', exist_ok=True)\n",
    "fit_data_with_features_clean.to_parquet('processed_data/binary_classification/data_w_features/fit_data_multiclass_stat_features_only.parquet', index=False)\n",
    "labels_clean.to_parquet('processed_data/binary_classification/data_w_features/labels_multiclass_stat_features_only.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi i file parquet con polars\n",
    "fit_data_with_features_clean = pd.read_parquet(\"C:/Users/Alessandro/Desktop/great/rongowai/processed_data/multiclass/fit_data_multiclass_stat_features_only.parquet\")\n",
    "labels_clean = pd.read_parquet(\"C:/Users/Alessandro/Desktop/great/rongowai/processed_data/multiclass/labels_multiclass.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fit_data_with_features_clean), len(labels_clean['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e681a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(data=fit_data_with_features_clean.values, labels=labels_clean)\n",
    "model_trainer.train(model_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b6173",
   "metadata": {},
   "source": [
    "TARGET MAPPING = -1.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carica il modello finale addestrato da pycaret\n",
    "final_model = load_model('best_binary_classification_model')\n",
    "\n",
    "# Ottieni la feature importance se disponibile\n",
    "\n",
    "if hasattr(final_model, \"feature_importances_\"):\n",
    "    importances = final_model.feature_importances_\n",
    "elif hasattr(final_model, \"coef_\"):\n",
    "    importances = final_model.coef_.ravel()\n",
    "else:\n",
    "    print(\"Il modello non supporta la feature importance.\")\n",
    "    importances = None\n",
    "\n",
    "if importances is not None:\n",
    "    # Usa i nomi delle colonne di fit_data_with_features se disponibili\n",
    "    feature_names = [str(i) for i in range(fit_data_with_features.shape[1])]\n",
    "    top_n = 20\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(top_n), importances[indices][::-1], align=\"center\")\n",
    "    plt.yticks(range(top_n), [feature_names[i] for i in indices][::-1])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
