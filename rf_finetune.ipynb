{
 "cells": [
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 8,
=======
   "cell_type": "markdown",
>>>>>>> 0264ece9685b37360329b0c8876cb21fdcc3c8b7
   "id": "e257e919",
   "metadata": {},
   "source": [
    "### Routine per il fine tuning del modello "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": null,
>>>>>>> 0264ece9685b37360329b0c8876cb21fdcc3c8b7
   "id": "3b7c2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV,\n",
    "    \n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> 0264ece9685b37360329b0c8876cb21fdcc3c8b7
   "id": "5a0cee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi i dati e le etichette dai file Parquet nella cartella preprocessed/binary/data_w_features\n",
    "features_df = pd.read_parquet('processed_data/binary_classification/data_w_features/combined_features.parquet')\n",
    "labels_df = pd.read_parquet('processed_data/binary_classification/data_w_features/labels_binary_stats_features_only.parquet')\n",
    "\n",
    "# Campiona 10000 righe con distribuzione bilanciata tra le classi di labels_df\n",
    "n_samples_per_class = 100000  # 10000/2 per due classi\n",
    "sampled_indices = (\n",
    "    labels_df.groupby(labels_df.iloc[:, 0])\n",
    "    .apply(lambda x: x.sample(n=n_samples_per_class, random_state=42))\n",
    "    .index.get_level_values(1)\n",
    ")\n",
    "features_df = features_df.loc[sampled_indices].reset_index(drop=True)\n",
    "labels_df = labels_df.loc[sampled_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 4,
>>>>>>> 0264ece9685b37360329b0c8876cb21fdcc3c8b7
   "id": "3ade8861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520000, 520000)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 11,
=======
     "execution_count": 4,
>>>>>>> 0264ece9685b37360329b0c8876cb21fdcc3c8b7
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_df), len(labels_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 5,
>>>>>>> 0264ece9685b37360329b0c8876cb21fdcc3c8b7
   "id": "e3bd3bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 260000, 1: 260000})"
      ]
     },
<<<<<<< HEAD
     "execution_count": 12,
=======
     "execution_count": 5,
>>>>>>> 0264ece9685b37360329b0c8876cb21fdcc3c8b7
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels_df.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestTuner:\n",
    "    def __init__(self, features_df, labels_df, target_column='O', test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Inizializza il tuner per Random Forest\n",
    "        \n",
    "        Args:\n",
    "            features_df: DataFrame con le features\n",
    "            labels_df: DataFrame con le labels\n",
    "            target_column: nome della colonna target\n",
    "            test_size: proporzione del test set\n",
    "            random_state: seed per riproducibilitÃ \n",
    "        \"\"\"\n",
    "        self.features_df = features_df\n",
    "        self.labels_df = labels_df\n",
    "        self.target_column = target_column\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Inizializza attributi\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.cv_results = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepara i dati per il training\"\"\"\n",
    "        print(\"Preparazione dei dati...\")\n",
    "        \n",
    "        # Estrai features e target\n",
    "        X = self.features_df.copy()\n",
    "        y = self.labels_df[self.target_column].copy()\n",
    "        \n",
    "        # Gestisci valori mancanti\n",
    "        X = X.fillna(X.median() if X.select_dtypes(include=[np.number]).shape[1] > 0 else X.mode().iloc[0])\n",
    "        \n",
    "        # Encoding delle variabili categoriche se presenti\n",
    "        categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "        if len(categorical_columns) > 0:\n",
    "            print(f\"Encoding di {len(categorical_columns)} variabili categoriche\")\n",
    "            for col in categorical_columns:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "        \n",
    "        # Split train/test\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, \n",
    "            stratify=y if len(np.unique(y)) > 1 else None\n",
    "        )\n",
    "        \n",
    "        print(f\"Dimensioni training set: {self.X_train.shape}\")\n",
    "        print(f\"Dimensioni test set: {self.X_test.shape}\")\n",
    "        print(\"Distribuzione classi nel training set:\")\n",
    "        print(self.y_train.value_counts().sort_index())\n",
    "        \n",
    "    def define_param_grids(self):\n",
    "        \"\"\"Definisce le griglie di parametri per la ricerca\"\"\"\n",
    "        \n",
    "        # Griglia completa per GridSearch (piÃ¹ lenta ma esaustiva)\n",
    "        self.grid_params = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "        \n",
    "        # Griglia per RandomizedSearch (piÃ¹ veloce)\n",
    "        self.random_params = {\n",
    "            'n_estimators': stats.randint(50, 500),\n",
    "            'max_depth': [None] + list(range(10, 50, 5)),\n",
    "            'min_samples_split': stats.randint(2, 20),\n",
    "            'min_samples_leaf': stats.randint(1, 10),\n",
    "            'max_features': ['sqrt', 'log2', None, 0.5, 0.7, 0.9],\n",
    "            'bootstrap': [True, False],\n",
    "            'max_samples': [None, 0.7, 0.8, 0.9] if True else [None]  # Solo se bootstrap=True\n",
    "        }\n",
    "        \n",
    "    def grid_search_tuning(self, cv_folds=5, scoring='accuracy', n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Esegue Grid Search per trovare i parametri ottimali\n",
    "        \n",
    "        Args:\n",
    "            cv_folds: numero di fold per cross-validation\n",
    "            scoring: metrica di scoring\n",
    "            n_jobs: numero di processi paralleli\n",
    "        \"\"\"\n",
    "        print(\"\\n=== GRID SEARCH TUNING ===\")\n",
    "        print(\"Avvio Grid Search (puÃ² richiedere diversi minuti)...\")\n",
    "        \n",
    "        # Definisci il modello base\n",
    "        rf = RandomForestClassifier(random_state=self.random_state)\n",
    "        \n",
    "        # Setup cross-validation\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # Grid Search\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=rf,\n",
    "            param_grid=self.grid_params,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Fit del modello\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Salva i risultati\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "        \n",
    "        print(\"\\nMigliori parametri trovati:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nMiglior score CV: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        return grid_search\n",
    "    \n",
    "    def randomized_search_tuning(self, n_iter=100, cv_folds=5, scoring='accuracy', n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Esegue Randomized Search per trovare i parametri ottimali\n",
    "        \n",
    "        Args:\n",
    "            n_iter: numero di combinazioni di parametri da testare\n",
    "            cv_folds: numero di fold per cross-validation\n",
    "            scoring: metrica di scoring\n",
    "            n_jobs: numero di processi paralleli\n",
    "        \"\"\"\n",
    "        print(\"\\n=== RANDOMIZED SEARCH TUNING ===\")\n",
    "        print(f\"Avvio Randomized Search con {n_iter} iterazioni...\")\n",
    "        \n",
    "        # Definisci il modello base\n",
    "        rf = RandomForestClassifier(random_state=self.random_state)\n",
    "        \n",
    "        # Setup cross-validation\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # Randomized Search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=rf,\n",
    "            param_distributions=self.random_params,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=1,\n",
    "            random_state=self.random_state,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Fit del modello\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Salva i risultati\n",
    "        self.best_model = random_search.best_estimator_\n",
    "        self.best_params = random_search.best_params_\n",
    "        self.cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "        \n",
    "        print(\"\\nMigliori parametri trovati:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        print(f\"\\nMiglior score CV: {random_search.best_score_:.4f}\")\n",
    "        \n",
    "        return random_search\n",
    "    \n",
    "    def evaluate_model(self, plot_results=True):\n",
    "        \"\"\"Valuta il modello ottimizzato\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"Errore: Nessun modello Ã¨ stato addestrato ancora!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n=== VALUTAZIONE DEL MODELLO ===\")\n",
    "        \n",
    "        # Predizioni\n",
    "        y_pred = self.best_model.predict(self.X_test)\n",
    "        y_pred_proba = self.best_model.predict_proba(self.X_test)\n",
    "        \n",
    "        # Metriche di base\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(self.y_test, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        # ROC AUC per problemi binari o multiclasse\n",
    "        try:\n",
    "            if len(np.unique(self.y_test)) == 2:\n",
    "                auc = roc_auc_score(self.y_test, y_pred_proba[:, 1])\n",
    "                print(f\"ROC AUC: {auc:.4f}\")\n",
    "            else:\n",
    "                auc = roc_auc_score(self.y_test, y_pred_proba, multi_class='ovr')\n",
    "                print(f\"ROC AUC (multiclass): {auc:.4f}\")\n",
    "        except ValueError:\n",
    "            print(\"ROC AUC non calcolabile\")\n",
    "        \n",
    "        # Classification report dettagliato\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred))\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(self.best_model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': self.X_train.columns,\n",
    "                'importance': self.best_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(\"\\nTop 10 Feature piÃ¹ importanti:\")\n",
    "            print(feature_importance.head(10))\n",
    "        \n",
    "        # Plot dei risultati\n",
    "        if plot_results:\n",
    "            self.plot_results(y_pred, feature_importance if 'feature_importance' in locals() else None)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "    \n",
    "    def plot_results(self, y_pred, feature_importance=None):\n",
    "        \"\"\"Crea visualizzazioni dei risultati\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Confusion Matrix\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Confusion Matrix')\n",
    "        axes[0,0].set_xlabel('Predicted')\n",
    "        axes[0,0].set_ylabel('Actual')\n",
    "        \n",
    "        # 2. Feature Importance\n",
    "        if feature_importance is not None:\n",
    "            top_features = feature_importance.head(15)\n",
    "            axes[0,1].barh(range(len(top_features)), top_features['importance'])\n",
    "            axes[0,1].set_yticks(range(len(top_features)))\n",
    "            axes[0,1].set_yticklabels(top_features['feature'])\n",
    "            axes[0,1].set_title('Top 15 Feature Importance')\n",
    "            axes[0,1].set_xlabel('Importance')\n",
    "        \n",
    "        # 3. Cross-validation scores\n",
    "        if self.cv_results is not None:\n",
    "            cv_scores = self.cv_results['mean_test_score']\n",
    "            axes[1,0].hist(cv_scores, bins=30, alpha=0.7, color='skyblue')\n",
    "            axes[1,0].axvline(cv_scores.max(), color='red', linestyle='--', \n",
    "                             label=f'Best: {cv_scores.max():.4f}')\n",
    "            axes[1,0].set_title('Distribuzione CV Scores')\n",
    "            axes[1,0].set_xlabel('CV Score')\n",
    "            axes[1,0].set_ylabel('Frequency')\n",
    "            axes[1,0].legend()\n",
    "        \n",
    "        # 4. Distribuzione classi predette vs reali\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Actual': self.y_test,\n",
    "            'Predicted': y_pred\n",
    "        })\n",
    "        \n",
    "        actual_counts = comparison_df['Actual'].value_counts().sort_index()\n",
    "        pred_counts = comparison_df['Predicted'].value_counts().sort_index()\n",
    "        \n",
    "        x_pos = np.arange(len(actual_counts))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[1,1].bar(x_pos - width/2, actual_counts.values, width, \n",
    "                     label='Actual', alpha=0.8)\n",
    "        axes[1,1].bar(x_pos + width/2, pred_counts.values, width, \n",
    "                     label='Predicted', alpha=0.8)\n",
    "        axes[1,1].set_title('Distribuzione Classi: Actual vs Predicted')\n",
    "        axes[1,1].set_xlabel('Classe')\n",
    "        axes[1,1].set_ylabel('Count')\n",
    "        axes[1,1].set_xticks(x_pos)\n",
    "        axes[1,1].set_xticklabels(actual_counts.index)\n",
    "        axes[1,1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def run_complete_pipeline(self, search_type='randomized', **kwargs):\n",
    "        \"\"\"\n",
    "        Esegue la pipeline completa di tuning\n",
    "        \n",
    "        Args:\n",
    "            search_type: 'grid' o 'randomized'\n",
    "            **kwargs: parametri aggiuntivi per la ricerca\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ AVVIO PIPELINE COMPLETA DI TUNING\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # 1. Preparazione dati\n",
    "        self.prepare_data()\n",
    "        \n",
    "        # 2. Definizione parametri\n",
    "        self.define_param_grids()\n",
    "        \n",
    "        # 3. Ricerca parametri\n",
    "        if search_type.lower() == 'grid':\n",
    "            search_results = self.grid_search_tuning(**kwargs)\n",
    "        else:\n",
    "            search_results = self.randomized_search_tuning(**kwargs)\n",
    "        \n",
    "        # 4. Valutazione finale\n",
    "        evaluation_results = self.evaluate_model()\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ PIPELINE COMPLETATA!\")\n",
    "        return search_results, evaluation_results\n",
    "\n",
    "# ================================\n",
    "# ESEMPIO DI UTILIZZO\n",
    "# ================================\n",
    "\n",
    "def main_example():\n",
    "    \"\"\"Esempio di utilizzo della pipeline\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    # Inizializza il tuner\n",
    "    tuner = RandomForestTuner(\n",
    "        features_df=features_df, \n",
    "        labels_df=labels_df, \n",
    "        target_column='0',\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Opzione 1: Pipeline completa con Randomized Search (raccomandato)\n",
    "    search_results, eval_results = tuner.run_complete_pipeline(\n",
    "        search_type='randomized',\n",
    "        n_iter=50,  # Riduci per test piÃ¹ veloci\n",
    "        cv_folds=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Opzione 2: Pipeline completa con Grid Search (piÃ¹ lenta)\n",
<<<<<<< HEAD
    "    search_results, eval_results = tuner.run_complete_pipeline(\n",
    "         search_type='grid',\n",
    "         cv_folds=5,  # Riduci per velocizzare\n",
    "         scoring='accuracy',\n",
    "        \n",
    "         n_jobs=-1\n",
    "     )\n",
=======
    "    # search_results, eval_results = tuner.run_complete_pipeline(\n",
    "    #     search_type='grid',\n",
    "    #     cv_folds=3,  # Riduci per velocizzare\n",
    "    #     scoring='f1_weighted',\n",
    "    #     n_jobs=-\n",
    "    # )\n",
>>>>>>> 0264ece9685b37360329b0c8876cb21fdcc3c8b7
    "    \n",
    "    # Accesso ai risultati\n",
    "    print(f\"\\nMiglior modello: {tuner.best_model}\")\n",
    "    print(f\"Migliori parametri: {tuner.best_params}\")\n",
    "    \n",
    "    return tuner, search_results, eval_results\n",
    "\n",
    "# Per eseguire la pipeline, decommentare:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "106ea8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ AVVIO PIPELINE COMPLETA DI TUNING\n",
      "==================================================\n",
      "Preparazione dei dati...\n",
      "Dimensioni training set: (416000, 134)\n",
      "Dimensioni test set: (104000, 134)\n",
      "Distribuzione classi nel training set:\n",
      "0\n",
      "0    208000\n",
      "1    208000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== RANDOMIZED SEARCH TUNING ===\n",
      "Avvio Randomized Search con 50 iterazioni...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[43mmain_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 339\u001b[0m, in \u001b[0;36mmain_example\u001b[1;34m()\u001b[0m\n\u001b[0;32m    330\u001b[0m tuner \u001b[38;5;241m=\u001b[39m RandomForestTuner(\n\u001b[0;32m    331\u001b[0m     features_df\u001b[38;5;241m=\u001b[39mfeatures_df, \n\u001b[0;32m    332\u001b[0m     labels_df\u001b[38;5;241m=\u001b[39mlabels_df, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    335\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m    336\u001b[0m )\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Opzione 1: Pipeline completa con Randomized Search (raccomandato)\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m search_results, eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandomized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Riduci per test piÃ¹ veloci\u001b[39;49;00m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Opzione 2: Pipeline completa con Grid Search (piÃ¹ lenta)\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# search_results, eval_results = tuner.run_complete_pipeline(\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m#     search_type='grid',\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Accesso ai risultati\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMiglior modello: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtuner\u001b[38;5;241m.\u001b[39mbest_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 310\u001b[0m, in \u001b[0;36mRandomForestTuner.run_complete_pipeline\u001b[1;34m(self, search_type, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_search_tuning(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandomized_search_tuning(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# 4. Valutazione finale\u001b[39;00m\n\u001b[0;32m    313\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_model()\n",
      "Cell \u001b[1;32mIn[13], line 159\u001b[0m, in \u001b[0;36mRandomForestTuner.randomized_search_tuning\u001b[1;34m(self, n_iter, cv_folds, scoring, n_jobs)\u001b[0m\n\u001b[0;32m    146\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m    147\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrf,\n\u001b[0;32m    148\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Fit del modello\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Salva i risultati\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\alessandro.togni\\AppData\\Local\\anaconda3\\envs\\great_clf\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alessandro.togni\\AppData\\Local\\anaconda3\\envs\\great_clf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alessandro.togni\\AppData\\Local\\anaconda3\\envs\\great_clf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alessandro.togni\\AppData\\Local\\anaconda3\\envs\\great_clf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alessandro.togni\\AppData\\Local\\anaconda3\\envs\\great_clf\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alessandro.togni\\AppData\\Local\\anaconda3\\envs\\great_clf\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alessandro.togni\\AppData\\Local\\anaconda3\\envs\\great_clf\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alessandro.togni\\AppData\\Local\\anaconda3\\envs\\great_clf\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner = main_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2da22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
