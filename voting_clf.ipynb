{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a002bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, Union, List, Dict\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           roc_auc_score, confusion_matrix, classification_report,\n",
    "                           roc_curve, precision_recall_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import joblib\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Configurazione per grafici migliori\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model_path = r\"./models/catboost_test120625_1946/catboost_trained_on_full_data_model_v1.cbm\"\n",
    "cztboost_scaler_path = \"./models/catboost_trained_on_full_data_model_v1_scaler.joblib\"\n",
    "xg_boost_model_path = \"./models/xgboost_trained_on_full_data_model_v1.json\"\n",
    "xg_boost_scaler_path = \"./models/xgboost_trained_on_full_data_model_v1_scaler.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_catboost_model(model_path=str):\n",
    "        \"\"\"Carica il modello CatBoost dal percorso specificato\"\"\"\n",
    "        try:\n",
    "            model = CatBoostClassifier()\n",
    "            model.load_model(str(model_path))\n",
    "            print(f\"Modello caricato con successo da: {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento del modello: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SampleConfig:\n",
    "    \"\"\"Configurazione per un singolo campionamento.\"\"\"\n",
    "    name: str\n",
    "    n_samples_per_class: int\n",
    "    random_state: int = 42\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetInfo:\n",
    "    \"\"\"Informazioni sul dataset.\"\"\"\n",
    "    n_samples: int\n",
    "    n_features: int\n",
    "    class_distribution: Dict[int, int]\n",
    "    features_columns: List[str]\n",
    "    labels_columns: List[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataLoader:\n",
    "    features_path: Union[str, Path]\n",
    "    labels_path: Union[str, Path]\n",
    "    _features_df_cache: Optional[pd.DataFrame] = field(default=None, init=False, repr=False)\n",
    "    _labels_df_cache: Optional[pd.DataFrame] = field(default=None, init=False, repr=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Converte i path in oggetti Path e verifica che esistano.\"\"\"\n",
    "        self.features_path = Path(self.features_path)\n",
    "        self.labels_path = Path(self.labels_path)\n",
    "        \n",
    "        if not self.features_path.exists():\n",
    "            raise FileNotFoundError(f\"Features file not found: {self.features_path}\")\n",
    "        if not self.labels_path.exists():\n",
    "            raise FileNotFoundError(f\"Labels file not found: {self.labels_path}\")\n",
    "    \n",
    "    def load_full_data(self, use_cache: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Carica tutti i dati senza campionamento.\n",
    "        \n",
    "        Args:\n",
    "            use_cache: Se True, usa i dati in cache se disponibili\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: features_df, labels_df\n",
    "        \"\"\"\n",
    "        if use_cache and self._features_df_cache is not None and self._labels_df_cache is not None:\n",
    "            return self._features_df_cache, self._labels_df_cache\n",
    "        \n",
    "        features_df = pd.read_parquet(self.features_path)\n",
    "        labels_df = pd.read_parquet(self.labels_path)\n",
    "        \n",
    "        # Verifica che abbiano lo stesso numero di righe\n",
    "        if len(features_df) != len(labels_df):\n",
    "            raise ValueError(f\"Features and labels have different lengths: \"\n",
    "                           f\"{len(features_df)} vs {len(labels_df)}\")\n",
    "        \n",
    "        if use_cache:\n",
    "            self._features_df_cache = features_df\n",
    "            self._labels_df_cache = labels_df\n",
    "        \n",
    "        return features_df, labels_df\n",
    "    \n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"Pulisce la cache dei dati.\"\"\"\n",
    "        self._features_df_cache = None\n",
    "        self._labels_df_cache = None\n",
    "    \n",
    "    def load_balanced_sample(self, n_samples_per_class: int, \n",
    "                           random_state: int = 42,\n",
    "                           use_cache: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Carica un campione bilanciato dei dati.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: features_df campionate, labels_df campionate\n",
    "        \"\"\"\n",
    "        # Carica i dati completi\n",
    "        features_df_full, labels_df_full = self.load_full_data(use_cache=use_cache)\n",
    "        \n",
    "        # Ottieni gli indici campionati in modo bilanciato\n",
    "        sampled_indices = self._get_balanced_indices(\n",
    "            labels_df_full, \n",
    "            n_samples_per_class, \n",
    "            random_state\n",
    "        )\n",
    "        \n",
    "        # Campiona i dati\n",
    "        features_df = features_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = labels_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        \n",
    "        return features_df, labels_df\n",
    "    \n",
    "    def load_balanced_sample_memory_efficient(self, n_samples_per_class: int, \n",
    "                                            random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Versione memory-efficient che libera la memoria dopo il campionamento.\n",
    "        \n",
    "        Args:\n",
    "            n_samples_per_class: Numero di campioni per ogni classe\n",
    "            random_state: Seed per la riproducibilità\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: features_df campionate, labels_df campionate\n",
    "        \"\"\"\n",
    "        # Carica i dati completi (senza cache)\n",
    "        features_df_full = pd.read_parquet(self.features_path)\n",
    "        labels_df_full = pd.read_parquet(self.labels_path)\n",
    "        \n",
    "        # Ottieni gli indici campionati\n",
    "        sampled_indices = self._get_balanced_indices(\n",
    "            labels_df_full, \n",
    "            n_samples_per_class, \n",
    "            random_state\n",
    "        )\n",
    "        \n",
    "        # Campiona i dati\n",
    "        features_df = features_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        labels_df = labels_df_full.loc[sampled_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Libera la memoria\n",
    "        del features_df_full, labels_df_full, sampled_indices\n",
    "        \n",
    "        return features_df, labels_df\n",
    "    \n",
    "\n",
    "    \n",
    "    def _get_balanced_indices(self, labels_df: pd.DataFrame, \n",
    "                            n_samples_per_class: int, \n",
    "                            random_state: int) -> pd.Index:\n",
    "        \"\"\"\n",
    "        Ottiene gli indici per un campionamento bilanciato.\n",
    "        \n",
    "        Args:\n",
    "            labels_df: DataFrame delle labels\n",
    "            n_samples_per_class: Numero di campioni per classe\n",
    "            random_state: Seed per la riproducibilità\n",
    "            \n",
    "        Returns:\n",
    "            pd.Index: Indici campionati\n",
    "        \"\"\"\n",
    "        # Usa la prima colonna per il groupby (assumendo sia la colonna delle classi)\n",
    "        label_column = labels_df.iloc[:, 0]\n",
    "        \n",
    "        # Verifica che ci siano abbastanza campioni per ogni classe\n",
    "        class_counts = label_column.value_counts()\n",
    "        for class_label, count in class_counts.items():\n",
    "            if count < n_samples_per_class:\n",
    "                raise ValueError(f\"Class {class_label} has only {count} samples, \"\n",
    "                               f\"but {n_samples_per_class} requested\")\n",
    "        \n",
    "        # Campiona gli indici\n",
    "        sampled_indices = (\n",
    "            labels_df.groupby(label_column)\n",
    "            .apply(lambda x: x.sample(n=n_samples_per_class, random_state=random_state))\n",
    "            .index.get_level_values(1)\n",
    "        )\n",
    "        \n",
    "        return sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6402758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico un dataset più piccolo a classi bilanciate per la ricerca degli iperparametri \n",
    "# e uno più esteso per ri-trainare il modello con i parametri ottimali trovati\n",
    "\n",
    "loader = DataLoader(\n",
    "    features_path='processed_data/binary_classification/data_w_features/combined_features.parquet',\n",
    "    labels_path='processed_data/binary_classification/data_w_features/labels_binary_stats_features_only.parquet'\n",
    ")\n",
    "\n",
    "# Esempio 1: Carica 40k campioni bilanciati (20k per classe) per training\n",
    "features_df, labels_df = loader.load_balanced_sample_memory_efficient(n_samples_per_class=20000)\n",
    "print(f\"Training set shape: features={features_df.shape}, labels={labels_df.shape}\")\n",
    "\n",
    "# Esempio 2: Carica 500k campioni bilanciati (250k per classe) per fit finale\n",
    "features_df_to_finalize, labels_df_to_finalize = loader.load_balanced_sample_memory_efficient(\n",
    "    n_samples_per_class=250000\n",
    ")\n",
    "print(f\"Final set shape: features={features_df_to_finalize.shape}, labels={labels_df_to_finalize.shape}\")\n",
    "print(f\"Class distribution in dev set set: {labels_df.iloc[:, 0].value_counts()}\")\n",
    "print(f\"Class distribution in final set: {labels_df_to_finalize.iloc[:, 0].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. CARICAMENTO MODELLI E SCALER\n",
    "print(\"Caricamento modelli e scaler...\")\n",
    "\n",
    "# Carica CatBoost (assumendo che la funzione load_catboost_model sia definita)\n",
    "catboost_model_final = _load_catboost_model(catboost_model_path)\n",
    "\n",
    "# Carica gli scaler (assumendo che i path siano definiti)\n",
    "try:\n",
    "    catboost_scaler = joblib.load(cztboost_scaler_path)\n",
    "    print(\"✓ CatBoost scaler caricato\")\n",
    "except:\n",
    "    print(\"⚠ CatBoost scaler non trovato\")\n",
    "    catboost_scaler = StandardScaler()\n",
    "\n",
    "# Carica XGBoost (se il modello è disponibile)\n",
    "xg_boost_model_final = None\n",
    "try:\n",
    "    xg_boost_model_final = joblib.load(xg_boost_model_path) \n",
    "    print(\"✓ XGBoost modello caricato\")\n",
    "except:\n",
    "    print(\"⚠ XGBoost modello non trovato\")\n",
    "    xg_boost_model_final = None\n",
    "\n",
    "# Se XGBoost dovesse essere riattivato in futuro\n",
    "xgboost_scaler = None\n",
    "if xg_boost_model_final is not None:\n",
    "    try:\n",
    "        xgboost_scaler = joblib.load(xg_boost_scaler_path)\n",
    "        print(\"✓ XGBoost scaler caricato\")\n",
    "    except:\n",
    "        print(\"⚠ XGBoost scaler non trovato\")\n",
    "        xgboost_scaler = StandardScaler()\n",
    "# Funzione per verificare se i modelli sono stati caricati correttamente\n",
    "def check_models_loaded():  \n",
    "    if catboost_model_final is None:\n",
    "        print(\"⚠ CatBoost model non caricato correttamente.\")\n",
    "    else:\n",
    "        print(\"✓ CatBoost model caricato correttamente.\")\n",
    "\n",
    "    if xg_boost_model_final is None:\n",
    "        print(\"⚠ XGBoost model non caricato correttamente.\")\n",
    "    else:\n",
    "        print(\"✓ XGBoost model caricato correttamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf90436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363802a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba0224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. CUSTOM VOTING CLASSIFIER\n",
    "class MaxProbVotingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Voting Classifier che seleziona la classe con la probabilità più alta.\n",
    "    In caso di pareggio, privilegia CatBoost.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimators, scalers=None):\n",
    "        self.estimators = estimators\n",
    "        self.scalers = scalers if scalers else {}\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fit degli scaler se necessario\n",
    "        for name, scaler in self.scalers.items():\n",
    "            if scaler is not None and not hasattr(scaler, 'mean_'):\n",
    "                scaler.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Calcola le probabilità medie o massime per ogni classe\"\"\"\n",
    "        probas = []\n",
    "        \n",
    "        for name, model in self.estimators:\n",
    "            if model is None:\n",
    "                continue\n",
    "                \n",
    "            # Applica lo scaler se disponibile\n",
    "            X_scaled = X\n",
    "            if name in self.scalers and self.scalers[name] is not None:\n",
    "                X_scaled = self.scalers[name].transform(X)\n",
    "            \n",
    "            # Ottieni le probabilità\n",
    "            proba = model.predict_proba(X_scaled)\n",
    "            probas.append((name, proba))\n",
    "        \n",
    "        if not probas:\n",
    "            raise ValueError(\"Nessun modello disponibile per le predizioni\")\n",
    "        \n",
    "        # Per ogni sample, prendi la probabilità massima per la classe positiva\n",
    "        n_samples = X.shape[0]\n",
    "        final_probas = np.zeros((n_samples, 2))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            max_prob_class_1 = -1\n",
    "            best_model = None\n",
    "            \n",
    "            for name, proba in probas:\n",
    "                if proba[i, 1] > max_prob_class_1:\n",
    "                    max_prob_class_1 = proba[i, 1]\n",
    "                    best_model = name\n",
    "                    final_probas[i] = proba[i]\n",
    "                elif proba[i, 1] == max_prob_class_1 and name == 'catboost':\n",
    "                    # In caso di pareggio, privilegia CatBoost\n",
    "                    best_model = name\n",
    "                    final_probas[i] = proba[i]\n",
    "        \n",
    "        return final_probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predizione basata sulle probabilità\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "# 3. PREPARAZIONE DATI\n",
    "print(\"\\nPreparazione dati...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_df_to_finalize, \n",
    "    labels_df_to_finalize,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels_df_to_finalize\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Distribuzione classi train: {np.bincount(y_train)}\")\n",
    "print(f\"Distribuzione classi test: {np.bincount(y_test)}\")\n",
    "\n",
    "# 4. CREAZIONE VOTING CLASSIFIER\n",
    "estimators = []\n",
    "scalers = {}\n",
    "\n",
    "if catboost_model_final is not None:\n",
    "    estimators.append(('catboost', catboost_model_final))\n",
    "    scalers['catboost'] = catboost_scaler\n",
    "\n",
    "if xg_boost_model_final is not None:\n",
    "    estimators.append(('xgboost', xg_boost_model_final))\n",
    "    scalers['xgboost'] = xgboost_scaler\n",
    "\n",
    "voting_clf = MaxProbVotingClassifier(estimators=estimators, scalers=scalers)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. FUNZIONE PER TROVARE SOGLIA OTTIMALE\n",
    "def find_optimal_threshold(model, X, y, metric='recall', conservative=True):\n",
    "    \"\"\"\n",
    "    Trova la soglia ottimale per massimizzare una metrica.\n",
    "    Se conservative=True, in caso di pareggio prende la soglia più bassa (più conservativa).\n",
    "    \"\"\"\n",
    "    probas = model.predict_proba(X)[:, 1]\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    \n",
    "    scores = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (probas >= threshold).astype(int)\n",
    "        \n",
    "        if metric == 'recall':\n",
    "            score = recall_score(y, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y, y_pred)\n",
    "        elif metric == 'f1':\n",
    "            score = f1_score(y, y_pred)\n",
    "        else:\n",
    "            raise ValueError(f\"Metrica non supportata: {metric}\")\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    if conservative:\n",
    "        # Trova il primo threshold che massimizza la metrica\n",
    "        optimal_idx = np.where(scores == scores.max())[0][0]\n",
    "    else:\n",
    "        # Trova l'ultimo threshold che massimizza la metrica\n",
    "        optimal_idx = np.where(scores == scores.max())[0][-1]\n",
    "    \n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_score = scores[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold, optimal_score, thresholds, scores\n",
    "\n",
    "# 6. VALUTAZIONE MODELLI\n",
    "def evaluate_model(model, X_test, y_test, model_name, threshold=0.5):\n",
    "    \"\"\"Valuta un modello con metriche multiple\"\"\"\n",
    "    probas = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (probas >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, probas)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} - Soglia: {threshold:.3f}\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"AUC-ROC:   {metrics['auc']:.4f}\")\n",
    "    \n",
    "    return metrics, y_pred, probas\n",
    "\n",
    "# Valutazione con soglia standard (0.5)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALUTAZIONE MODELLI CON SOGLIA STANDARD (0.5)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_standard = {}\n",
    "\n",
    "# CatBoost\n",
    "if catboost_model_final is not None:\n",
    "    X_test_scaled_cb = catboost_scaler.transform(X_test) if catboost_scaler else X_test\n",
    "    metrics, y_pred, probas = evaluate_model(\n",
    "        catboost_model_final, X_test_scaled_cb, y_test, \"CatBoost\"\n",
    "    )\n",
    "    results_standard['CatBoost'] = {'metrics': metrics, 'y_pred': y_pred, 'probas': probas}\n",
    "\n",
    "# XGBoost (se disponibile)\n",
    "if xg_boost_model_final is not None:\n",
    "    X_test_scaled_xgb = xgboost_scaler.transform(X_test) if xgboost_scaler else X_test\n",
    "    metrics, y_pred, probas = evaluate_model(\n",
    "        xg_boost_model_final, X_test_scaled_xgb, y_test, \"XGBoost\"\n",
    "    )\n",
    "    results_standard['XGBoost'] = {'metrics': metrics, 'y_pred': y_pred, 'probas': probas}\n",
    "\n",
    "# Voting Classifier\n",
    "metrics, y_pred, probas = evaluate_model(voting_clf, X_test, y_test, \"Voting Classifier\")\n",
    "results_standard['Voting'] = {'metrics': metrics, 'y_pred': y_pred, 'probas': probas}\n",
    "\n",
    "# 7. OTTIMIZZAZIONE SOGLIA PER RECALL\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OTTIMIZZAZIONE SOGLIA PER MASSIMIZZARE RECALL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_optimized = {}\n",
    "\n",
    "# Trova soglie ottimali per ogni modello\n",
    "for model_name, model in [('CatBoost', catboost_model_final), \n",
    "                          ('XGBoost', xg_boost_model_final),\n",
    "                          ('Voting', voting_clf)]:\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    if model_name == 'CatBoost':\n",
    "        X_val = catboost_scaler.transform(X_train) if catboost_scaler else X_train\n",
    "        X_test_model = catboost_scaler.transform(X_test) if catboost_scaler else X_test\n",
    "    elif model_name == 'XGBoost':\n",
    "        X_val = xgboost_scaler.transform(X_train) if xgboost_scaler else X_train\n",
    "        X_test_model = xgboost_scaler.transform(X_test) if xgboost_scaler else X_test\n",
    "    else:  # Voting\n",
    "        X_val = X_train\n",
    "        X_test_model = X_test\n",
    "    \n",
    "    optimal_threshold, optimal_recall, thresholds, scores = find_optimal_threshold(\n",
    "        model, X_val, y_train, metric='recall', conservative=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Soglia ottimale: {optimal_threshold:.3f}\")\n",
    "    print(f\"Recall sul train: {optimal_recall:.4f}\")\n",
    "    \n",
    "    # Valuta con soglia ottimale\n",
    "    metrics, y_pred, probas = evaluate_model(\n",
    "        model, X_test_model, y_test, f\"{model_name} (Ottimizzato)\", optimal_threshold\n",
    "    )\n",
    "    \n",
    "    results_optimized[model_name] = {\n",
    "        'metrics': metrics,\n",
    "        'y_pred': y_pred,\n",
    "        'probas': probas,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'threshold_scores': (thresholds, scores)\n",
    "    }\n",
    "\n",
    "# 8. VISUALIZZAZIONI\n",
    "\n",
    "# 8.1 Confronto metriche\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Confronto Performance Modelli', fontsize=16)\n",
    "\n",
    "# Prepara dati per confronto\n",
    "models = list(results_standard.keys())\n",
    "metrics_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "for idx, metric in enumerate(metrics_names):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    \n",
    "    standard_scores = [results_standard[m]['metrics'][metric] for m in models]\n",
    "    optimized_scores = [results_optimized[m]['metrics'][metric] \n",
    "                       for m in models if m in results_optimized]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, standard_scores, width, label='Soglia 0.5', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, optimized_scores, width, label='Soglia Ottimizzata', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Modello')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_title(f'{metric.capitalize()} per Modello')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Aggiungi valori sulle barre\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.2 Curve di ottimizzazione soglia\n",
    "fig, axes = plt.subplots(1, len(results_optimized), figsize=(15, 5))\n",
    "if len(results_optimized) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_optimized.items()):\n",
    "    thresholds, scores = result['threshold_scores']\n",
    "    optimal_threshold = result['optimal_threshold']\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.plot(thresholds, scores, 'b-', linewidth=2)\n",
    "    ax.axvline(optimal_threshold, color='red', linestyle='--', \n",
    "               label=f'Soglia Ottimale: {optimal_threshold:.3f}')\n",
    "    ax.scatter([optimal_threshold], [scores[np.where(thresholds == optimal_threshold)[0][0]]], \n",
    "               color='red', s=100, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Soglia')\n",
    "    ax.set_ylabel('Recall')\n",
    "    ax.set_title(f'Ottimizzazione Soglia - {model_name}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.3 ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in results_standard.keys():\n",
    "    if model_name in results_optimized:\n",
    "        probas = results_optimized[model_name]['probas']\n",
    "    else:\n",
    "        probas = results_standard[model_name]['probas']\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "    auc = roc_auc_score(y_test, probas)\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Confronto Modelli')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 8.4 Precision-Recall Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in results_standard.keys():\n",
    "    if model_name in results_optimized:\n",
    "        probas = results_optimized[model_name]['probas']\n",
    "    else:\n",
    "        probas = results_standard[model_name]['probas']\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, probas)\n",
    "    \n",
    "    plt.plot(recall, precision, linewidth=2, label=model_name)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - Confronto Modelli')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 8.5 Confusion Matrices\n",
    "n_models = len(results_optimized)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_optimized.items()):\n",
    "    cm = confusion_matrix(y_test, result['y_pred'])\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=True)\n",
    "    ax.set_title(f'Confusion Matrix - {model_name}\\n(Soglia: {result[\"optimal_threshold\"]:.3f})')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8.6 Distribuzione delle probabilità\n",
    "fig, axes = plt.subplots(1, len(results_optimized), figsize=(15, 5))\n",
    "if len(results_optimized) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_optimized.items()):\n",
    "    probas = result['probas']\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Separa probabilità per classe\n",
    "    probas_class_0 = probas[y_test == 0]\n",
    "    probas_class_1 = probas[y_test == 1]\n",
    "    \n",
    "    # Istogrammi\n",
    "    ax.hist(probas_class_0, bins=30, alpha=0.5, label='Classe 0', color='blue', density=True)\n",
    "    ax.hist(probas_class_1, bins=30, alpha=0.5, label='Classe 1', color='red', density=True)\n",
    "    \n",
    "    # Soglia ottimale\n",
    "    ax.axvline(result['optimal_threshold'], color='green', linestyle='--', linewidth=2,\n",
    "               label=f'Soglia: {result[\"optimal_threshold\"]:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Probabilità Classe 1')\n",
    "    ax.set_ylabel('Densità')\n",
    "    ax.set_title(f'Distribuzione Probabilità - {model_name}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. REPORT FINALE\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REPORT FINALE - CONFRONTO MODELLI\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Tabella riassuntiva\n",
    "summary_data = []\n",
    "\n",
    "for model_name in results_standard.keys():\n",
    "    # Risultati con soglia standard\n",
    "    std_metrics = results_standard[model_name]['metrics']\n",
    "    row = {\n",
    "        'Modello': f\"{model_name} (Soglia 0.5)\",\n",
    "        'Accuracy': f\"{std_metrics['accuracy']:.4f}\",\n",
    "        'Precision': f\"{std_metrics['precision']:.4f}\",\n",
    "        'Recall': f\"{std_metrics['recall']:.4f}\",\n",
    "        'F1-Score': f\"{std_metrics['f1']:.4f}\",\n",
    "        'AUC-ROC': f\"{std_metrics['auc']:.4f}\"\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "    \n",
    "    # Risultati con soglia ottimizzata\n",
    "    if model_name in results_optimized:\n",
    "        opt_metrics = results_optimized[model_name]['metrics']\n",
    "        opt_threshold = results_optimized[model_name]['optimal_threshold']\n",
    "        row = {\n",
    "            'Modello': f\"{model_name} (Soglia {opt_threshold:.3f})\",\n",
    "            'Accuracy': f\"{opt_metrics['accuracy']:.4f}\",\n",
    "            'Precision': f\"{opt_metrics['precision']:.4f}\",\n",
    "            'Recall': f\"{opt_metrics['recall']:.4f}\",\n",
    "            'F1-Score': f\"{opt_metrics['f1']:.4f}\",\n",
    "            'AUC-ROC': f\"{opt_metrics['auc']:.4f}\"\n",
    "        }\n",
    "        summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\", summary_df.to_string(index=False))\n",
    "\n",
    "# Miglioramento del Recall\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MIGLIORAMENTO DEL RECALL CON OTTIMIZZAZIONE SOGLIA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name in results_standard.keys():\n",
    "    if model_name in results_optimized:\n",
    "        recall_before = results_standard[model_name]['metrics']['recall']\n",
    "        recall_after = results_optimized[model_name]['metrics']['recall']\n",
    "        improvement = (recall_after - recall_before) / recall_before * 100\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Recall prima: {recall_before:.4f}\")\n",
    "        print(f\"  Recall dopo:  {recall_after:.4f}\")\n",
    "        print(f\"  Miglioramento: {improvement:+.1f}%\")\n",
    "\n",
    "# 10. SALVATAGGIO MODELLO FINALE E CONFIGURAZIONE\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SALVATAGGIO CONFIGURAZIONE FINALE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Salva il voting classifier e la configurazione ottimale\n",
    "best_model_name = 'Voting'  # Puoi cambiare con il modello migliore\n",
    "best_threshold = results_optimized[best_model_name]['optimal_threshold']\n",
    "\n",
    "config = {\n",
    "    'model_type': 'voting_classifier',\n",
    "    'optimal_threshold': best_threshold,\n",
    "    'models_included': list(dict(voting_clf.estimators).keys()),\n",
    "    'performance_metrics': results_optimized[best_model_name]['metrics'],\n",
    "    'scalers': {name: scaler for name, scaler in scalers.items() if scaler is not None}\n",
    "}\n",
    "\n",
    "# Salva il modello\n",
    "joblib.dump(voting_clf, 'voting_classifier_final.pkl')\n",
    "joblib.dump(config, 'model_config.pkl')\n",
    "\n",
    "print(\"✓ Voting Classifier salvato in 'voting_classifier_final.pkl'\")\n",
    "print(\"✓ Configurazione salvata in 'model_config.pkl'\")\n",
    "print(f\"✓ Soglia ottimale per Recall: {best_threshold:.3f}\")\n",
    "print(f\"✓ Recall finale: {results_optimized[best_model_name]['metrics']['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica i dati per i test \n",
    "fit_data_with_features_clean = pd.read_parquet('test_data/binary_classification/combined_features_filtered.parquet')\n",
    "labels_clean = pd.read_parquet('test_data/binary_classification/labels_binary_filtered.parquet').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc400195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea 10 subset bilanciati di 50000 campioni ciascuno e controllo integrità e distribuzione delle classi\n",
    "num_subsets = 10\n",
    "subset_size = 50000\n",
    "\n",
    "subsets_X_filtered = []\n",
    "subsets_y_filtered = []\n",
    "\n",
    "# Trova gli indici delle due classi\n",
    "idx_0 = np.where(labels_clean == 0)[0]\n",
    "idx_1 = np.where(labels_clean == 1)[0]\n",
    "min_class_size = min(len(idx_0), len(idx_1), subset_size // 2)\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    idx_0_sample = np.random.choice(idx_0, size=min_class_size, replace=False)\n",
    "    idx_1_sample = np.random.choice(idx_1, size=min_class_size, replace=False)\n",
    "    idx_balanced = np.concatenate([idx_0_sample, idx_1_sample])\n",
    "    np.random.shuffle(idx_balanced)\n",
    "    subsets_X_filtered.append(fit_data_with_features_clean[idx_balanced])\n",
    "    subsets_y_filtered.append(labels_clean[idx_balanced])\n",
    "\n",
    "\n",
    "# Controlla la distribuzione delle classi in ogni subset\n",
    "for i, (X, y) in enumerate(zip(subsets_X_filtered, subsets_y_filtered)):\n",
    "    assert len(X) == subset_size, f\"Subset {i+1} size mismatch: {len(X)} != {subset_size}\"\n",
    "    assert len(y) == subset_size, f\"Subset {i+1} labels size mismatch: {len(y)} != {subset_size}\"\n",
    "    print(f\"Subset {i+1}: {Counter(y)}\")\n",
    "\n",
    "\n",
    "# Check if all elements in subsets_X_unfiltered are different\n",
    "all_unique = True\n",
    "for i in range(num_subsets):\n",
    "    for j in range(i + 1, num_subsets):\n",
    "        if np.array_equal(subsets_X_filtered[i], subsets_X_filtered[j]):\n",
    "            print(f\"Subset {i+1} and Subset {j+1} are identical!\")\n",
    "            all_unique = False\n",
    "if all_unique:\n",
    "    print(\"All subsets in subsets_X_filtered are different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10563059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SU DATASET BILANCIATO (50K SAMPLES)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SU DATASET BILANCIATO - 50.000 SAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Assegna i nuovi dati di test\n",
    "X_test_balanced = subsets_X_filtered\n",
    "y_test_balanced = subsets_y_filtered\n",
    "\n",
    "print(f\"\\nDimensioni test set bilanciato: {X_test_balanced.shape}\")\n",
    "print(f\"Distribuzione classi: {np.bincount(y_test_balanced)}\")\n",
    "print(f\"Bilanciamento: {np.bincount(y_test_balanced)[1] / len(y_test_balanced):.2%} classe positiva\")\n",
    "\n",
    "# Funzione per test esteso con analisi dettagliata\n",
    "def extended_test_evaluation(model, X_test, y_test, model_name, threshold, scaler=None):\n",
    "    \"\"\"Valutazione estesa con analisi per sottogruppi\"\"\"\n",
    "    \n",
    "    # Applica scaler se necessario\n",
    "    if scaler is not None:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_test_scaled = X_test\n",
    "    \n",
    "    # Calcola probabilità e predizioni\n",
    "    probas = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred = (probas >= threshold).astype(int)\n",
    "    \n",
    "    # Metriche generali\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, probas)\n",
    "    }\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    # Metriche aggiuntive\n",
    "    specificity = tn / (tn + fp)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{model_name} - Test Set Bilanciato (Soglia: {threshold:.3f})\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Accuracy:    {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision:   {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:      {metrics['recall']:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"F1-Score:    {metrics['f1']:.4f}\")\n",
    "    print(f\"AUC-ROC:     {metrics['auc']:.4f}\")\n",
    "    print(f\"NPV:         {npv:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"  TP: {tp:,}  FP: {fp:,}\")\n",
    "    print(f\"  FN: {fn:,}  TN: {tn:,}\")\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'probas': probas,\n",
    "        'y_pred': y_pred,\n",
    "        'confusion_matrix': (tn, fp, fn, tp),\n",
    "        'specificity': specificity,\n",
    "        'npv': npv\n",
    "    }\n",
    "\n",
    "# Test tutti i modelli con soglie ottimizzate\n",
    "results_balanced = {}\n",
    "\n",
    "# CatBoost\n",
    "if catboost_model_final is not None:\n",
    "    threshold = results_optimized['CatBoost']['optimal_threshold']\n",
    "    results = extended_test_evaluation(\n",
    "        catboost_model_final, X_test_balanced, y_test_balanced, \n",
    "        \"CatBoost\", threshold, catboost_scaler\n",
    "    )\n",
    "    results_balanced['CatBoost'] = results\n",
    "\n",
    "# XGBoost (se disponibile)\n",
    "if xg_boost_model_final is not None:\n",
    "    threshold = results_optimized['XGBoost']['optimal_threshold']\n",
    "    results = extended_test_evaluation(\n",
    "        xg_boost_model_final, X_test_balanced, y_test_balanced,\n",
    "        \"XGBoost\", threshold, xgboost_scaler\n",
    "    )\n",
    "    results_balanced['XGBoost'] = results\n",
    "\n",
    "# Voting Classifier\n",
    "threshold = results_optimized['Voting']['optimal_threshold']\n",
    "results = extended_test_evaluation(\n",
    "    voting_clf, X_test_balanced, y_test_balanced,\n",
    "    \"Voting Classifier\", threshold\n",
    ")\n",
    "results_balanced['Voting'] = results\n",
    "\n",
    "# 12. VISUALIZZAZIONI PER TEST SET BILANCIATO\n",
    "\n",
    "# 12.1 Confronto performance tra test set originale e bilanciato\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Confronto Performance: Test Set Originale vs Bilanciato (50K samples)', fontsize=16)\n",
    "\n",
    "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity']\n",
    "models_to_compare = ['CatBoost', 'Voting']  # Aggiungi 'XGBoost' se disponibile\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_compare):\n",
    "    ax = axes[idx//3, idx%3]\n",
    "    \n",
    "    x = np.arange(len(models_to_compare))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Valori dal test set originale\n",
    "    original_values = []\n",
    "    for model in models_to_compare:\n",
    "        if model in results_optimized:\n",
    "            if metric == 'specificity':\n",
    "                # Calcola specificity per test originale\n",
    "                cm = confusion_matrix(y_test, results_optimized[model]['y_pred'])\n",
    "                tn, fp = cm[0, 0], cm[0, 1]\n",
    "                spec = tn / (tn + fp)\n",
    "                original_values.append(spec)\n",
    "            else:\n",
    "                original_values.append(results_optimized[model]['metrics'].get(metric, 0))\n",
    "    \n",
    "    # Valori dal test set bilanciato\n",
    "    balanced_values = []\n",
    "    for model in models_to_compare:\n",
    "        if model in results_balanced:\n",
    "            if metric == 'specificity':\n",
    "                balanced_values.append(results_balanced[model]['specificity'])\n",
    "            else:\n",
    "                balanced_values.append(results_balanced[model]['metrics'].get(metric, 0))\n",
    "    \n",
    "    # Plot\n",
    "    bars1 = ax.bar(x - width/2, original_values, width, label='Test Originale', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, balanced_values, width, label='Test Bilanciato', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Modello')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_title(f'{metric.capitalize()}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models_to_compare)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Aggiungi valori\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 12.2 Distribuzione delle probabilità nel test set bilanciato\n",
    "fig, axes = plt.subplots(1, len(results_balanced), figsize=(15, 5))\n",
    "if len(results_balanced) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_balanced.items()):\n",
    "    probas = result['probas']\n",
    "    threshold = results_optimized[model_name]['optimal_threshold']\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Separa probabilità per classe\n",
    "    probas_class_0 = probas[y_test_balanced == 0]\n",
    "    probas_class_1 = probas[y_test_balanced == 1]\n",
    "    \n",
    "    # Box plot\n",
    "    data = [probas_class_0, probas_class_1]\n",
    "    bp = ax.boxplot(data, labels=['Classe 0', 'Classe 1'], patch_artist=True)\n",
    "    \n",
    "    # Colori\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    # Soglia\n",
    "    ax.axhline(threshold, color='green', linestyle='--', linewidth=2,\n",
    "               label=f'Soglia: {threshold:.3f}')\n",
    "    \n",
    "    ax.set_ylabel('Probabilità Classe 1')\n",
    "    ax.set_title(f'Distribuzione Probabilità - {model_name}\\n(Test Set Bilanciato)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 12.3 Analisi errori per intervalli di confidenza\n",
    "fig, axes = plt.subplots(1, len(results_balanced), figsize=(15, 6))\n",
    "if len(results_balanced) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, result) in enumerate(results_balanced.items()):\n",
    "    probas = result['probas']\n",
    "    y_pred = result['y_pred']\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Definisci intervalli di confidenza\n",
    "    bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    bin_labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
    "    \n",
    "    # Calcola errori per intervallo\n",
    "    errors_by_bin = []\n",
    "    counts_by_bin = []\n",
    "    \n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (probas >= bins[i]) & (probas < bins[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            errors = (y_pred[mask] != y_test_balanced[mask]).mean()\n",
    "            errors_by_bin.append(errors)\n",
    "            counts_by_bin.append(mask.sum())\n",
    "        else:\n",
    "            errors_by_bin.append(0)\n",
    "            counts_by_bin.append(0)\n",
    "    \n",
    "    # Plot\n",
    "    x = np.arange(len(bin_labels))\n",
    "    bars = ax.bar(x, errors_by_bin, alpha=0.7)\n",
    "    \n",
    "    # Aggiungi count sopra le barre\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts_by_bin)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'n={count:,}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Intervallo di Probabilità')\n",
    "    ax.set_ylabel('Tasso di Errore')\n",
    "    ax.set_title(f'Errori per Intervallo di Confidenza - {model_name}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(bin_labels)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 12.4 Curve di calibrazione\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Curve di calibrazione\n",
    "ax1 = axes[0]\n",
    "for model_name, result in results_balanced.items():\n",
    "    probas = result['probas']\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_test_balanced, probas, n_bins=10\n",
    "    )\n",
    "    \n",
    "    ax1.plot(mean_predicted_value, fraction_of_positives, \n",
    "             marker='o', linewidth=2, label=model_name)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Perfettamente calibrato')\n",
    "ax1.set_xlabel('Probabilità Media Predetta')\n",
    "ax1.set_ylabel('Frazione di Positivi')\n",
    "ax1.set_title('Curve di Calibrazione - Test Set Bilanciato')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Istogramma delle predizioni\n",
    "ax2 = axes[1]\n",
    "for model_name, result in results_balanced.items():\n",
    "    probas = result['probas']\n",
    "    ax2.hist(probas, bins=20, alpha=0.5, label=model_name, density=True)\n",
    "\n",
    "ax2.set_xlabel('Probabilità Predetta')\n",
    "ax2.set_ylabel('Densità')\n",
    "ax2.set_title('Distribuzione delle Probabilità Predette')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 13. REPORT FINALE COMPARATIVO\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REPORT FINALE - CONFRONTO TEST SET ORIGINALE VS BILANCIATO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crea tabella comparativa\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in results_balanced.keys():\n",
    "    # Test originale\n",
    "    if model_name in results_optimized:\n",
    "        orig = results_optimized[model_name]['metrics']\n",
    "        comparison_data.append({\n",
    "            'Modello': model_name,\n",
    "            'Test Set': 'Originale',\n",
    "            'Samples': len(y_test),\n",
    "            'Accuracy': f\"{orig['accuracy']:.4f}\",\n",
    "            'Precision': f\"{orig['precision']:.4f}\",\n",
    "            'Recall': f\"{orig['recall']:.4f}\",\n",
    "            'F1-Score': f\"{orig['f1']:.4f}\",\n",
    "            'AUC-ROC': f\"{orig['auc']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Test bilanciato\n",
    "    bal = results_balanced[model_name]['metrics']\n",
    "    comparison_data.append({\n",
    "        'Modello': model_name,\n",
    "        'Test Set': 'Bilanciato',\n",
    "        'Samples': len(y_test_balanced),\n",
    "        'Accuracy': f\"{bal['accuracy']:.4f}\",\n",
    "        'Precision': f\"{bal['precision']:.4f}\",\n",
    "        'Recall': f\"{bal['recall']:.4f}\",\n",
    "        'F1-Score': f\"{bal['f1']:.4f}\",\n",
    "        'AUC-ROC': f\"{bal['auc']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# Analisi delle differenze\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALISI DELLE DIFFERENZE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name in results_balanced.keys():\n",
    "    if model_name in results_optimized:\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # Calcola differenze\n",
    "        for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "            orig_val = results_optimized[model_name]['metrics'][metric]\n",
    "            bal_val = results_balanced[model_name]['metrics'][metric]\n",
    "            diff = bal_val - orig_val\n",
    "            diff_pct = (diff / orig_val) * 100 if orig_val > 0 else 0\n",
    "            \n",
    "            print(f\"  {metric.capitalize():12} Δ = {diff:+.4f} ({diff_pct:+.1f}%)\")\n",
    "\n",
    "# Raccomandazioni finali\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RACCOMANDAZIONI FINALI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_recall = max(results_balanced.items(), key=lambda x: x[1]['metrics']['recall'])\n",
    "best_f1 = max(results_balanced.items(), key=lambda x: x[1]['metrics']['f1'])\n",
    "best_balanced = max(results_balanced.items(), \n",
    "                   key=lambda x: x[1]['metrics']['recall'] * x[1]['metrics']['precision'])\n",
    "\n",
    "print(f\"\\n✓ Miglior Recall: {best_recall[0]} ({best_recall[1]['metrics']['recall']:.4f})\")\n",
    "print(f\"✓ Miglior F1-Score: {best_f1[0]} ({best_f1[1]['metrics']['f1']:.4f})\")\n",
    "print(f\"✓ Miglior bilanciamento Precision-Recall: {best_balanced[0]}\")\n",
    "\n",
    "print(\"\\nNOTE:\")\n",
    "print(\"- Il test set bilanciato fornisce una valutazione più robusta delle performance\")\n",
    "print(\"- Le differenze tra i due test set indicano quanto il modello sia sensibile\")\n",
    "print(\"  alla distribuzione delle classi\")\n",
    "print(\"- Per applicazioni critiche dove il Recall è fondamentale, considerare\")\n",
    "print(\"  l'uso di soglie ancora più conservative\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
