{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e257e919",
   "metadata": {},
   "source": [
    "### Routine to asses model performances. \n",
    "\n",
    "- 10 different test sets, class balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a91782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.fft import fft\n",
    "from catboost import CatBoostClassifier\n",
    "from joblib import load\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7983fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = 'E:/data/RONGOWAI_L1_SDR_V1.0/' # Change this to your root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb43cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurfaceTypeUtils:\n",
    "    surface_type_dict = {\n",
    "        -1: \"Ocean\",\n",
    "        0: \"NaN\",\n",
    "        1: \"Artifical\",\n",
    "        2: \"Barely vegetated\",\n",
    "        3: \"Inland water\",\n",
    "        4: \"Crop\",\n",
    "        5: \"Grass\",\n",
    "        6: \"Shrub\",\n",
    "        7: \"Forest\"\n",
    "    }\n",
    "    \n",
    "    ddm_antennas = {\n",
    "        0: 'None',\n",
    "        1: 'Zenith',\n",
    "        2: 'LHCP',\n",
    "        3: 'RHCP',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa9b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCDFPreprocessor:\n",
    "    def __init__(self, root_dir, preprocessing_method=str):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "        self.preprocessing_method = preprocessing_method\n",
    "        if self.preprocessing_method not in ['filtered', 'with_lat_lons', 'unfiltered']:\n",
    "            raise ValueError(\"Invalid preprocessing method. Choose from 'filtered', 'with_lat_lons', or 'unfiltered'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def check_integrity(f):\n",
    "        \"\"\"Check integrity of the netCDF file\"\"\"\n",
    "        if not isinstance(f, netCDF4.Dataset):\n",
    "            raise ValueError(\"Input must be a netCDF4.Dataset object\")\n",
    "        if 'raw_counts' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'raw_counts' variable\")\n",
    "        if 'sp_alt' not in f.variables or 'sp_inc_angle' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_alt' or 'sp_inc_angle' variables\")\n",
    "        if 'sp_rx_gain_copol' not in f.variables or 'sp_rx_gain_xpol' not in f.variables or 'ddm_snr' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_rx_gain_copol', 'sp_rx_gain_xpol' or 'ddm_snr' variables\")\n",
    "        if 'sp_lat' not in f.variables or 'sp_lon' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_lat' or 'sp_lon' variables\")\n",
    "        if 'sp_surface_type' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'sp_surface_type' variable\")\n",
    "        if 'ac_alt' not in f.variables:\n",
    "            raise KeyError(\"The netCDF file does not contain 'ac_alt' variable\")\n",
    "        if f.variables['raw_counts'].ndim != 4:\n",
    "            raise ValueError(\"The 'raw_counts' variable must have 4 dimensions\")\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        \"\"\" Preprocess the netCDF file and return fit data and labels \"\"\"\n",
    "        # Check integrity of the netCDF file\n",
    "        self.check_integrity(f)\n",
    "\n",
    "        raw_counts = f.variables['raw_counts'][:]\n",
    "        ac_alt = f.variables['ac_alt'][:]\n",
    "        sp_alt = f.variables['sp_alt'][:]\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        sp_inc_angle = f.variables['sp_inc_angle'][:]\n",
    "\n",
    "        distance_2d = (ac_alt[:, np.newaxis] - sp_alt) / np.cos(np.deg2rad(sp_inc_angle)) # Distance between the aircraft and the specular point\n",
    "\n",
    "        # Filtering mask\n",
    "        keep_mask = (\n",
    "            (copol >= 5) & # # SP copolarized gain\n",
    "            (xpol >= 5) & # SP cross-polarized gain\n",
    "            (snr > 0) & # Positive signal-to-Noise Ratio\n",
    "            (distance_2d >= 2000) & #SP distance min\n",
    "            (distance_2d <= 10000) & #SP distance max\n",
    "            ~np.isnan(copol) & \n",
    "            ~np.isnan(xpol) & \n",
    "            ~np.isnan(snr) & \n",
    "            ~np.isnan(distance_2d)\n",
    "        )\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "        i_indices, j_indices = np.where(keep_mask)\n",
    "        output_array[i_indices, j_indices] = raw_counts[i_indices, j_indices]\n",
    "\n",
    "        n_time, n_samples = raw_counts.shape[:2]\n",
    "        raw_counts_reshaped = output_array.reshape(n_time * n_samples, *raw_counts.shape[2:])\n",
    "        del output_array\n",
    "        # Valid mask to filter out NaN and zero-sum rows\n",
    "        valid_mask = ~np.any(np.isnan(raw_counts_reshaped), axis=(1, 2)) & (np.sum(raw_counts_reshaped, axis=(1, 2)) > 0)\n",
    "\n",
    "        fit_data = raw_counts_reshaped[valid_mask].reshape(valid_mask.sum(), -1)\n",
    "\n",
    "        surface_types = np.nan_to_num(f.variables[\"sp_surface_type\"][:], nan=0).ravel()\n",
    "        label_data = np.isin(surface_types, np.arange(1, 8)).astype(np.int32)\n",
    "        label_data = label_data[valid_mask]\n",
    "        # Ensure that fit_data and label_data have the same length\n",
    "        assert fit_data.shape[0] == len(label_data), \\\n",
    "            f\"Shape mismatch: fit_data {fit_data.shape[0]}, label_data {len(label_data)}\"\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def preprocess_w_lat_lons(self, f):\n",
    "        \"\"\" Version of the preprocessing function returning latitude and longitudes of the specular points \"\"\"\n",
    "\n",
    "        self.check_integrity(f)\n",
    "        raw_counts = np.array(f.variables['raw_counts'])\n",
    "\n",
    "        # Distance between the aircraft and the specular point\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "        specular_point_lat = f.variables['sp_lat'][:]\n",
    "        specular_point_lon = f.variables['sp_lon'][:]\n",
    "\n",
    "        # Filtering with mask\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data) & ~np.isnan(specular_point_lat.data) & ~np.isnan(specular_point_lon.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "\n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        specular_point_lats = specular_point_lat[to_keep_indices[:, 0]]\n",
    "        specular_point_lons = specular_point_lon[to_keep_indices[:, 0]]\n",
    "\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "        # Reshape the output array to match the original dimensions\n",
    "            raw_counts_filtered = output_array.copy()\n",
    "\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "        del output_array\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "\n",
    "        specular_point_lats = specular_point_lat.ravel()[keep_indices]\n",
    "        specular_point_lons = specular_point_lon.ravel()[keep_indices]\n",
    "\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "        label_data = [1 if surface_type in np.arange(1, 8) else 0 for surface_type in surface_types_unravelled]\n",
    "        label_data = [label_data[lab] for lab in range(len(label_data)) if lab in keep_indices]\n",
    "\n",
    "        assert np.array(fit_data).shape[0] == len(label_data) == np.array(specular_point_lats).shape[0] == np.array(specular_point_lons).shape[0], \\\n",
    "            f\"Shape mismatch: fit_data {np.array(fit_data).shape[0]}, label_data {len(label_data)}, lats {np.array(specular_point_lats).shape[0]}, lons {np.array(specular_point_lons).shape[0]}\"\n",
    "\n",
    "\n",
    "        return fit_data, label_data, specular_point_lats, specular_point_lons\n",
    "\n",
    "    def preprocess_snr_unfiltered(self, f):\n",
    "        \"\"\" Preprocess the netCDF file and return fit data and labels without filtering on signal-to-noise ratio \"\"\"\n",
    "        # Check integrity of the netCDF file\n",
    "        self.check_integrity(f)\n",
    "\n",
    "        raw_counts = f.variables['raw_counts'][:]\n",
    "        ac_alt = f.variables['ac_alt'][:]\n",
    "        sp_alt = f.variables['sp_alt'][:]\n",
    "        sp_inc_angle = f.variables['sp_inc_angle'][:]\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        #snr = f.variables['ddm_snr'][:]\n",
    "\n",
    "        #Distance between the aircraft and the specular point\n",
    "        distance_2d = (ac_alt[:, np.newaxis] - sp_alt) / np.cos(np.deg2rad(sp_inc_angle))\n",
    "        # Filtering mask without SNR\n",
    "        keep_mask = (\n",
    "            (copol >= 5) & \n",
    "            (xpol >= 5) & \n",
    "        #   (snr > 0)  &\n",
    "            (distance_2d >= 2000) & \n",
    "            (distance_2d <= 10000) &\n",
    "            ~np.isnan(copol) & \n",
    "            ~np.isnan(xpol) & \n",
    "            #~np.isnan(snr) & \n",
    "            ~np.isnan(distance_2d)\n",
    "        )\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "        i_indices, j_indices = np.where(keep_mask)\n",
    "        output_array[i_indices, j_indices] = raw_counts[i_indices, j_indices]\n",
    "\n",
    "        n_time, n_samples = raw_counts.shape[:2]\n",
    "        raw_counts_reshaped = output_array.reshape(n_time * n_samples, *raw_counts.shape[2:])\n",
    "        del output_array\n",
    "        valid_mask = ~np.any(np.isnan(raw_counts_reshaped), axis=(1, 2)) & (np.sum(raw_counts_reshaped, axis=(1, 2)) > 0)\n",
    "        fit_data = raw_counts_reshaped[valid_mask].reshape(valid_mask.sum(), -1)\n",
    "\n",
    "        surface_types = np.nan_to_num(f.variables[\"sp_surface_type\"][:], nan=0).ravel()\n",
    "        label_data = np.isin(surface_types, np.arange(1, 8)).astype(np.int32)\n",
    "        label_data = label_data[valid_mask]\n",
    "        # Ensure that fit_data and label_data have the same length\n",
    "        assert fit_data.shape[0] == len(label_data), \\\n",
    "            f\"Shape mismatch: fit_data {fit_data.shape[0]}, label_data {len(label_data)}\"\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "\n",
    "    def process_all_files_random_picked(self, chunk_size = int, sample_fraction = float, n_files_to_pick= int, remove_chunks= bool):\n",
    "        \"\"\" Process all netCDF files in the directory, randomly picking a specified number of files,\n",
    "        and save the processed data and labels in chunks.\"\"\"\n",
    "\n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        counter = 0\n",
    "        # Take a random number of netCDF files\n",
    "        if int(len(self.netcdf_file_list)) > n_files_to_pick: # type: ignore\n",
    "            np.random.seed(42)\n",
    "            random_netcdf_selected_files = np.random.choice(self.netcdf_file_list, n_files_to_pick, replace=False) # type: ignore\n",
    "            print('Selezionati 500 file netCDF casuali dalla lista')\n",
    "        else:\n",
    "            random_netcdf_selected_files = self.netcdf_file_list\n",
    "            print(f\"Total number of netCDF files: {len(self.netcdf_file_list)}. Processing all files.\")\n",
    "\n",
    "        for file_name in tqdm(random_netcdf_selected_files, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                print(f\"Skipping file {file_name} as it does not have the expected extension.\")\n",
    "                continue\n",
    "            try:\n",
    "                f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                if self.preprocessing_method == 'unfiltered':\n",
    "                    data, labels = self.preprocess_snr_unfiltered(f)\n",
    "                elif self.preprocessing_method == 'with_lat_lons':\n",
    "                    data, labels, latitudes, longitudes = self.preprocess_w_lat_lons(f)\n",
    "                else:\n",
    "                    # Default to filtered preprocessing\n",
    "                    data, labels = self.preprocess(f)\n",
    "                assert (len(data) == len(labels)), f\"Data and labels length mismatch in file {file_name}: {len(data)} != {len(labels)}\"\n",
    "                full_data.append(data)\n",
    "                full_labels.append(labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "            counter += 1\n",
    "            if counter == n_files_to_pick:\n",
    "                break\n",
    "        print(f\"Processed {counter} files out of {len(random_netcdf_selected_files)} selected files.\")\n",
    "        # Filtering on data shape\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "        print(f\"Number of valid data arrays after filtering: {len(full_data_clean)}\")\n",
    "\n",
    "        # Chunking \n",
    "        os.makedirs('test_data/binary_classification', exist_ok=True)\n",
    "        chunk_size = chunk_size # chunk dimension\n",
    "        sample_fraction = sample_fraction # Fraction of data to sample from each chunk\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))  # type: ignore\n",
    "        print(f\"Total number of chunks: {num_chunks}\")\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size # type: ignore\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean)) # type: ignore\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            if chunk_data.size == 0 or chunk_labels.size == 0:\n",
    "                print(f\"Skipping empty chunk {idx + 1}/{num_chunks}\")\n",
    "                continue\n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "\n",
    "            if chunk_data.shape[0] == 0 or chunk_labels.shape[0] == 0:\n",
    "                print(f\"Skipping empty chunk {idx + 1}/{num_chunks}\")\n",
    "                continue\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'test_data/binary_classification/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'test_data/binary_classification/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            # Stratified sampling from each chunk\n",
    "            _, X_sampled, _, y_sampled = train_test_split(\n",
    "                chunk_data, chunk_labels,\n",
    "                test_size=sample_fraction,  # type: ignore\n",
    "                stratify=chunk_labels,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        del full_data, full_labels, \n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "\n",
    "        del full_data_sampled, full_labels_sampled\n",
    "        print(f\"Shape of sampled data after chunking and sampling: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels after chunking and sampling: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "\n",
    "        # Save the final sampled data and labels in parquet format\n",
    "        if not os.path.exists('test_data/binary_classification'):\n",
    "            print(\"Creating directory test_data/binary_classification\")\n",
    "            os.makedirs('test_data/binary_classification', exist_ok=True)\n",
    "\n",
    "        # Save fit_data \n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'test_data/binary_classification/fit_data_binary_test.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "        )\n",
    "        # Save labels\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'test_data/binary_classification/labels_binary_test.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "        )\n",
    "        # Clean up\n",
    "        del fit_data_df, labels_df, table_fit, table_labels\n",
    "\n",
    "        print(\"Data and labels saved in test_data/binary_classification directory.\")\n",
    "        # Remove all chunk parquet files if flag is set (to save space)\n",
    "        if remove_chunks:\n",
    "            try:\n",
    "                chunk_dir = 'test_data/binary_classification'\n",
    "                for fname in os.listdir(chunk_dir):\n",
    "                    if fname.startswith('fit_data_chunk_') or fname.startswith('labels_chunk_'):\n",
    "                        os.remove(os.path.join(chunk_dir, fname))\n",
    "                print(\"All chunk files removed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing chunk files: {e}\")\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdea7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def gini(self, array):\n",
    "            \"\"\"Gini coefficient calculation\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))  \n",
    "      \n",
    "    def extract_ddm_features(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract features from DDM data.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float64) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. General statistics\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = self.gini(x)\n",
    "\n",
    "            # 2. Positional \n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentations in thirds\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "\n",
    "            # 3.1 Segmentations in windows of 5\n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivative statistics and differences\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelations (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT \n",
    "            spectrum = np.abs(fft(x)) # type: ignore\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  \n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "            \n",
    "            #Quadrant and center statistics\n",
    "            ddm = row.reshape(10, 20)  # 10x20\n",
    "\n",
    "            # Quadrants\n",
    "            q1 = ddm[:5, :10].ravel()\n",
    "            q2 = ddm[:5, 10:].ravel()\n",
    "            q3 = ddm[5:, :10].ravel()\n",
    "            q4 = ddm[5:, 10:].ravel()\n",
    "            # Quadrante centrale (4x8 centrale)\n",
    "            center = ddm[3:7, 6:14].ravel()\n",
    "            # Statistiche dei quadranti\n",
    "            f['q1_mean'] = np.mean(q1)\n",
    "            f['q2_mean'] = np.mean(q2)\n",
    "            f['q3_mean'] = np.mean(q3)\n",
    "            f['q4_mean'] = np.mean(q4)\n",
    "            f['center_mean'] = np.mean(center)\n",
    "            f['q1_std'] = np.std(q1)\n",
    "            f['q2_std'] = np.std(q2)\n",
    "            f['q3_std'] = np.std(q3)\n",
    "            f['q4_std'] = np.std(q4)\n",
    "            f['center_std'] = np.std(center)\n",
    "            f['q1_min'] = np.min(q1)\n",
    "            f['q2_min'] = np.min(q2)\n",
    "            f['q3_min'] = np.min(q3)\n",
    "            f['q4_min'] = np.min(q4)\n",
    "            f['center_min'] = np.min(center)\n",
    "            f['q1_max'] = np.max(q1)\n",
    "            f['q2_max'] = np.max(q2)\n",
    "            f['q3_max'] = np.max(q3)\n",
    "            f['q4_max'] = np.max(q4)\n",
    "            f['center_max'] = np.max(center)\n",
    "            f['q1_median'] = np.median(q1)\n",
    "            f['q2_median'] = np.median(q2)\n",
    "            f['q3_median'] = np.median(q3)\n",
    "            f['q4_median'] = np.median(q4)\n",
    "            f['center_median'] = np.median(center)\n",
    "            f['q1_range'] = np.max(q1) - np.min(q1)\n",
    "            f['q2_range'] = np.max(q2) - np.min(q2)\n",
    "            f['q3_range'] = np.max(q3) - np.min(q3)\n",
    "            f['q4_range'] = np.max(q4) - np.min(q4)\n",
    "            f['center_range'] = np.max(center) - np.min(center)\n",
    "            f['q1_skew'] = skew(q1)\n",
    "            f['q2_skew'] = skew(q2)\n",
    "            f['q3_skew'] = skew(q3)\n",
    "            f['q4_skew'] = skew(q4)\n",
    "            f['center_skew'] = skew(center)\n",
    "            f['q1_kurtosis'] = kurtosis(q1)\n",
    "            f['q2_kurtosis'] = kurtosis(q2)\n",
    "            f['q3_kurtosis'] = kurtosis(q3)\n",
    "            f['q4_kurtosis'] = kurtosis(q4)\n",
    "            f['center_kurtosis'] = kurtosis(center)\n",
    "            f['q1_entropy'] = entropy(q1 + 1e-10)\n",
    "            f['q2_entropy'] = entropy(q2 + 1e-10)\n",
    "            f['q3_entropy'] = entropy(q3 + 1e-10)\n",
    "            f['q4_entropy'] = entropy(q4 + 1e-10)\n",
    "            f['center_entropy'] = entropy(center + 1e-10)\n",
    "            f['q1_gini'] = self.gini(q1)\n",
    "            f['q2_gini'] = self.gini(q2)\n",
    "            f['q3_gini'] = self.gini(q3)\n",
    "            f['q4_gini'] = self.gini(q4)\n",
    "            f['center_gini'] = self.gini(center)\n",
    "\n",
    "            # Confrontations between quadrants and center \n",
    "            \n",
    "            # Mean differences between quadrants and center\n",
    "            f['q1_center_mean_diff'] = f['q1_mean'] - f['center_mean']\n",
    "            f['q2_center_mean_diff'] = f['q2_mean'] - f['center_mean']\n",
    "            f['q3_center_mean_diff'] = f['q3_mean'] - f['center_mean']\n",
    "            f['q4_center_mean_diff'] = f['q4_mean'] - f['center_mean']\n",
    "\n",
    "            # Standard deviation differences between quadrants and center\n",
    "            f['q1_center_std_diff'] = f['q1_std'] - f['center_std']\n",
    "            f['q2_center_std_diff'] = f['q2_std'] - f['center_std']\n",
    "            f['q3_center_std_diff'] = f['q3_std'] - f['center_std']\n",
    "            f['q4_center_std_diff'] = f['q4_std'] - f['center_std']\n",
    "\n",
    "            # Maximum differences between quadrants and center\n",
    "            f['q1_center_max_diff'] = f['q1_max'] - f['center_max']\n",
    "            f['q2_center_max_diff'] = f['q2_max'] - f['center_max']\n",
    "            f['q3_center_max_diff'] = f['q3_max'] - f['center_max']\n",
    "            f['q4_center_max_diff'] = f['q4_max'] - f['center_max']\n",
    "\n",
    "            # Minimum differences between quadrants and center\n",
    "            f['q1_center_min_diff'] = f['q1_min'] - f['center_min']\n",
    "            f['q2_center_min_diff'] = f['q2_min'] - f['center_min']\n",
    "            f['q3_center_min_diff'] = f['q3_min'] - f['center_min']\n",
    "            f['q4_center_min_diff'] = f['q4_min'] - f['center_min']\n",
    "\n",
    "            # entropy differences between quadrants and center\n",
    "            f['q1_center_entropy_diff'] = f['q1_entropy'] - f['center_entropy']\n",
    "            f['q2_center_entropy_diff'] = f['q2_entropy'] - f['center_entropy']\n",
    "            f['q3_center_entropy_diff'] = f['q3_entropy'] - f['center_entropy']\n",
    "            f['q4_center_entropy_diff'] = f['q4_entropy'] - f['center_entropy']\n",
    "\n",
    "            # Gini differences between quadrants and center\n",
    "            f['q1_center_gini_diff'] = f['q1_gini'] - f['center_gini']\n",
    "            f['q2_center_gini_diff'] = f['q2_gini'] - f['center_gini']\n",
    "            f['q3_center_gini_diff'] = f['q3_gini'] - f['center_gini']\n",
    "            f['q4_center_gini_diff'] = f['q4_gini'] - f['center_gini']\n",
    "\n",
    "            # Skewness differences between quadrants and center\n",
    "            f['q1_center_skew_diff'] = f['q1_skew'] - f['center_skew']\n",
    "            f['q2_center_skew_diff'] = f['q2_skew'] - f['center_skew']\n",
    "            f['q3_center_skew_diff'] = f['q3_skew'] - f['center_skew']\n",
    "            f['q4_center_skew_diff'] = f['q4_skew'] - f['center_skew']\n",
    "\n",
    "            # Kurtosis differences between quadrants and center\n",
    "            f['q1_center_kurtosis_diff'] = f['q1_kurtosis'] - f['center_kurtosis']\n",
    "            f['q2_center_kurtosis_diff'] = f['q2_kurtosis'] - f['center_kurtosis']\n",
    "            f['q3_center_kurtosis_diff'] = f['q3_kurtosis'] - f['center_kurtosis']\n",
    "            f['q4_center_kurtosis_diff'] = f['q4_kurtosis'] - f['center_kurtosis']\n",
    "\n",
    "            features.append(f)\n",
    "        return features # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4971b94",
   "metadata": {},
   "source": [
    "### Test against 10 test_set, 50k samples, classes balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73581470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selezionati 500 file netCDF casuali dalla lista\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  78%|███████▊  | 39/50 [00:05<00:01,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file 20250212-195305_NZAA-NZWR_L1.nc: cannot reshape array of size 0 into shape (0,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 50/50 [00:07<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 49 files out of 50 selected files.\n",
      "Number of valid data arrays after filtering: 49\n",
      "Total number of chunks: 1\n",
      "Chunk 1/1 processed with shape (332689, 200) and labels shape (332689,)\n",
      "Shape of sampled data after chunking and sampling: (299421, 200)\n",
      "Shape of sampled labels after chunking and sampling: (299421,)\n",
      "Data and labels saved in test_data/binary_classification directory.\n",
      "All chunk files removed.\n"
     ]
    }
   ],
   "source": [
    "read_from_backup = False\n",
    "if read_from_backup:\n",
    "\n",
    "    fit_data_pl = pd.read_parquet('./test_data/binary_classification/fit_data_binary_test.parquet')\n",
    "    labels_pl = pd.read_parquet('./test_data/binary_classification/labels_binary_test.parquet')\n",
    "\n",
    "    # Trasforma in numpy array\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()\n",
    "    del fit_data_pl, labels_pl\n",
    "else:\n",
    "    preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR, preprocessing_method='filtered') # type: ignore\n",
    "    # Process all files and get fit_data and labels\n",
    "    fit_data, labels = preprocessor.process_all_files_random_picked(chunk_size=250, \n",
    "                                                                    n_files_to_pick= 1000, \n",
    "                                                                    sample_fraction=0.9,\n",
    "                                                                    remove_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e4fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a subset of the data for training\n",
    "num_samples = 2000000 \n",
    "indices = np.random.choice(fit_data.shape[0], size=num_samples, replace=False)\n",
    "fit_data = fit_data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1282f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the feature extractor\n",
    "preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR, preprocessing_method='filtered')\n",
    "features_extractor = DDMFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "537d6064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction: 100%|██████████| 50000/50000 [00:18<00:00, 2656.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#Extract features from the fit_data using parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_ddm_features_row(row):\n",
    "    return features_extractor.extract_ddm_features(np.array([row]))\n",
    "\n",
    "combined_features = Parallel(n_jobs=12, backend=\"loky\")(delayed(extract_ddm_features_row)(row) for row in tqdm(fit_data, desc=\"Feature extraction\")) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4490a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_features = [row[0] if isinstance(row, list) and len(row) > 0 else row for row in combined_features]\n",
    "FEATURES = list(flat_features[0].keys())\n",
    "\n",
    "combined_features = np.array([[row[key] for key in FEATURES] for row in flat_features])\n",
    "combined_features.shape\n",
    "\n",
    "# Clean the features and labels\n",
    "# Remove rows with NaN or infinite values\n",
    "mask_finite = np.isfinite(combined_features).all(axis=1) & (np.abs(combined_features) < np.finfo(np.float64).max).all(axis=1)\n",
    "fit_data_with_features_clean = combined_features[mask_finite]\n",
    "labels_clean = labels[mask_finite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup memory\n",
    "del labels, fit_data\n",
    "del combined_features, flat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48182ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data and labels to parquet files\n",
    "save = True\n",
    "if save:\n",
    "    # Crea la cartella processed_data/binary_classification/data_w_features se non esiste\n",
    "    os.makedirs('processed_data/binary_classification/data_w_features', exist_ok=True)\n",
    "    pd.DataFrame(fit_data_with_features_clean, columns=FEATURES).to_parquet('test_data/binary_classification/combined_features_filtered.parquet', index=False)\n",
    "    pd.DataFrame(labels_clean).to_parquet('test_data/binary_classification/labels_binary_filtered.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e5641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1: Counter({1: 2500, 0: 2500})\n",
      "Subset 2: Counter({0: 2500, 1: 2500})\n",
      "Subset 3: Counter({1: 2500, 0: 2500})\n",
      "Subset 4: Counter({1: 2500, 0: 2500})\n",
      "Subset 5: Counter({0: 2500, 1: 2500})\n",
      "Subset 6: Counter({1: 2500, 0: 2500})\n",
      "Subset 7: Counter({0: 2500, 1: 2500})\n",
      "Subset 8: Counter({1: 2500, 0: 2500})\n",
      "Subset 9: Counter({1: 2500, 0: 2500})\n",
      "Subset 10: Counter({1: 2500, 0: 2500})\n",
      "All subsets in subsets_X_filtered are different.\n"
     ]
    }
   ],
   "source": [
    "# Create subsets of the data with balanced classes\n",
    "num_subsets = 10\n",
    "subset_size = 50000\n",
    "\n",
    "subsets_X_filtered = []\n",
    "subsets_y_filtered = []\n",
    "\n",
    "# Trova gli indici delle due classi\n",
    "idx_0 = np.where(labels_clean == 0)[0]\n",
    "idx_1 = np.where(labels_clean == 1)[0]\n",
    "min_class_size = min(len(idx_0), len(idx_1), subset_size // 2)\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    idx_0_sample = np.random.choice(idx_0, size=min_class_size, replace=False)\n",
    "    idx_1_sample = np.random.choice(idx_1, size=min_class_size, replace=False)\n",
    "    idx_balanced = np.concatenate([idx_0_sample, idx_1_sample])\n",
    "    np.random.shuffle(idx_balanced)\n",
    "    subsets_X_filtered.append(fit_data_with_features_clean[idx_balanced])\n",
    "    subsets_y_filtered.append(labels_clean[idx_balanced])\n",
    "\n",
    "# Check the size of each subset and the distribution of classes\n",
    "for i, (X, y) in enumerate(zip(subsets_X_filtered, subsets_y_filtered)):\n",
    "    assert len(X) == subset_size, f\"Subset {i+1} size mismatch: {len(X)} != {subset_size}\"\n",
    "    assert len(y) == subset_size, f\"Subset {i+1} labels size mismatch: {len(y)} != {subset_size}\"\n",
    "    print(f\"Subset {i+1}: {Counter(y)}\")\n",
    "\n",
    "# Check if all elements in subsets_X_unfiltered are different\n",
    "all_unique = True\n",
    "for i in range(num_subsets):\n",
    "    for j in range(i + 1, num_subsets):\n",
    "        if np.array_equal(subsets_X_filtered[i], subsets_X_filtered[j]):\n",
    "            print(f\"Subset {i+1} and Subset {j+1} are identical!\")\n",
    "            all_unique = False\n",
    "if all_unique:\n",
    "    print(\"All subsets in subsets_X_filtered are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87517bbb",
   "metadata": {},
   "source": [
    "### Caricamento modelli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea08cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_catboost_model(model_path=str):\n",
    "        \"\"\"Carica il modello CatBoost dal percorso specificato\"\"\"\n",
    "        try:\n",
    "            model = CatBoostClassifier()\n",
    "            model.load_model(str(model_path))\n",
    "            #print(f\"Modello caricato con successo da: {model_path}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento del modello: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d009b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model_path = r\"./models/catboost_test120625_1946/catboost_trained_on_full_data_model_v1.cbm\"\n",
    "catboost_scaler_path = \"./models/catboost_test120625_1946/catboost_trained_on_full_data_model_v1_scaler.joblib\"\n",
    "catboost_scaler = load(catboost_scaler_path)\n",
    "xg_boost_model_path = \"./models/xgboost_run_300625_1623/xgboost_production_model_300625_1623.joblib\"\n",
    "xg_boost_scaler_path = \"./models/xgboost_run_300625_1623/xgboost_production_model_300625_1623_scaler.joblib\"\n",
    "xg_boost_scaler = load(xg_boost_scaler_path)\n",
    "\n",
    "voting_classifier_path = \"./models/voting/voting_classifier_final.pkl\"\n",
    "\n",
    "models = {\n",
    "    'catboost': _load_catboost_model(catboost_model_path),\n",
    "    'xgboost': load(xg_boost_model_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d111275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model swithches\n",
    "catboost_model_final = True\n",
    "xg_boost_model_final = True\n",
    "voting_classifier = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ed6ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m     has_too_large \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mabs(arr) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m has_inf \u001b[38;5;129;01mor\u001b[39;00m has_too_large\n\u001b[1;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m check_infinite_or_large(catboost_scaler\u001b[38;5;241m.\u001b[39mtransform(\u001b[43msubsets_X_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContains infs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "def check_infinite_or_large(arr, threshold=np.finfo(np.float64).max):\n",
    "    \"\"\"Check if the array contains infinite values or values larger than the max capacity of a float64.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    has_inf = np.isinf(arr).any()\n",
    "    has_too_large = (np.abs(arr) >= threshold).any()\n",
    "    return has_inf or has_too_large\n",
    "\n",
    "\n",
    "result = check_infinite_or_large(catboost_scaler.transform(subsets_X_filtered[0]))\n",
    "print(\"Contains infs:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226dba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for xg_boost_model_final (dict with scaler and label_encoder)\n",
    "\n",
    "def preprocess_for_xgboost(X):\n",
    "    return xg_boost_scaler.transform(X)\n",
    "\n",
    "def predict_with_xgboost(X):\n",
    "    X_proc = preprocess_for_xgboost(X)\n",
    "    model = load(xg_boost_model_path)\n",
    "    return model.predict(X_proc)\n",
    "\n",
    "def preprocess_for_catboost(X):\n",
    "        return catboost_scaler.transform(X)\n",
    "\n",
    "def predict_with_catboost(X):\n",
    "    X_proc = preprocess_for_catboost(X)\n",
    "    model = _load_catboost_model(catboost_model_path) # type: ignore\n",
    "    return model.predict(X_proc)\n",
    "\n",
    "# Preprocessing for sklearn models \n",
    "def preprocess_for_sklearn(X):\n",
    "    # Assume features are already in correct format\n",
    "    return X\n",
    "\n",
    "def predict_with_sklearn(model, X):\n",
    "    X_proc = preprocess_for_sklearn(X)\n",
    "    return model.predict(X_proc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b6b23",
   "metadata": {},
   "source": [
    "### Test on filtered test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build models dict with wrappers for preprocessing\n",
    "\n",
    "models = {}\n",
    "\n",
    "if xg_boost_model_final is not None:\n",
    "    models[\"xgboost\"] = type(\n",
    "        \"XGBWrapper\", (), {\"predict\": staticmethod(predict_with_xgboost)}\n",
    "    )()\n",
    "\n",
    "#if best_binary_classification_model is not None:\n",
    "    #models[\"Best Binary Classification Model\"] = type(\n",
    "    #    \"PyCaretWrapper\", (), {\"predict\": staticmethod(lambda X: predict_with_pycaret(best_binary_classification_model, X))}\n",
    "    #)()\n",
    "\n",
    "#if best_random_forest_model is not None:\n",
    "    #models[\"Best Random Forest Model\"] = type(\n",
    "    #    \"RFWrapper\", (), {\"predict\": staticmethod(lambda X: predict_with_sklearn(best_random_forest_model, X))}\n",
    "    #)()\n",
    "\n",
    "#if best_binary_classification_ensembled_model is not None:\n",
    "    #models[\"Best Binary Classification Ensembled Model\"] = type(\n",
    "    #    \"PyCaretEnsembleWrapper\", (), {\"predict\": staticmethod(lambda X: predict_with_pycaret(best_binary_classification_ensembled_model, X))}\n",
    "    #)()\n",
    "\n",
    "if catboost_model_final is not None:\n",
    "    models[\"catboost\"] = type(\n",
    "        \"CatBoostWrapper\", (), {\"predict\": staticmethod(predict_with_catboost)}\n",
    "    )()\n",
    "\n",
    "if voting_classifier is not None:\n",
    "    models[\"Voting Classifier\"] = type(\n",
    "        \"VotingClassifierWrapper\", (), {\"predict\": staticmethod(lambda X: voting_classifier.predict(X))}\n",
    "    )()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    accs = []\n",
    "    cms = []\n",
    "    for i in range(num_subsets):\n",
    "        X = subsets_X_filtered[i]\n",
    "        y = subsets_y_filtered[i]\n",
    "\n",
    "        # Searching for the optimal threshold if the model supports it\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X)[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y, y_proba)\n",
    "            # Soglia ottimale: massimizza tpr - fpr (Youden's J statistic)\n",
    "            optimal_idx = np.argmax(tpr - fpr)\n",
    "            optimal_threshold = thresholds[optimal_idx]\n",
    "            print(f\"SOGLIA OTTIMALE DI CLASSIFICAZIONE: {optimal_threshold:.4f}\")\n",
    "            y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "        else:\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        print(f\"Subset {i+1}: Accuracy = {acc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        accs.append(acc)\n",
    "        cms.append(cm)\n",
    "    print(f\"Mean Accuracy: {np.mean(accs):.4f}\")\n",
    "    print(f\"Std Accuracy: {np.std(accs):.4f}\")\n",
    "    print(f\"Mean Confusion Matrix:\\n{np.mean(np.array(cms), axis=0).astype(int)}\")\n",
    "\n",
    "    # AVG accuracy and confusion matrix across all subsets\n",
    "    mean_acc = np.mean(accs)\n",
    "    std_acc = np.std(accs)\n",
    "    mean_cm = np.mean(np.array(cms), axis=0)\n",
    "    print(f\"\\nMedia accuratezze: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "    print(f\"Confusion Matrix media:\\n{mean_cm.astype(int)}\")\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot accuracy per subset\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, num_subsets + 1), accs, marker='o')\n",
    "    plt.title(f'Accuracy per subset - {model_name}')\n",
    "    plt.xlabel('Subset')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot mean confusion matrix\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(mean_cm, annot=True, fmt='.0f', cmap='Blues')\n",
    "    plt.title(f'Mean Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcola il classification report medio su tutti i subset\n",
    "    reports = []\n",
    "    for i in range(num_subsets):\n",
    "        y_true = subsets_y_filtered[i]\n",
    "        y_pred = model.predict(subsets_X_filtered[i])\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        reports.append(report)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(range(1, num_subsets + 1), accs, color='red')\n",
    "    plt.title(f'Scatter Plot Accuracy per Test Set - {model_name}')\n",
    "    plt.xlabel('Test Set Number')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3436bd3",
   "metadata": {},
   "source": [
    "### Test su dati non filtrati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_backup = False\n",
    "if read_from_backup:\n",
    "\n",
    "    fit_data_pl = pd.read_parquet('C:/Users/atogni/Desktop/rongowai/test_data/binary_classification/fit_data_binary_test.parquet')\n",
    "    labels_pl = pd.read_parquet('C:/Users/atogni/Desktop/rongowai/test_data/binary_classification/labels_binary_test.parquet')\n",
    "\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()\n",
    "    del fit_data_pl, labels_pl\n",
    "else:\n",
    "    preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR, preprocessing_method='unfiltered')\n",
    "    fit_data, labels = preprocessor.process_all_files_random_picked(chunk_size=250, n_files_to_pick=1000, sample_fraction=0.9,remove_chunks=True)\n",
    "\n",
    "# Randomly select a subset of the data for training\n",
    "num_samples = 2000000 \n",
    "indices = np.random.choice(fit_data.shape[0], size=num_samples, replace=False)\n",
    "fit_data = fit_data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = NetCDFPreprocessor(root_dir=ROOT_DIR, preprocessing_method='unfiltered')\n",
    "features_extractor = DDMFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estrazione delle features DDM\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def extract_ddm_features_row(row):\n",
    "    return features_extractor.extract_ddm_features(np.array([row]))\n",
    "\n",
    "combined_features_unfiltered = Parallel(n_jobs=12, backend=\"loky\")(delayed(extract_ddm_features_row)(row) for row in tqdm(fit_data, desc=\"Estrazione features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_features_unfiltered = [row[0] if isinstance(row, list) and len(row) > 0 else row for row in combined_features_unfiltered]\n",
    "FEATURES = list(flat_features_unfiltered[0].keys())\n",
    "\n",
    "combined_features_unfiltered = np.array([[row[key] for key in FEATURES] for row in flat_features_unfiltered])\n",
    "combined_features_unfiltered.shape\n",
    "\n",
    "# Clean the features and labels\n",
    "# Remove rows with NaN or infinite values\n",
    "mask_finite = np.isfinite(combined_features_unfiltered).all(axis=1) & (np.abs(combined_features_unfiltered) < np.finfo(np.float64).max).all(axis=1)\n",
    "fit_data_with_features_unfiltered_clean = combined_features_unfiltered[mask_finite]\n",
    "labels_unfiltered_clean = labels[mask_finite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9586c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup memory\n",
    "del labels, fit_data\n",
    "del combined_features_unfiltered, flat_features_unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b644b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data and labels to parquet files\n",
    "save = True\n",
    "if save:\n",
    "    # Crea la cartella processed_data/binary_classification/data_w_features se non esiste\n",
    "    os.makedirs('processed_data/binary_classification/data_w_features', exist_ok=True)\n",
    "    pd.DataFrame(fit_data_with_features_unfiltered_clean, columns=FEATURES).to_parquet('test_data/binary_classification/combined_features_binary_snr_unfiltered.parquet', index=False)\n",
    "    pd.DataFrame(labels_unfiltered_clean).to_parquet('test_data/binary_classification/labels_binary_snr_unfiltered.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7edbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subsets of the data with balanced classes\n",
    "num_subsets = 10\n",
    "subset_size = 50000\n",
    "\n",
    "subsets_X_unfiltered = []\n",
    "subsets_y_unfiltered = []\n",
    "\n",
    "idx_0 = np.where(labels_clean == 0)[0]\n",
    "idx_1 = np.where(labels_clean == 1)[0]\n",
    "min_class_size = min(len(idx_0), len(idx_1), subset_size // 2)\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    idx_0_sample = np.random.choice(idx_0, size=min_class_size, replace=False)\n",
    "    idx_1_sample = np.random.choice(idx_1, size=min_class_size, replace=False)\n",
    "    idx_balanced = np.concatenate([idx_0_sample, idx_1_sample])\n",
    "    np.random.shuffle(idx_balanced)\n",
    "    subsets_X_unfiltered.append(fit_data_with_features_clean[idx_balanced])\n",
    "    subsets_y_unfiltered.append(labels_clean[idx_balanced])\n",
    "\n",
    "\n",
    "# Check the size of each subset and the distribution of classes\n",
    "for i, (X, y) in enumerate(zip(subsets_X_unfiltered, subsets_y_unfiltered)):\n",
    "    assert len(X) == subset_size, f\"Subset {i+1} size mismatch: {len(X)} != {subset_size}\"\n",
    "    assert len(y) == subset_size, f\"Subset {i+1} labels size mismatch: {len(y)} != {subset_size}\"\n",
    "    print(f\"Subset {i+1}: {Counter(y)}\")\n",
    "\n",
    "\n",
    "# Check if all elements in subsets_X_unfiltered are different\n",
    "all_unique = True\n",
    "for i in range(num_subsets):\n",
    "    for j in range(i + 1, num_subsets):\n",
    "        if np.array_equal(subsets_X_unfiltered[i], subsets_X_unfiltered[j]):\n",
    "            print(f\"Subset {i+1} and Subset {j+1} are identical!\")\n",
    "            all_unique = False\n",
    "if all_unique:\n",
    "    print(\"All subsets in subsets_X_unfiltered are different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687759d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models dict with wrappers for preprocessing\n",
    "models = {}\n",
    "\n",
    "if xg_boost_model_final is not None:\n",
    "    models[\"XGBoost Model Final\"] = type(\n",
    "        \"XGBWrapper\", (), {\"predict\": staticmethod(predict_with_xgboost)}\n",
    "    )()\n",
    "\n",
    "if best_binary_classification_model is not None:\n",
    "    models[\"Best Binary Classification Model\"] = type(\n",
    "        \"PyCaretWrapper\", (), {\"predict\": staticmethod(lambda X: predict_with_pycaret(best_binary_classification_model, X))}\n",
    "    )()\n",
    "\n",
    "if best_random_forest_model is not None:\n",
    "    models[\"Best Random Forest Model\"] = type(\n",
    "        \"RFWrapper\", (), {\"predict\": staticmethod(lambda X: predict_with_sklearn(best_random_forest_model, X))}\n",
    "    )()\n",
    "\n",
    "if best_binary_classification_ensembled_model is not None:\n",
    "    models[\"Best Binary Classification Ensembled Model\"] = type(\n",
    "        \"PyCaretEnsembleWrapper\", (), {\"predict\": staticmethod(lambda X: predict_with_pycaret(best_binary_classification_ensembled_model, X))}\n",
    "    )()\n",
    "if catboost_model_final is not None:\n",
    "    models[\"CatBoost Model Final\"] = type(\n",
    "        \"CatBoostWrapper\", (), {\"predict\": staticmethod(predict_with_catboost)}\n",
    "    )()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    accs = []\n",
    "    cms = []\n",
    "    for i in range(num_subsets):\n",
    "        X = subsets_X_unfiltered[i]\n",
    "        y = subsets_y_unfiltered[i]\n",
    "\n",
    "        # Searching for the optimal threshold if the model supports it\n",
    "        # If the model has a predict_proba method, we can find the optimal threshold\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X)[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y, y_proba)\n",
    "            # Soglia ottimale: massimizza tpr - fpr (Youden's J statistic)\n",
    "            optimal_idx = np.argmax(tpr - fpr)\n",
    "            optimal_threshold = thresholds[optimal_idx]\n",
    "            print(f\"SOGLIA OTTIMALE DI CLASSIFICAZIONE: {optimal_threshold:.4f}\")\n",
    "            y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "        else:\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        print(f\"Subset {i+1}: Accuracy = {acc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        accs.append(acc)\n",
    "        cms.append(cm)\n",
    "    print(f\"Mean Accuracy: {np.mean(accs):.4f}\")\n",
    "    print(f\"Std Accuracy: {np.std(accs):.4f}\")\n",
    "    print(f\"Mean Confusion Matrix:\\n{np.mean(np.array(cms), axis=0).astype(int)}\")\n",
    "    # AVG accuracy and confusion matrix across all subsets\n",
    "    mean_acc = np.mean(accs)\n",
    "    std_acc = np.std(accs)\n",
    "    mean_cm = np.mean(np.array(cms), axis=0)\n",
    "    print(f\"\\nMedia accuratezze: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "    print(f\"Confusion Matrix media:\\n{mean_cm.astype(int)}\")\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot accuracy per subset\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, num_subsets + 1), accs, marker='o')\n",
    "    plt.title(f'Accuracy per subset - {model_name}')\n",
    "    plt.xlabel('Subset')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot mean confusion matrix\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(mean_cm, annot=True, fmt='.0f', cmap='Blues')\n",
    "    plt.title(f'Mean Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    reports = []\n",
    "    for i in range(num_subsets):\n",
    "        y_true = subsets_y_unfiltered[i]\n",
    "        y_pred = model.predict(subsets_X_unfiltered[i])\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        reports.append(report)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(range(1, num_subsets + 1), accs, color='red')\n",
    "    plt.title(f'Scatter Plot Accuracy per Test Set Unfiltered for SNR - {model_name}')\n",
    "    plt.xlabel('Test Set Number')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
