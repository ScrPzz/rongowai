{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60808b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import uniform, randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "from scipy.fft import fft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from pycaret.classification import *\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class RandomForestBinaryPipeline:\n",
    "    def __init__(self, fit_data, labels_data, test_size=0.2, val_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Pipeline completa per classificatore binario Random Forest\n",
    "        \n",
    "        Args:\n",
    "            fit_data: DataFrame o array con le features\n",
    "            labels_data: Serie o array con le etichette\n",
    "            test_size: dimensione del test set\n",
    "            val_size: dimensione del validation set (dal training set)\n",
    "            random_state: seed per riproducibilitÃ \n",
    "        \"\"\"\n",
    "        self.fit_data = fit_data\n",
    "        self.labels_data = labels_data\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Modelli e risultati\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.train_scores = {}\n",
    "        self.val_scores = {}\n",
    "        self.test_scores = {}\n",
    "        \n",
    "        # Dati preprocessati\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        # Preprocessing\n",
    "        self.categorical_features = []\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = None\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocessa i dati e li divide in train/val/test\"\"\"\n",
    "        print(\"ðŸ”„ Preprocessing dei dati...\")\n",
    "        \n",
    "        # Converti in DataFrame se necessario\n",
    "        if not isinstance(self.fit_data, pd.DataFrame):\n",
    "            self.fit_data = pd.DataFrame(self.fit_data)\n",
    "        \n",
    "        # Copia per evitare modifiche ai dati originali\n",
    "        X = self.fit_data.copy()\n",
    "        \n",
    "        # Identifica e preprocessa feature categoriche\n",
    "        self.categorical_features = []\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "                self.categorical_features.append(col)\n",
    "                # Label encoding per Random Forest\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                self.label_encoders[col] = le\n",
    "        \n",
    "        print(f\"ðŸ“Š Feature categoriche identificate e codificate: {self.categorical_features}\")\n",
    "        \n",
    "        # Split train-test\n",
    "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
    "            X, self.labels_data, \n",
    "            test_size=self.test_size, \n",
    "            stratify=self.labels_data,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Split train-validation\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=self.val_size,\n",
    "            stratify=y_temp,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Scaling opzionale per Random Forest (generalmente non necessario, ma puÃ² aiutare)\n",
    "        # Commentare se non desiderato\n",
    "        \"\"\"\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train = pd.DataFrame(\n",
    "            self.scaler.fit_transform(self.X_train),\n",
    "            columns=self.X_train.columns,\n",
    "            index=self.X_train.index\n",
    "        )\n",
    "        self.X_val = pd.DataFrame(\n",
    "            self.scaler.transform(self.X_val),\n",
    "            columns=self.X_val.columns,\n",
    "            index=self.X_val.index\n",
    "        )\n",
    "        self.X_test = pd.DataFrame(\n",
    "            self.scaler.transform(self.X_test),\n",
    "            columns=self.X_test.columns,\n",
    "            index=self.X_test.index\n",
    "        )\n",
    "        print(\"ðŸ“ Scaling applicato ai dati\")\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ðŸ“ˆ Dimensioni datasets:\")\n",
    "        print(f\"  - Train: {self.X_train.shape}\")\n",
    "        print(f\"  - Validation: {self.X_val.shape}\")\n",
    "        print(f\"  - Test: {self.X_test.shape}\")\n",
    "        \n",
    "        # Statistiche sulle classi\n",
    "        print(f\"ðŸ“Š Distribuzione classi (Train): {pd.Series(self.y_train).value_counts().to_dict()}\")\n",
    "        \n",
    "    def define_search_space(self):\n",
    "        \"\"\"Definisce lo spazio di ricerca per gli iperparametri di Random Forest\"\"\"\n",
    "        param_distributions = {\n",
    "            'n_estimators': randint(50, 500),\n",
    "            'max_depth': [None] + list(randint(3, 50).rvs(10)),\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'max_features': ['sqrt', 'log2', None] + list(uniform(0.1, 0.8).rvs(5)),\n",
    "            'bootstrap': [True, False],\n",
    "            'class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_samples': [None] + list(uniform(0.5, 0.5).rvs(5)),  # 0.5 to 1.0\n",
    "        }\n",
    "        return param_distributions\n",
    "    \n",
    "    def hyperparameter_search(self, n_iter=50, cv_folds=5, scoring='roc_auc'):\n",
    "        \"\"\"\n",
    "        Esegue la ricerca degli iperparametri\n",
    "        \n",
    "        Args:\n",
    "            n_iter: numero di iterazioni per RandomizedSearchCV\n",
    "            cv_folds: numero di fold per cross-validation\n",
    "            scoring: metrica di scoring\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ” Avvio ricerca iperparametri ({n_iter} iterazioni, {cv_folds}-fold CV)...\")\n",
    "        \n",
    "        # Modello base\n",
    "        base_model = RandomForestClassifier(\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            oob_score=True  # Out-of-bag score per valutazione aggiuntiva\n",
    "        )\n",
    "        \n",
    "        # Spazio di ricerca\n",
    "        param_distributions = self.define_search_space()\n",
    "        \n",
    "        # Cross-validation stratificata\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # RandomizedSearchCV\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            random_state=self.random_state,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit del modello\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Salva i migliori parametri\n",
    "        self.best_params = random_search.best_params_\n",
    "        self.best_model = random_search.best_estimator_\n",
    "        \n",
    "        print(f\"âœ… Ricerca completata!\")\n",
    "        print(f\"ðŸ† Migliore score CV: {random_search.best_score_:.4f}\")\n",
    "        if hasattr(self.best_model, 'oob_score_'):\n",
    "            print(f\"ðŸŽ¯ OOB Score: {self.best_model.oob_score_:.4f}\")\n",
    "        print(f\"ðŸŽ¯ Migliori parametri:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  - {param}: {value}\")\n",
    "            \n",
    "    def train_final_model(self):\n",
    "        \"\"\"Addestra il modello finale con i migliori iperparametri\"\"\"\n",
    "        print(\"ðŸš€ Addestramento modello finale...\")\n",
    "        \n",
    "        if self.best_params is None:\n",
    "            print(\"âš ï¸  Nessun iperparametro trovato. Uso parametri di default.\")\n",
    "            self.best_model = RandomForestClassifier(\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1,\n",
    "                oob_score=True\n",
    "            )\n",
    "            self.best_model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        print(\"âœ… Addestramento completato!\")\n",
    "        if hasattr(self.best_model, 'oob_score_'):\n",
    "            print(f\"ðŸ“Š Final OOB Score: {self.best_model.oob_score_:.4f}\")\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Valuta il modello su tutti i dataset\"\"\"\n",
    "        print(\"ðŸ“Š Valutazione del modello...\")\n",
    "        \n",
    "        datasets = {\n",
    "            'train': (self.X_train, self.y_train),\n",
    "            'validation': (self.X_val, self.y_val),\n",
    "            'test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        all_scores = {}\n",
    "        \n",
    "        for name, (X, y) in datasets.items():\n",
    "            # Predizioni\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            y_pred_proba = self.best_model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            # Metriche\n",
    "            scores = {\n",
    "                'accuracy': accuracy_score(y, y_pred),\n",
    "                'precision': precision_score(y, y_pred),\n",
    "                'recall': recall_score(y, y_pred),\n",
    "                'f1': f1_score(y, y_pred),\n",
    "                'roc_auc': roc_auc_score(y, y_pred_proba)\n",
    "            }\n",
    "            \n",
    "            all_scores[name] = scores\n",
    "            \n",
    "            print(f\"\\nðŸ“ˆ Metriche {name.upper()}:\")\n",
    "            for metric, value in scores.items():\n",
    "                print(f\"  - {metric.upper()}: {value:.4f}\")\n",
    "        \n",
    "        # Salva i risultati\n",
    "        self.train_scores = all_scores['train']\n",
    "        self.val_scores = all_scores['validation']\n",
    "        self.test_scores = all_scores['test']\n",
    "        \n",
    "        return all_scores\n",
    "    \n",
    "    def plot_feature_importance(self, top_n=20):\n",
    "        \"\"\"Visualizza l'importanza delle feature\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"âš ï¸  Modello non ancora addestrato!\")\n",
    "            return\n",
    "            \n",
    "        # Ottieni feature importance\n",
    "        feature_names = self.X_train.columns if hasattr(self.X_train, 'columns') else [f'feature_{i}' for i in range(self.X_train.shape[1])]\n",
    "        importance = self.best_model.feature_importances_\n",
    "        \n",
    "        # Crea DataFrame per plotting\n",
    "        fi_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False).head(top_n)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=fi_df, x='importance', y='feature')\n",
    "        plt.title(f'Top {top_n} Feature Importance (Random Forest)')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_trees_analysis(self):\n",
    "        \"\"\"Visualizza analisi degli alberi\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"âš ï¸  Modello non ancora addestrato!\")\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # ProfonditÃ  degli alberi\n",
    "        depths = [tree.tree_.max_depth for tree in self.best_model.estimators_]\n",
    "        axes[0].hist(depths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title('Distribuzione ProfonditÃ  Alberi')\n",
    "        axes[0].set_xlabel('ProfonditÃ ')\n",
    "        axes[0].set_ylabel('Frequenza')\n",
    "        \n",
    "        # Numero di nodi fogliari\n",
    "        n_leaves = [tree.tree_.n_leaves for tree in self.best_model.estimators_]\n",
    "        axes[1].hist(n_leaves, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        axes[1].set_title('Distribuzione Numero Foglie')\n",
    "        axes[1].set_xlabel('Numero Foglie')\n",
    "        axes[1].set_ylabel('Frequenza')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"ðŸ“Š Statistiche alberi:\")\n",
    "        print(f\"  - ProfonditÃ  media: {np.mean(depths):.2f} Â± {np.std(depths):.2f}\")\n",
    "        print(f\"  - Foglie medie: {np.mean(n_leaves):.2f} Â± {np.std(n_leaves):.2f}\")\n",
    "        \n",
    "    def plot_confusion_matrices(self):\n",
    "        \"\"\"Visualizza le confusion matrix per tutti i dataset\"\"\"\n",
    "        datasets = {\n",
    "            'Train': (self.X_train, self.y_train),\n",
    "            'Validation': (self.X_val, self.y_val),\n",
    "            'Test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        for idx, (name, (X, y)) in enumerate(datasets.items()):\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            cm = confusion_matrix(y, y_pred)\n",
    "            \n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx], cmap='Blues')\n",
    "            axes[idx].set_title(f'Confusion Matrix - {name}')\n",
    "            axes[idx].set_xlabel('Predicted')\n",
    "            axes[idx].set_ylabel('Actual')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_classification_reports(self):\n",
    "        \"\"\"Stampa i classification report dettagliati\"\"\"\n",
    "        datasets = {\n",
    "            'Train': (self.X_train, self.y_train),\n",
    "            'Validation': (self.X_val, self.y_val),\n",
    "            'Test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        for name, (X, y) in datasets.items():\n",
    "            y_pred = self.best_model.predict(X)\n",
    "            print(f\"\\nðŸ“‹ CLASSIFICATION REPORT - {name.upper()}\")\n",
    "            print(\"=\" * 50)\n",
    "            print(classification_report(y, y_pred))\n",
    "    \n",
    "    def get_model_complexity_info(self):\n",
    "        \"\"\"Informazioni sulla complessitÃ  del modello\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"âš ï¸  Modello non ancora addestrato!\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\nðŸŒ³ INFORMAZIONI MODELLO RANDOM FOREST:\")\n",
    "        print(f\"  - Numero alberi: {self.best_model.n_estimators}\")\n",
    "        print(f\"  - ProfonditÃ  max: {self.best_model.max_depth}\")\n",
    "        print(f\"  - Features per split: {self.best_model.max_features}\")\n",
    "        print(f\"  - Min samples split: {self.best_model.min_samples_split}\")\n",
    "        print(f\"  - Min samples leaf: {self.best_model.min_samples_leaf}\")\n",
    "        if hasattr(self.best_model, 'oob_score_'):\n",
    "            print(f\"  - OOB Score: {self.best_model.oob_score_:.4f}\")\n",
    "    \n",
    "    def run_complete_pipeline(self, n_iter=50, cv_folds=5):\n",
    "        \"\"\"Esegue la pipeline completa\"\"\"\n",
    "        print(\"ðŸš€ AVVIO PIPELINE COMPLETA RANDOM FOREST\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Preprocessing\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # 2. Ricerca iperparametri\n",
    "        self.hyperparameter_search(n_iter=n_iter, cv_folds=cv_folds)\n",
    "        \n",
    "        # 3. Addestramento finale\n",
    "        self.train_final_model()\n",
    "        \n",
    "        # 4. Valutazione\n",
    "        scores = self.evaluate_model()\n",
    "        \n",
    "        # 5. Informazioni modello\n",
    "        self.get_model_complexity_info()\n",
    "        \n",
    "        # 6. Report dettagliati\n",
    "        self.get_classification_reports()\n",
    "        \n",
    "        # 7. Visualizzazioni\n",
    "        self.plot_feature_importance()\n",
    "        self.plot_trees_analysis()\n",
    "        self.plot_confusion_matrices()\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ PIPELINE COMPLETATA CON SUCCESSO!\")\n",
    "        print(f\"ðŸ† Performance finale (Test Set): AUC = {self.test_scores['roc_auc']:.4f}\")\n",
    "        \n",
    "        return self.best_model, self.best_params, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c9d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCDFPreprocessor:\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.netcdf_file_list = os.listdir(root_dir)\n",
    "\n",
    "    def preprocess(self, f):\n",
    "        \n",
    "        raw_counts = f.variables['raw_counts']\n",
    "        raw_counts = np.array(raw_counts)\n",
    "\n",
    "        # Calcolo distanza tra il punto speculare e l'aereo\n",
    "        ac_alt_2d = np.repeat(np.array(f.variables['ac_alt'])[:, np.newaxis], 20, axis=1)\n",
    "        distance_2d = (ac_alt_2d - f.variables['sp_alt'][:]) / np.cos(np.deg2rad(f.variables['sp_inc_angle'][:]))\n",
    "\n",
    "        # Seleziona gli indici dove sp_rx_gain_copol > 5, sp_rx_gain_xpol > 5 e ddm_snr > 0 e distanza tra punto speculare e antenna > 2000 e < 10000\n",
    "        copol = f.variables['sp_rx_gain_copol'][:]\n",
    "        xpol = f.variables['sp_rx_gain_xpol'][:]\n",
    "        snr = f.variables['ddm_snr'][:]\n",
    "        dist = distance_2d[:]\n",
    "\n",
    "        keep_mask = (copol >= 5) & (xpol >= 5) & (snr > 0) & ((dist >= 2000) & (dist <= 10000)) & (~np.isnan(copol.data) & ~np.isnan(xpol.data) & ~np.isnan(snr.data) & ~np.isnan(dist.data))\n",
    "        to_keep_indices = np.argwhere(keep_mask)\n",
    "        \n",
    "        filtered_raw_counts = [raw_counts[i, j] for i, j in to_keep_indices]\n",
    "        #filtered_raw_counts_arr = np.array(filtered_raw_counts)\n",
    "\n",
    "        output_array = np.full(raw_counts.shape, np.nan, dtype=np.float32)\n",
    "\n",
    "        # Inserisci i dati filtrati nelle posizioni di to_keep_indices\n",
    "        for idx, (i, j) in enumerate(to_keep_indices):\n",
    "            output_array[i, j] = filtered_raw_counts[idx]\n",
    "\n",
    "        raw_counts_filtered = output_array.copy()\n",
    "        del output_array\n",
    "\n",
    "        ddm_data_dict = {\n",
    "            'Raw_Counts': raw_counts_filtered.reshape(raw_counts_filtered.shape[0]*raw_counts_filtered.shape[1], raw_counts_filtered.shape[2], raw_counts_filtered.shape[3]),\n",
    "        }\n",
    "        keep_indices = np.where(\n",
    "            np.all(~np.isnan(ddm_data_dict['Raw_Counts']), axis=(1, 2)) & (np.sum(ddm_data_dict['Raw_Counts'], axis=(1, 2)) > 0)\n",
    "        )[0]\n",
    "        fit_data = np.array([ddm_data_dict['Raw_Counts'][f].ravel() for f in keep_indices])\n",
    "        surface_types = f.variables[\"sp_surface_type\"][:]\n",
    "        surface_types = np.nan_to_num(surface_types, nan=0)\n",
    "        surface_types_unravelled = surface_types.ravel()\n",
    "\n",
    "        label_data = surface_types_unravelled\n",
    "        label_data = [label_data[l] for l in range(len(label_data)) if l in keep_indices]\n",
    "\n",
    "        return fit_data, label_data\n",
    "\n",
    "    def process_all_files(self, chunk_size = int, sample_fraction = float, remove_chunks= bool):\n",
    "        \n",
    "        full_data = []\n",
    "        full_labels = []\n",
    "        #counter = 0\n",
    "        for file_name in tqdm(self.netcdf_file_list, desc=\"Processing files\"):\n",
    "            if not file_name.endswith('.nc'):\n",
    "                continue\n",
    "            try:\n",
    "                f = netCDF4.Dataset(f'{self.root_dir}{file_name}')\n",
    "                data, labels = self.preprocess(f)\n",
    "                full_data.append(data)\n",
    "                full_labels.append(labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "            #counter += 1\n",
    "            #if counter == 100:  # Limita a 50 file per il caricamento\n",
    "            #    break\n",
    "        \n",
    "        # Trova gli indici degli elementi di full_data con seconda dimensione uguale a 200\n",
    "        valid_indices = [i for i, arr in enumerate(full_data) if arr.ndim == 2 if arr.shape[1] == 200]\n",
    "\n",
    "        # Applica la selezione a full_data e full_labels\n",
    "        full_data_clean = [full_data[i] for i in valid_indices]\n",
    "        full_labels_clean = [full_labels[i] for i in valid_indices]\n",
    "\n",
    "        # Chunking \n",
    "        \n",
    "        os.makedirs('processed_data/multiclass', exist_ok=True)\n",
    "\n",
    "        chunk_size = chunk_size # dimensione del chunk in numero di campioni\n",
    "        sample_fraction = sample_fraction  # frazione di dati da campionare per ogni chunk\n",
    "\n",
    "        full_data_sampled = []\n",
    "        full_labels_sampled = []\n",
    "\n",
    "        num_chunks = int(np.ceil(len(full_data_clean) / chunk_size))\n",
    "        for idx in range(num_chunks):\n",
    "            start = idx * chunk_size\n",
    "            end = min((idx + 1) * chunk_size, len(full_data_clean))\n",
    "            chunk_data = np.vstack(full_data_clean[start:end])\n",
    "            chunk_labels = np.hstack(full_labels_clean[start:end])\n",
    "            print(f\"Distribuzione etichette nel chunk: {Counter(chunk_labels)}\")\n",
    "            \n",
    "            print(f\"Chunk {idx + 1}/{num_chunks} processed with shape {chunk_data.shape} and labels shape {chunk_labels.shape}\")\n",
    "            print(''-' * 50')\n",
    "            \n",
    "\n",
    "            # Salva ogni chunk come file parquet separato\n",
    "            fit_data_df = pd.DataFrame(chunk_data)\n",
    "            labels_df = pd.DataFrame(chunk_labels, columns=['label'])\n",
    "\n",
    "            table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "            table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "\n",
    "            pq.write_table(\n",
    "                table_fit,\n",
    "                f'processed_data/multiclass/fit_data_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "            pq.write_table(\n",
    "                table_labels,\n",
    "                f'processed_data/multiclass/labels_chunk_{idx}.parquet',\n",
    "                compression='zstd',\n",
    "                use_dictionary=True,\n",
    "            )\n",
    "\n",
    "        # Imposta la frazione di dati da campionare per ogni chunk (es: 0.2 per il 20%)\n",
    "        \n",
    "            #_, X_sampled, _, y_sampled = train_test_split(\n",
    "            #    chunk_data, chunk_labels, \n",
    "            #    test_size=sample_fraction, \n",
    "            #    stratify=chunk_labels, \n",
    "            #    random_state=42\n",
    "            #) \n",
    "\n",
    "            # Trova le classi piÃ¹ rare (meno rappresentate)\n",
    "            label_counts = Counter(chunk_labels)\n",
    "            min_count = min(label_counts.values())\n",
    "            rare_classes = [cls for cls, count in label_counts.items() if count == min_count]\n",
    "\n",
    "            # Seleziona tutte le occorrenze delle classi rare\n",
    "            rare_indices = np.isin(chunk_labels, rare_classes)\n",
    "            X_rare = chunk_data[rare_indices]\n",
    "            y_rare = chunk_labels[rare_indices]\n",
    "\n",
    "            # Per le altre classi, esegui un campionamento casuale per raggiungere la frazione desiderata\n",
    "            other_indices = ~rare_indices\n",
    "            X_other = chunk_data[other_indices]\n",
    "            y_other = chunk_labels[other_indices]\n",
    "\n",
    "            _, X_other_sampled, _, y_other_sampled = train_test_split(\n",
    "                X_other, y_other,\n",
    "                test_size=sample_fraction,\n",
    "                stratify=y_other,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            # Combina i dati delle classi rare con quelli campionati delle altre classi\n",
    "            X_sampled = np.vstack([X_rare, X_other_sampled])\n",
    "            y_sampled = np.hstack([y_rare, y_other_sampled])\n",
    "\n",
    "            print(f\"Distribuzione etichette prima del campionamento nel chunk {idx + 1}: {Counter(chunk_labels)}\")\n",
    "            print(f\"Distribuzione etichette dopo campionamento nel chunk {idx + 1}: {Counter(y_sampled)}\")\n",
    "            \n",
    "            del full_data, full_labels\n",
    "            \n",
    "            full_data_sampled.append(X_sampled)\n",
    "            full_labels_sampled.append(y_sampled)\n",
    "\n",
    "        full_data_sampled_stratified = np.vstack(full_data_sampled)\n",
    "        full_labels_sampled_stratified = np.hstack(full_labels_sampled)\n",
    "\n",
    "        print(f\"Distribuzione totale etichette dopo stratificazione: {Counter(full_labels_sampled_stratified)}\")\n",
    "\n",
    "        \n",
    "        print(f\"Shape of sampled data after chunking and sampling: {np.array(full_data_sampled_stratified).shape}\")\n",
    "        print(f\"Shape of sampled labels after chunking and sampling: {np.array(full_labels_sampled_stratified).shape}\")\n",
    "        \n",
    "        # Crea la cartella processed_data se non esiste\n",
    "        os.makedirs('processed_data/multiclass', exist_ok=True)\n",
    "\n",
    "        # Salva fit_data in formato parquet ottimizzato\n",
    "        fit_data_df = pd.DataFrame(full_data_sampled_stratified)\n",
    "        table_fit = pa.Table.from_pandas(fit_data_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_fit,\n",
    "            'processed_data/multiclass/fit_data_multiclass.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Salva labels in formato parquet ottimizzato\n",
    "        labels_df = pd.DataFrame(full_labels_sampled_stratified, columns=['label'])\n",
    "        table_labels = pa.Table.from_pandas(labels_df, preserve_index=False)\n",
    "        pq.write_table(\n",
    "            table_labels,\n",
    "            'processed_data/multiclass/labels_multiclass.parquet',\n",
    "            compression='zstd',\n",
    "            use_dictionary=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "        del fit_data_df, labels_df, table_fit, table_labels\n",
    "        # Remove all chunk parquet files if flag is set\n",
    "        if remove_chunks:\n",
    "            try:\n",
    "                chunk_dir = 'processed_data/multiclass'\n",
    "                for fname in os.listdir(chunk_dir):\n",
    "                    if fname.startswith('fit_data_chunk_') or fname.startswith('labels_chunk_'):\n",
    "                        os.remove(os.path.join(chunk_dir, fname))\n",
    "                print(\"All chunk files removed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing chunk files: {e}\")\n",
    "\n",
    "        return full_data_sampled_stratified, full_labels_sampled_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDMFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_ddm_features_MORE(self, fit_data: np.ndarray) -> pd.DataFrame:\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Estrae features dettagliate da raw_counts DDM (shape: n_samples x 200)\n",
    "        \"\"\"\n",
    "\n",
    "        def gini(array):\n",
    "            \"\"\"Calcola il coefficiente di Gini (disuguaglianza)\"\"\"\n",
    "            array = np.sort(array)\n",
    "            index = np.arange(1, array.shape[0] + 1)\n",
    "            return (np.sum((2 * index - array.shape[0] - 1) * array)) / (array.shape[0] * np.sum(array))\n",
    "        \n",
    "\n",
    "        features = []\n",
    "\n",
    "        for row in tqdm(fit_data, desc=\"Extracting DDM features\"):\n",
    "            f = {}\n",
    "            x = np.array(row, dtype=np.float32) + 1e-10  # evita log(0)\n",
    "\n",
    "            # 1. Statistiche base\n",
    "            f['mean'] = np.mean(x)\n",
    "            f['std'] = np.std(x)\n",
    "            f['min'] = np.min(x)\n",
    "            f['max'] = np.max(x)\n",
    "            f['median'] = np.median(x)\n",
    "            f['range'] = np.max(x) - np.min(x)\n",
    "            f['skew'] = skew(x)\n",
    "            f['kurtosis'] = kurtosis(x)\n",
    "            f['entropy'] = entropy(x)\n",
    "            f['gini'] = gini(x)\n",
    "\n",
    "            # 2. Posizionali\n",
    "            f['peak_index'] = np.argmax(x)\n",
    "            f['peak_value'] = np.max(x)\n",
    "            f['center_of_mass'] = np.sum(np.arange(len(x)) * x) / np.sum(x)\n",
    "            f['inertia'] = np.sum(((np.arange(len(x)) - f['center_of_mass'])**2) * x)\n",
    "\n",
    "            # 3. Segmentazione\n",
    "            thirds = np.array_split(x, 3)\n",
    "            for i, part in enumerate(thirds):\n",
    "                f[f'sum_third_{i+1}'] = np.sum(part)\n",
    "                f[f'mean_third_{i+1}'] = np.mean(part)\n",
    "                f[f'max_third_{i+1}'] = np.max(part)\n",
    "            \n",
    "            windows = np.array_split(x, 5)\n",
    "            for i, w in enumerate(windows):\n",
    "                f[f'mean_w{i+1}'] = np.mean(w)\n",
    "                f[f'std_w{i+1}'] = np.std(w)\n",
    "                f[f'max_w{i+1}'] = np.max(w)\n",
    "\n",
    "            # 4. Derivate e cambiamenti\n",
    "            dx = np.diff(x)\n",
    "            f['mean_diff'] = np.mean(dx)\n",
    "            f['std_diff'] = np.std(dx)\n",
    "            f['max_diff'] = np.max(dx)\n",
    "            f['min_diff'] = np.min(dx)\n",
    "            f['n_positive_diff'] = np.sum(dx > 0)\n",
    "            f['n_negative_diff'] = np.sum(dx < 0)\n",
    "            f['n_zero_diff'] = np.sum(dx == 0)\n",
    "\n",
    "            # 5. Autocorrelazioni (lag 1-3)\n",
    "            for lag in range(1, 4):\n",
    "                ac = np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag else np.nan\n",
    "                f[f'autocorr_lag{lag}'] = ac\n",
    "\n",
    "            # 6. FFT (spettro frequenze)\n",
    "            spectrum = np.abs(fft(x))\n",
    "            half_spectrum = spectrum[:len(spectrum)//2]  # simmetrico\n",
    "            f['fft_peak_freq'] = np.argmax(half_spectrum)\n",
    "            f['fft_max'] = np.max(half_spectrum)\n",
    "            f['fft_median'] = np.median(half_spectrum)\n",
    "            f['fft_mean'] = np.mean(half_spectrum)\n",
    "\n",
    "            features.append(f)\n",
    "\n",
    "        return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR= 'D:/data/RONGOWAI_L1_SDR_V1.0/'\n",
    "\n",
    "read_from_backup = True\n",
    "if read_from_backup:\n",
    "    #import polars as pl\n",
    "\n",
    "    # Leggi i file parquet con polars\n",
    "    fit_data_pl = pd.read_parquet('processed_data/multiclass/fit_data_multiclass.parquet')\n",
    "    labels_pl = pd.read_parquet('processed_data/multiclass/labels_multiclass.parquet')\n",
    "\n",
    "    # Trasforma in numpy array\n",
    "    fit_data = fit_data_pl.to_numpy()\n",
    "    labels = labels_pl['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f54387",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extractor = DDMFeatureExtractor()\n",
    "ddm_features = features_extractor.create_ddm_features_MORE(fit_data)\n",
    "fit_data_with_features = np.hstack([fit_data, ddm_features.values])\n",
    "fit_data_with_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RandomForestBinaryPipeline(fit_data, labels)\n",
    "    \n",
    "    # Esegui pipeline completa\n",
    "    best_model, best_params, final_scores = pipeline.run_complete_pipeline(\n",
    "        n_iter=30,  # Riduci per test piÃ¹ veloce\n",
    "        cv_folds=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ RISULTATI FINALI:\")\n",
    "    print(f\"Migliori parametri: {best_params}\")\n",
    "    print(f\"Score finale: {final_scores}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
